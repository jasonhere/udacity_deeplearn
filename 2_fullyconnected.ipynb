{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 2\n",
    "------------\n",
    "\n",
    "Previously in `1_notmnist.ipynb`, we created a pickle with formatted datasets for training, development and testing on the [notMNIST dataset](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html).\n",
    "\n",
    "The goal of this assignment is to progressively train deeper and more accurate models using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 19456,
     "status": "ok",
     "timestamp": 1449847956073,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "0ddb1607-1fc4-4ddb-de28-6c7ab7fb0c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (300000, 28, 28), (300000,))\n",
      "('Validation set', (60000, 28, 28), (60000,))\n",
      "('Test set', (18724, 28, 28), (18724,))\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 19723,
     "status": "ok",
     "timestamp": 1449847956364,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "2ba0fc75-1487-4ace-a562-cf81cae82793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (300000, 784), (300000, 10))\n",
      "('Validation set', (60000, 784), (60000, 10))\n",
      "('Test set', (18724, 784), (18724, 10))\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCLVqyQ5vPPH"
   },
   "source": [
    "We're first going to train a multinomial logistic regression using simple gradient descent.\n",
    "\n",
    "TensorFlow works like this:\n",
    "* First you describe the computation that you want to see performed: what the inputs, the variables, and the operations look like. These get created as nodes over a computation graph. This description is all contained within the block below:\n",
    "\n",
    "      with graph.as_default():\n",
    "          ...\n",
    "\n",
    "* Then you can run the operations on this graph as many times as you want by calling `session.run()`, providing it outputs to fetch from the graph that get returned. This runtime operation is all contained in the block below:\n",
    "\n",
    "      with tf.Session(graph=graph) as session:\n",
    "          ...\n",
    "\n",
    "Let's load all the data into TensorFlow and build the computation graph corresponding to our training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Nfv39qvtvOl_"
   },
   "outputs": [],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 10000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random valued following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQcL4uqISHjP"
   },
   "source": [
    "Let's run this computation and iterate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 9
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 57454,
     "status": "ok",
     "timestamp": 1449847994134,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "z2cjdenH869W",
    "outputId": "4c037ba1-b526-4d8e-e632-91e2a0333267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 19.431513\n",
      "Training accuracy: 6.6%\n",
      "Validation accuracy: 10.3%\n",
      "Loss at step 100: 2.436723\n",
      "Training accuracy: 70.9%\n",
      "Validation accuracy: 70.2%\n",
      "Loss at step 200: 1.942931\n",
      "Training accuracy: 73.9%\n",
      "Validation accuracy: 72.8%\n",
      "Loss at step 300: 1.682984\n",
      "Training accuracy: 75.2%\n",
      "Validation accuracy: 73.8%\n",
      "Loss at step 400: 1.511981\n",
      "Training accuracy: 76.0%\n",
      "Validation accuracy: 74.2%\n",
      "Loss at step 500: 1.387057\n",
      "Training accuracy: 76.6%\n",
      "Validation accuracy: 74.5%\n",
      "Loss at step 600: 1.289858\n",
      "Training accuracy: 77.2%\n",
      "Validation accuracy: 74.7%\n",
      "Loss at step 700: 1.210702\n",
      "Training accuracy: 77.8%\n",
      "Validation accuracy: 74.9%\n",
      "Loss at step 800: 1.144348\n",
      "Training accuracy: 78.2%\n",
      "Validation accuracy: 75.1%\n",
      "Test accuracy: 82.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # This is a one-time operation which ensures the parameters get initialized as\n",
    "  # we described in the graph: random weights for the matrix, zeros for the\n",
    "  # biases. \n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the loss value and the training predictions returned as numpy\n",
    "    # arrays.\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    if (step % 100 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(\n",
    "        predictions, train_labels[:train_subset, :]))\n",
    "      # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "      # just to get that one numpy array. Note that it recomputes all its graph\n",
    "      # dependencies.\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x68f-hxRGm3H"
   },
   "source": [
    "Let's now switch to stochastic gradient descent training instead, which is much faster.\n",
    "\n",
    "The graph will be similar, except that instead of holding all the training data into a constant node, we create a `Placeholder` node which will be fed actual data at every call of `sesion.run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "qhPMzWYRGrzM"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XmVZESmtG4JH"
   },
   "source": [
    "Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 6
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 66292,
     "status": "ok",
     "timestamp": 1449848003013,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "FoF91pknG_YW",
    "outputId": "d255c80e-954d-4183-ca1c-c7333ce91d0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 20.545992\n",
      "Minibatch accuracy: 3.9%\n",
      "Validation accuracy: 7.4%\n",
      "Minibatch loss at step 500: 1.644372\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 1000: 1.516829\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 1500: 1.501507\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 2000: 0.905604\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 2500: 1.338037\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 3000: 1.062147\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 78.5%\n",
      "Test accuracy: 86.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7omWxtvLLxik"
   },
   "source": [
    "---\n",
    "Problem\n",
    "-------\n",
    "\n",
    "Turn the logistic regression example with SGD into a 1-hidden layer neural network with rectified linear units (nn.relu()) and 1024 hidden nodes. This model should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "hidden_nodes = 2048\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(None, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "  #tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  #tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # input layer variables\n",
    "  weights_i = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "  biases_i = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "  #input operation\n",
    "  #logits = tf.matmul(tf_train_dataset, weights_i) + biases_i\n",
    "    \n",
    "  # hidden layer variables\n",
    "  #W_fc1 = tf.Variable(tf.truncated_normal([1024, num_labels]))\n",
    "  #b_fc1 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "  #h_pool2_flat = tf.reshape(weights_i, [-1, 7*7*64])\n",
    "  #h_fc1 = tf.nn.relu(tf.matmul(logits, W_fc1) + b_fc1)\n",
    "  h_fc1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_i) + biases_i)\n",
    "\n",
    "  # output layer variables\n",
    "  weights_o = tf.Variable(tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "  biases_o = tf.Variable(tf.zeros([num_labels]))\n",
    "  #output operation\n",
    "  logits_o = tf.matmul(h_fc1, weights_o) + biases_o\n",
    "  y_conv = tf.nn.softmax(logits_o)  \n",
    "  #define loss\n",
    "  #loss = tf.reduce_mean(cross_entropy)  \n",
    "  #loss = -tf.reduce_sum(tf_train_labels*tf.log(y_conv))\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits_o, tf_train_labels))\n",
    "  # Optimizer.\n",
    "  #optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "  #decay learning rate\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    0.1, global_step * batch_size, train_dataset.shape[0] / 2, 0.95, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  #train_prediction = tf.nn.softmax(logits_o)\n",
    "  #valid_prediction = tf.nn.softmax(\n",
    "  #  tf.matmul(tf_valid_dataset, weights_i) + biases_i)\n",
    "  #test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights_o) + biases_o)  \n",
    "  is_correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(tf_train_labels,1))\n",
    "  accuracy_l = tf.reduce_mean(tf.cast(is_correct_prediction, tf.float32))\n",
    "  #tf.nn.softmax(tf.matmul(tf_test_dataset, weights_o) + biases_o)\n",
    "  saver = tf.train.Saver(max_to_keep=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 495.862335 with learning rate 0.100000\n",
      "step 0, training accuracy 0.335938\n",
      "Validation accuracy: 0.277633\n",
      "Minibatch loss at step 200: 31.228142 with learning rate 0.100000\n",
      "step 200, training accuracy 0.828125\n",
      "Validation accuracy: 0.78495\n",
      "Minibatch loss at step 400: 33.188515 with learning rate 0.095000\n",
      "step 400, training accuracy 0.841797\n",
      "Validation accuracy: 0.7955\n",
      "Minibatch loss at step 600: 24.497917 with learning rate 0.090250\n",
      "step 600, training accuracy 0.849609\n",
      "Validation accuracy: 0.826083\n",
      "Minibatch loss at step 800: 20.878269 with learning rate 0.090250\n",
      "step 800, training accuracy 0.853516\n",
      "Validation accuracy: 0.830217\n",
      "Minibatch loss at step 1000: 19.089355 with learning rate 0.085737\n",
      "step 1000, training accuracy 0.871094\n",
      "Validation accuracy: 0.835483\n",
      "Minibatch loss at step 1200: 17.598488 with learning rate 0.081451\n",
      "step 1200, training accuracy 0.878906\n",
      "Validation accuracy: 0.836467\n",
      "Minibatch loss at step 1400: 16.535933 with learning rate 0.081451\n",
      "step 1400, training accuracy 0.833984\n",
      "Validation accuracy: 0.827533\n",
      "Minibatch loss at step 1600: 16.807833 with learning rate 0.077378\n",
      "step 1600, training accuracy 0.853516\n",
      "Validation accuracy: 0.841717\n",
      "Minibatch loss at step 1800: 17.849846 with learning rate 0.073509\n",
      "step 1800, training accuracy 0.835938\n",
      "Validation accuracy: 0.841933\n",
      "Minibatch loss at step 2000: 12.066371 with learning rate 0.073509\n",
      "step 2000, training accuracy 0.859375\n",
      "Validation accuracy: 0.844233\n",
      "Minibatch loss at step 2200: 10.607289 with learning rate 0.069834\n",
      "step 2200, training accuracy 0.888672\n",
      "Validation accuracy: 0.847733\n",
      "Minibatch loss at step 2400: 9.007179 with learning rate 0.066342\n",
      "step 2400, training accuracy 0.894531\n",
      "Validation accuracy: 0.851633\n",
      "Minibatch loss at step 2600: 6.833298 with learning rate 0.066342\n",
      "step 2600, training accuracy 0.921875\n",
      "Validation accuracy: 0.8524\n",
      "Minibatch loss at step 2800: 8.417318 with learning rate 0.063025\n",
      "step 2800, training accuracy 0.896484\n",
      "Validation accuracy: 0.853033\n",
      "Minibatch loss at step 3000: 7.721565 with learning rate 0.059874\n",
      "step 3000, training accuracy 0.896484\n",
      "Validation accuracy: 0.851733\n",
      "Minibatch loss at step 3200: 7.968235 with learning rate 0.059874\n",
      "step 3200, training accuracy 0.896484\n",
      "Validation accuracy: 0.85245\n",
      "Minibatch loss at step 3400: 9.711945 with learning rate 0.056880\n",
      "step 3400, training accuracy 0.882812\n",
      "Validation accuracy: 0.8519\n",
      "Minibatch loss at step 3600: 7.286077 with learning rate 0.054036\n",
      "step 3600, training accuracy 0.882812\n",
      "Validation accuracy: 0.849683\n",
      "Minibatch loss at step 3800: 7.240571 with learning rate 0.054036\n",
      "step 3800, training accuracy 0.880859\n",
      "Validation accuracy: 0.834883\n",
      "Minibatch loss at step 4000: 7.139401 with learning rate 0.051334\n",
      "step 4000, training accuracy 0.890625\n",
      "Validation accuracy: 0.853533\n",
      "Minibatch loss at step 4200: 5.549515 with learning rate 0.048767\n",
      "step 4200, training accuracy 0.904297\n",
      "Validation accuracy: 0.856433\n",
      "Minibatch loss at step 4400: 7.479745 with learning rate 0.046329\n",
      "step 4400, training accuracy 0.898438\n",
      "Validation accuracy: 0.856067\n",
      "Minibatch loss at step 4600: 5.623536 with learning rate 0.046329\n",
      "step 4600, training accuracy 0.914062\n",
      "Validation accuracy: 0.85085\n",
      "Minibatch loss at step 4800: 6.554952 with learning rate 0.044013\n",
      "step 4800, training accuracy 0.867188\n",
      "Validation accuracy: 0.854183\n",
      "Minibatch loss at step 5000: 5.869461 with learning rate 0.041812\n",
      "step 5000, training accuracy 0.90625\n",
      "Validation accuracy: 0.857533\n",
      "Minibatch loss at step 5200: 5.195463 with learning rate 0.041812\n",
      "step 5200, training accuracy 0.917969\n",
      "Validation accuracy: 0.855767\n",
      "Minibatch loss at step 5400: 4.933799 with learning rate 0.039721\n",
      "step 5400, training accuracy 0.904297\n",
      "Validation accuracy: 0.8481\n",
      "Minibatch loss at step 5600: 4.991116 with learning rate 0.037735\n",
      "step 5600, training accuracy 0.914062\n",
      "Validation accuracy: 0.859133\n",
      "Minibatch loss at step 5800: 5.765675 with learning rate 0.037735\n",
      "step 5800, training accuracy 0.904297\n",
      "Validation accuracy: 0.85745\n",
      "Minibatch loss at step 6000: 4.052460 with learning rate 0.035849\n",
      "step 6000, training accuracy 0.910156\n",
      "Validation accuracy: 0.84075\n",
      "Minibatch loss at step 6200: 4.540936 with learning rate 0.034056\n",
      "step 6200, training accuracy 0.916016\n",
      "Validation accuracy: 0.857033\n",
      "Minibatch loss at step 6400: 5.345493 with learning rate 0.034056\n",
      "step 6400, training accuracy 0.878906\n",
      "Validation accuracy: 0.859117\n",
      "Minibatch loss at step 6600: 4.572281 with learning rate 0.032353\n",
      "step 6600, training accuracy 0.90625\n",
      "Validation accuracy: 0.85965\n",
      "Minibatch loss at step 6800: 2.754115 with learning rate 0.030736\n",
      "step 6800, training accuracy 0.931641\n",
      "Validation accuracy: 0.85955\n",
      "Minibatch loss at step 7000: 4.651890 with learning rate 0.030736\n",
      "step 7000, training accuracy 0.910156\n",
      "Validation accuracy: 0.858983\n",
      "Minibatch loss at step 7200: 5.304098 with learning rate 0.029199\n",
      "step 7200, training accuracy 0.904297\n",
      "Validation accuracy: 0.85965\n",
      "Minibatch loss at step 7400: 3.436081 with learning rate 0.027739\n",
      "step 7400, training accuracy 0.917969\n",
      "Validation accuracy: 0.861033\n",
      "Minibatch loss at step 7600: 3.535626 with learning rate 0.027739\n",
      "step 7600, training accuracy 0.90625\n",
      "Validation accuracy: 0.860567\n",
      "Minibatch loss at step 7800: 5.509851 with learning rate 0.026352\n",
      "step 7800, training accuracy 0.896484\n",
      "Validation accuracy: 0.857283\n",
      "Minibatch loss at step 8000: 3.572270 with learning rate 0.025034\n",
      "step 8000, training accuracy 0.919922\n",
      "Validation accuracy: 0.859617\n",
      "Minibatch loss at step 8200: 3.410087 with learning rate 0.025034\n",
      "step 8200, training accuracy 0.921875\n",
      "Validation accuracy: 0.86095\n",
      "Minibatch loss at step 8400: 3.563816 with learning rate 0.023783\n",
      "step 8400, training accuracy 0.917969\n",
      "Validation accuracy: 0.858617\n",
      "Minibatch loss at step 8600: 4.136174 with learning rate 0.022594\n",
      "step 8600, training accuracy 0.925781\n",
      "Validation accuracy: 0.8603\n",
      "Minibatch loss at step 8800: 4.863126 with learning rate 0.021464\n",
      "step 8800, training accuracy 0.896484\n",
      "Validation accuracy: 0.860183\n",
      "Minibatch loss at step 9000: 5.020451 with learning rate 0.021464\n",
      "step 9000, training accuracy 0.900391\n",
      "Validation accuracy: 0.857433\n",
      "Minibatch loss at step 9200: 4.032941 with learning rate 0.020391\n",
      "step 9200, training accuracy 0.917969\n",
      "Validation accuracy: 0.8611\n",
      "Minibatch loss at step 9400: 4.960226 with learning rate 0.019371\n",
      "step 9400, training accuracy 0.894531\n",
      "Validation accuracy: 0.8597\n",
      "Minibatch loss at step 9600: 4.210440 with learning rate 0.019371\n",
      "step 9600, training accuracy 0.898438\n",
      "Validation accuracy: 0.859517\n",
      "Minibatch loss at step 9800: 3.327087 with learning rate 0.018403\n",
      "step 9800, training accuracy 0.927734\n",
      "Validation accuracy: 0.859833\n",
      "Minibatch loss at step 10000: 3.016731 with learning rate 0.017482\n",
      "step 10000, training accuracy 0.921875\n",
      "Validation accuracy: 0.858583\n",
      "test accuracy 0.920263\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, lr = session.run(\n",
    "      [optimizer, loss, learning_rate], feed_dict=feed_dict)\n",
    "    if (step % 200 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f with learning rate %f\" % (step, l, lr))\n",
    "      train_accuracy = accuracy_l.eval(feed_dict={tf_train_dataset:batch_data, tf_train_labels: batch_labels})\n",
    "      print \"step %d, training accuracy %g\" % (step, train_accuracy)\n",
    "      print(\"Validation accuracy: %g\" % \n",
    "            accuracy_l.eval(feed_dict={ tf_train_dataset: valid_dataset, tf_train_labels: valid_labels}))\n",
    "      #save model\n",
    "      saver.save(session, './model.ckpt', global_step=step)\n",
    "    \n",
    "  print(\"test accuracy %g\" % accuracy_l.eval(feed_dict={ tf_train_dataset: test_dataset, tf_train_labels: test_labels}))\n",
    "  #print(\"Test accuracy: %.1f%%\" % accuracy_l.eval(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.921598\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU0AAAESCAYAAAB5KIfmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f830ca3b4d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.gridspec as gridspec\n",
    "import random\n",
    "\n",
    "def predict_letter(image, model):\n",
    "    with model.as_default():\n",
    "        probs = y_conv.eval(feed_dict={ tf_train_dataset: image, tf_train_labels: [[0,0,0,0,0,0,0,0,0,0]]})\n",
    "    #print probs\n",
    "    return tf.argmax(probs,1).eval() #return index of max value\n",
    "    \n",
    "\n",
    "def display_sample(dataset, labels, title_label, model=None):    \n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.3, hspace=1)\n",
    "    fig = plt.figure()    \n",
    "    fig.suptitle(title_label)    \n",
    "    for d in range(0, 16):\n",
    "        a=fig.add_subplot(gs[d])\n",
    "        letter_idx = int(len(dataset)*random.random())\n",
    "        #print(letter_idx)\n",
    "        plt.imshow(dataset[letter_idx].reshape((28,28)), cmap = cm.Greys_r)\n",
    "        l_title = chr(tf.argmax(labels[letter_idx],0).eval() + ord('A'))\n",
    "        if model is not None:\n",
    "            letter_class = predict_letter(dataset[letter_idx].reshape(1, -1), model)\n",
    "            #print letter_class\n",
    "            l_title = l_title + ' / ~' + chr(letter_class + ord('A'))\n",
    "        a.set_title(l_title)\n",
    "        a.axis('off')    \n",
    "    plt.show()\n",
    "    \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    #load model\n",
    "    ckpt = tf.train.get_checkpoint_state('./')\n",
    "    #print ckpt\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    print(\"test accuracy %g\" % \n",
    "          accuracy_l.eval(feed_dict={ tf_train_dataset: test_dataset, tf_train_labels: test_labels}))\n",
    "    display_sample(test_dataset, test_labels, \"tensorflow 1024h\", sess)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
