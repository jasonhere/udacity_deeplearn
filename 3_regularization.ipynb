{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (469114, 28, 28), (469114,))\n",
      "('Validation set', (60000, 28, 28), (60000,))\n",
      "('Test set', (18724, 28, 28), (18724,))\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNISTfull.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (469114, 784), (469114, 10))\n",
      "('Validation set', (60000, 784), (60000, 10))\n",
      "('Test set', (18724, 784), (18724, 10))\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape, prev_n_count=2):\n",
    "    initial = tf.truncated_normal(shape, stddev=math.sqrt(2.0/prev_n_count))\n",
    "    #initial = tf.truncated_normal(shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.0, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def train_model(num_steps, checkpoint_every, batch_size, graph, is_saving=False, keep_prob = 0.5):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print(\"Initialized\")\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,\n",
    "                         tf_batch_size: batch_size, tf_keep_prob: keep_prob}\n",
    "            _, l, lr = session.run([optimizer, loss, learning_rate], feed_dict=feed_dict)\n",
    "            if (step % checkpoint_every == 0):\n",
    "                print(\"Minibatch loss at step %d: %f with learning rate %f\" % (step, l, lr))\n",
    "                train_accuracy = accuracy_l.eval(feed_dict={tf_train_dataset:batch_data, tf_train_labels: batch_labels,\n",
    "                                                           tf_keep_prob: 1.0})\n",
    "                print \"step %d, training accuracy %g\" % (step, train_accuracy)\n",
    "                print(\"Validation accuracy: %g\" % \n",
    "                    accuracy_l.eval(feed_dict={ tf_train_dataset: valid_dataset, tf_train_labels: valid_labels,\n",
    "                                              tf_keep_prob: 1.0}))\n",
    "                #save model\n",
    "                if(is_saving):\n",
    "                    saver.save(session, './model.ckpt', global_step=step)\n",
    "        print(\"test accuracy %g\" % accuracy_l.eval(feed_dict={ tf_train_dataset: test_dataset,\n",
    "                                                              tf_train_labels: test_labels, tf_keep_prob: 1.0}))\n",
    "        \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.gridspec as gridspec\n",
    "import random\n",
    "\n",
    "def predict_letter(image, session):\n",
    "    with session.as_default():\n",
    "        probs = y_conv.eval(feed_dict={ tf_train_dataset: image, tf_train_labels: [[0,0,0,0,0,0,0,0,0,0]],\n",
    "                                      tf_keep_prob: 1.0})\n",
    "    #print probs\n",
    "    return tf.argmax(probs,1).eval() #return index of max value\n",
    "\n",
    "def find_incorrect(dataset, labels, session):\n",
    "    with session.as_default():\n",
    "        #tensor with True when prediction == label\n",
    "        correct_predicitons = is_correct_prediction.eval(feed_dict={ tf_train_dataset: dataset,\n",
    "                                                                    tf_train_labels: labels, tf_keep_prob: 1.0})\n",
    "    #return indexes of invalid\n",
    "    return np.where(correct_predicitons == False)\n",
    "    \n",
    "\n",
    "def display_sample(dataset, labels, title_label, session=None):    \n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.3, hspace=1)\n",
    "    fig = plt.figure()    \n",
    "    fig.suptitle(title_label, color='white')\n",
    "    fig.patch.set_facecolor('black')\n",
    "    for d in range(0, 16):\n",
    "        a=fig.add_subplot(gs[d])\n",
    "        letter_idx = int(len(dataset)*random.random())\n",
    "        #print(letter_idx)\n",
    "        plt.imshow(dataset[letter_idx].reshape((28,28)), cmap = cm.Greys_r)\n",
    "        l_title = chr(tf.argmax(labels[letter_idx],0).eval() + ord('A'))\n",
    "        if session is not None:\n",
    "            letter_class = predict_letter(dataset[letter_idx].reshape(1, -1), session)\n",
    "            #print letter_class\n",
    "            l_title = l_title + ' / ~' + chr(letter_class + ord('A'))\n",
    "        a.set_title(l_title, color='white')\n",
    "        a.axis('off')    \n",
    "    plt.show()\n",
    "    \n",
    "def display_incorrect_sample(dataset, labels, title_label, num, session=None):    \n",
    "    gs = gridspec.GridSpec(4, int(num / 4) + (0 if num % 4 == 0 else 1))\n",
    "    gs.update(wspace=0.3, hspace=1)\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle(title_label, color='white')\n",
    "    fig.patch.set_facecolor('black')\n",
    "    incorrect = find_incorrect(dataset, labels, session)[0]\n",
    "    no_incorrect = len(incorrect)\n",
    "    print('Incorrect no: %i' % no_incorrect)\n",
    "    #get permutation of incorrects\n",
    "    incorrect = incorrect[np.random.permutation(no_incorrect)]\n",
    "    for d in range(0, num):\n",
    "        a=fig.add_subplot(gs[d])             \n",
    "        letter_idx = incorrect[d]\n",
    "        #print(letter_idx)\n",
    "        plt.imshow(dataset[letter_idx].reshape((28,28)), cmap = cm.Greys_r)\n",
    "        l_title = chr(tf.argmax(labels[letter_idx],0).eval() + ord('A'))\n",
    "        if session is not None:\n",
    "            letter_class = predict_letter(dataset[letter_idx].reshape(1, -1), session)\n",
    "            #print letter_class\n",
    "            l_title = l_title + ' / ~' + chr(letter_class + ord('A'))\n",
    "        a.set_title(l_title, color='white')\n",
    "        a.axis('off')    \n",
    "    plt.show()\n",
    "    \n",
    "def restore_and_display_test(graph, title):\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        #load model\n",
    "        ckpt = tf.train.get_checkpoint_state('./')\n",
    "        #print ckpt\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print(\"test accuracy %g\" % \n",
    "              accuracy_l.eval(feed_dict={ tf_train_dataset: test_dataset, tf_train_labels: test_labels,\n",
    "                                        tf_keep_prob: 1.0}))\n",
    "        display_sample(test_dataset, test_labels, title, sess)\n",
    "        \n",
    "def restore_and_display_random_incorrect(num_samples, graph, title):\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        #load model\n",
    "        ckpt = tf.train.get_checkpoint_state('./')\n",
    "        #print ckpt\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print(\"test accuracy %g\" % \n",
    "              accuracy_l.eval(feed_dict={ tf_train_dataset: test_dataset, tf_train_labels: test_labels,\n",
    "                                        tf_keep_prob: 1.0}))\n",
    "        display_incorrect_sample(test_dataset, test_labels, title, num_samples, sess)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compue the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4cc01db5b00c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mhidden_nodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1024\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Input data. For the training data, we use a placeholder that will be fed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(None, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "    tf_batch_size = tf.placeholder(tf.int32)\n",
    "\n",
    "    # input layer variables\n",
    "    weights_i = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "    biases_i = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    \n",
    "    #hidden layer\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_i) + biases_i)\n",
    "\n",
    "    # output layer variables\n",
    "    weights_o = tf.Variable(tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "    biases_o = tf.Variable(tf.zeros([num_labels]))\n",
    "    #output operation\n",
    "    logits_o = tf.matmul(h_fc1, weights_o) + biases_o\n",
    "    y_conv = tf.nn.softmax(logits_o) #after softmax -> output probabilities / predictions\n",
    "    #define loss    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits_o, tf_train_labels) +\n",
    "                          0.01*tf.nn.l2_loss(weights_i))                \n",
    "    #decay learning rate\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        0.1, global_step * tf_batch_size, train_dataset.shape[0] / 2, 0.95, staircase=True)\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    #define operations to measure accuracy\n",
    "    is_correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(tf_train_labels,1))\n",
    "    accuracy_l = tf.reduce_mean(tf.cast(is_correct_prediction, tf.float32))\n",
    "    #saving mode\n",
    "    saver = tf.train.Saver(max_to_keep=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3487.843262 with learning rate 0.100000\n",
      "step 0, training accuracy 0.3125\n",
      "Validation accuracy: 0.2586\n",
      "Minibatch loss at step 100: 2563.161133 with learning rate 0.100000\n",
      "step 100, training accuracy 0.845703\n",
      "Validation accuracy: 0.770633\n",
      "Minibatch loss at step 200: 2098.815430 with learning rate 0.100000\n",
      "step 200, training accuracy 0.8125\n",
      "Validation accuracy: 0.77995\n",
      "Minibatch loss at step 300: 1717.917236 with learning rate 0.095000\n",
      "step 300, training accuracy 0.791016\n",
      "Validation accuracy: 0.797233\n",
      "Minibatch loss at step 400: 1416.304565 with learning rate 0.095000\n",
      "step 400, training accuracy 0.857422\n",
      "Validation accuracy: 0.808917\n",
      "Minibatch loss at step 500: 1170.868896 with learning rate 0.095000\n",
      "step 500, training accuracy 0.830078\n",
      "Validation accuracy: 0.8048\n",
      "Minibatch loss at step 600: 968.820679 with learning rate 0.090250\n",
      "step 600, training accuracy 0.845703\n",
      "Validation accuracy: 0.8212\n",
      "Minibatch loss at step 700: 808.045776 with learning rate 0.090250\n",
      "step 700, training accuracy 0.875\n",
      "Validation accuracy: 0.82205\n",
      "Minibatch loss at step 800: 674.671448 with learning rate 0.090250\n",
      "step 800, training accuracy 0.873047\n",
      "Validation accuracy: 0.82525\n",
      "Minibatch loss at step 900: 563.500305 with learning rate 0.085737\n",
      "step 900, training accuracy 0.880859\n",
      "Validation accuracy: 0.828033\n",
      "Minibatch loss at step 1000: 474.658905 with learning rate 0.085737\n",
      "step 1000, training accuracy 0.880859\n",
      "Validation accuracy: 0.825367\n",
      "Minibatch loss at step 1100: 400.002930 with learning rate 0.085737\n",
      "step 1100, training accuracy 0.855469\n",
      "Validation accuracy: 0.828267\n",
      "Minibatch loss at step 1200: 338.367737 with learning rate 0.081451\n",
      "step 1200, training accuracy 0.882812\n",
      "Validation accuracy: 0.836967\n",
      "Minibatch loss at step 1300: 286.815247 with learning rate 0.081451\n",
      "step 1300, training accuracy 0.871094\n",
      "Validation accuracy: 0.836867\n",
      "Minibatch loss at step 1400: 244.200226 with learning rate 0.081451\n",
      "step 1400, training accuracy 0.851562\n",
      "Validation accuracy: 0.822483\n",
      "Minibatch loss at step 1500: 207.141785 with learning rate 0.077378\n",
      "step 1500, training accuracy 0.90625\n",
      "Validation accuracy: 0.837517\n",
      "Minibatch loss at step 1600: 178.198715 with learning rate 0.077378\n",
      "step 1600, training accuracy 0.878906\n",
      "Validation accuracy: 0.835067\n",
      "Minibatch loss at step 1700: 151.585815 with learning rate 0.077378\n",
      "step 1700, training accuracy 0.894531\n",
      "Validation accuracy: 0.8359\n",
      "Minibatch loss at step 1800: 131.843201 with learning rate 0.073509\n",
      "step 1800, training accuracy 0.869141\n",
      "Validation accuracy: 0.843817\n",
      "Minibatch loss at step 1900: 112.869110 with learning rate 0.073509\n",
      "step 1900, training accuracy 0.900391\n",
      "Validation accuracy: 0.851917\n",
      "Minibatch loss at step 2000: 97.559227 with learning rate 0.073509\n",
      "step 2000, training accuracy 0.925781\n",
      "Validation accuracy: 0.848983\n",
      "Minibatch loss at step 2100: 84.435417 with learning rate 0.069834\n",
      "step 2100, training accuracy 0.908203\n",
      "Validation accuracy: 0.835517\n",
      "Minibatch loss at step 2200: 73.739410 with learning rate 0.069834\n",
      "step 2200, training accuracy 0.902344\n",
      "Validation accuracy: 0.8479\n",
      "Minibatch loss at step 2300: 63.971161 with learning rate 0.069834\n",
      "step 2300, training accuracy 0.917969\n",
      "Validation accuracy: 0.854483\n",
      "Minibatch loss at step 2400: 55.973824 with learning rate 0.066342\n",
      "step 2400, training accuracy 0.910156\n",
      "Validation accuracy: 0.853217\n",
      "Minibatch loss at step 2500: 48.974663 with learning rate 0.066342\n",
      "step 2500, training accuracy 0.912109\n",
      "Validation accuracy: 0.858683\n",
      "Minibatch loss at step 2600: 42.666615 with learning rate 0.066342\n",
      "step 2600, training accuracy 0.935547\n",
      "Validation accuracy: 0.854133\n",
      "Minibatch loss at step 2700: 37.712460 with learning rate 0.063025\n",
      "step 2700, training accuracy 0.919922\n",
      "Validation accuracy: 0.8619\n",
      "Minibatch loss at step 2800: 33.248962 with learning rate 0.063025\n",
      "step 2800, training accuracy 0.939453\n",
      "Validation accuracy: 0.860617\n",
      "Minibatch loss at step 2900: 29.531359 with learning rate 0.063025\n",
      "step 2900, training accuracy 0.871094\n",
      "Validation accuracy: 0.815217\n",
      "Minibatch loss at step 3000: 25.926941 with learning rate 0.059874\n",
      "step 3000, training accuracy 0.939453\n",
      "Validation accuracy: 0.86345\n",
      "Minibatch loss at step 3100: 22.985088 with learning rate 0.059874\n",
      "step 3100, training accuracy 0.966797\n",
      "Validation accuracy: 0.865683\n",
      "Minibatch loss at step 3200: 20.489851 with learning rate 0.059874\n",
      "step 3200, training accuracy 0.945312\n",
      "Validation accuracy: 0.864817\n",
      "Minibatch loss at step 3300: 18.409039 with learning rate 0.056880\n",
      "step 3300, training accuracy 0.927734\n",
      "Validation accuracy: 0.864117\n",
      "Minibatch loss at step 3400: 16.725784 with learning rate 0.056880\n",
      "step 3400, training accuracy 0.929688\n",
      "Validation accuracy: 0.854067\n",
      "Minibatch loss at step 3500: 14.821609 with learning rate 0.056880\n",
      "step 3500, training accuracy 0.929688\n",
      "Validation accuracy: 0.868733\n",
      "Minibatch loss at step 3600: 13.161375 with learning rate 0.054036\n",
      "step 3600, training accuracy 0.927734\n",
      "Validation accuracy: 0.861283\n",
      "Minibatch loss at step 3700: 11.748416 with learning rate 0.054036\n",
      "step 3700, training accuracy 0.958984\n",
      "Validation accuracy: 0.877233\n",
      "Minibatch loss at step 3800: 10.680598 with learning rate 0.054036\n",
      "step 3800, training accuracy 0.912109\n",
      "Validation accuracy: 0.841083\n",
      "Minibatch loss at step 3900: 9.693853 with learning rate 0.051334\n",
      "step 3900, training accuracy 0.962891\n",
      "Validation accuracy: 0.874083\n",
      "Minibatch loss at step 4000: 8.734142 with learning rate 0.051334\n",
      "step 4000, training accuracy 0.957031\n",
      "Validation accuracy: 0.877533\n",
      "Minibatch loss at step 4100: 7.868310 with learning rate 0.051334\n",
      "step 4100, training accuracy 0.964844\n",
      "Validation accuracy: 0.88155\n",
      "Minibatch loss at step 4200: 7.175800 with learning rate 0.048767\n",
      "step 4200, training accuracy 0.970703\n",
      "Validation accuracy: 0.879933\n",
      "Minibatch loss at step 4300: 6.514223 with learning rate 0.048767\n",
      "step 4300, training accuracy 0.964844\n",
      "Validation accuracy: 0.883533\n",
      "Minibatch loss at step 4400: 6.010068 with learning rate 0.046329\n",
      "step 4400, training accuracy 0.960938\n",
      "Validation accuracy: 0.885267\n",
      "Minibatch loss at step 4500: 5.419532 with learning rate 0.046329\n",
      "step 4500, training accuracy 0.970703\n",
      "Validation accuracy: 0.886333\n",
      "Minibatch loss at step 4600: 4.960341 with learning rate 0.046329\n",
      "step 4600, training accuracy 0.974609\n",
      "Validation accuracy: 0.886683\n",
      "Minibatch loss at step 4700: 4.617253 with learning rate 0.044013\n",
      "step 4700, training accuracy 0.953125\n",
      "Validation accuracy: 0.884367\n",
      "Minibatch loss at step 4800: 4.306877 with learning rate 0.044013\n",
      "step 4800, training accuracy 0.945312\n",
      "Validation accuracy: 0.88665\n",
      "Minibatch loss at step 4900: 3.941675 with learning rate 0.044013\n",
      "step 4900, training accuracy 0.960938\n",
      "Validation accuracy: 0.888683\n",
      "Minibatch loss at step 5000: 3.588850 with learning rate 0.041812\n",
      "step 5000, training accuracy 0.953125\n",
      "Validation accuracy: 0.89205\n",
      "Minibatch loss at step 5100: 3.321170 with learning rate 0.041812\n",
      "step 5100, training accuracy 0.972656\n",
      "Validation accuracy: 0.892083\n",
      "Minibatch loss at step 5200: 3.037947 with learning rate 0.041812\n",
      "step 5200, training accuracy 0.978516\n",
      "Validation accuracy: 0.895583\n",
      "Minibatch loss at step 5300: 2.865083 with learning rate 0.039721\n",
      "step 5300, training accuracy 0.962891\n",
      "Validation accuracy: 0.8938\n",
      "Minibatch loss at step 5400: 2.660676 with learning rate 0.039721\n",
      "step 5400, training accuracy 0.974609\n",
      "Validation accuracy: 0.890733\n",
      "Minibatch loss at step 5500: 2.480437 with learning rate 0.039721\n",
      "step 5500, training accuracy 0.976562\n",
      "Validation accuracy: 0.894883\n",
      "Minibatch loss at step 5600: 2.275465 with learning rate 0.037735\n",
      "step 5600, training accuracy 0.978516\n",
      "Validation accuracy: 0.896683\n",
      "Minibatch loss at step 5700: 2.150661 with learning rate 0.037735\n",
      "step 5700, training accuracy 0.980469\n",
      "Validation accuracy: 0.896067\n",
      "Minibatch loss at step 5800: 2.037137 with learning rate 0.037735\n",
      "step 5800, training accuracy 0.976562\n",
      "Validation accuracy: 0.8989\n",
      "Minibatch loss at step 5900: 1.913220 with learning rate 0.035849\n",
      "step 5900, training accuracy 0.972656\n",
      "Validation accuracy: 0.897533\n",
      "Minibatch loss at step 6000: 1.859454 with learning rate 0.035849\n",
      "step 6000, training accuracy 0.966797\n",
      "Validation accuracy: 0.897183\n",
      "Minibatch loss at step 6100: 1.741983 with learning rate 0.035849\n",
      "step 6100, training accuracy 0.964844\n",
      "Validation accuracy: 0.896583\n",
      "Minibatch loss at step 6200: 1.633204 with learning rate 0.034056\n",
      "step 6200, training accuracy 0.972656\n",
      "Validation accuracy: 0.899267\n",
      "Minibatch loss at step 6300: 1.513303 with learning rate 0.034056\n",
      "step 6300, training accuracy 0.972656\n",
      "Validation accuracy: 0.90075\n",
      "Minibatch loss at step 6400: 1.478370 with learning rate 0.034056\n",
      "step 6400, training accuracy 0.96875\n",
      "Validation accuracy: 0.900633\n",
      "Minibatch loss at step 6500: 1.388475 with learning rate 0.032353\n",
      "step 6500, training accuracy 0.953125\n",
      "Validation accuracy: 0.90095\n",
      "Minibatch loss at step 6600: 1.315152 with learning rate 0.032353\n",
      "step 6600, training accuracy 0.958984\n",
      "Validation accuracy: 0.902467\n",
      "Minibatch loss at step 6700: 1.228055 with learning rate 0.032353\n",
      "step 6700, training accuracy 0.976562\n",
      "Validation accuracy: 0.901767\n",
      "Minibatch loss at step 6800: 1.135897 with learning rate 0.030736\n",
      "step 6800, training accuracy 0.976562\n",
      "Validation accuracy: 0.90155\n",
      "Minibatch loss at step 6900: 1.123712 with learning rate 0.030736\n",
      "step 6900, training accuracy 0.976562\n",
      "Validation accuracy: 0.901583\n",
      "Minibatch loss at step 7000: 1.076858 with learning rate 0.030736\n",
      "step 7000, training accuracy 0.982422\n",
      "Validation accuracy: 0.903283\n",
      "Minibatch loss at step 7100: 1.008029 with learning rate 0.029199\n",
      "step 7100, training accuracy 0.974609\n",
      "Validation accuracy: 0.904183\n",
      "Minibatch loss at step 7200: 1.006288 with learning rate 0.029199\n",
      "step 7200, training accuracy 0.976562\n",
      "Validation accuracy: 0.902883\n",
      "Minibatch loss at step 7300: 1.012623 with learning rate 0.029199\n",
      "step 7300, training accuracy 0.962891\n",
      "Validation accuracy: 0.9045\n",
      "Minibatch loss at step 7400: 0.912900 with learning rate 0.027739\n",
      "step 7400, training accuracy 0.972656\n",
      "Validation accuracy: 0.90425\n",
      "Minibatch loss at step 7500: 0.889981 with learning rate 0.027739\n",
      "step 7500, training accuracy 0.972656\n",
      "Validation accuracy: 0.902917\n",
      "Minibatch loss at step 7600: 0.868000 with learning rate 0.027739\n",
      "step 7600, training accuracy 0.96875\n",
      "Validation accuracy: 0.904833\n",
      "Minibatch loss at step 7700: 0.810815 with learning rate 0.026352\n",
      "step 7700, training accuracy 0.972656\n",
      "Validation accuracy: 0.904033\n",
      "Minibatch loss at step 7800: 0.847017 with learning rate 0.026352\n",
      "step 7800, training accuracy 0.960938\n",
      "Validation accuracy: 0.90425\n",
      "Minibatch loss at step 7900: 0.763301 with learning rate 0.026352\n",
      "step 7900, training accuracy 0.976562\n",
      "Validation accuracy: 0.90475\n",
      "Minibatch loss at step 8000: 0.768085 with learning rate 0.025034\n",
      "step 8000, training accuracy 0.972656\n",
      "Validation accuracy: 0.904783\n",
      "Minibatch loss at step 8100: 0.736596 with learning rate 0.025034\n",
      "step 8100, training accuracy 0.964844\n",
      "Validation accuracy: 0.906\n",
      "Minibatch loss at step 8200: 0.725602 with learning rate 0.025034\n",
      "step 8200, training accuracy 0.966797\n",
      "Validation accuracy: 0.9078\n",
      "Minibatch loss at step 8300: 0.749483 with learning rate 0.023783\n",
      "step 8300, training accuracy 0.958984\n",
      "Validation accuracy: 0.9052\n",
      "Minibatch loss at step 8400: 0.699482 with learning rate 0.023783\n",
      "step 8400, training accuracy 0.976562\n",
      "Validation accuracy: 0.905367\n",
      "Minibatch loss at step 8500: 0.651334 with learning rate 0.022594\n",
      "step 8500, training accuracy 0.970703\n",
      "Validation accuracy: 0.90685\n",
      "Minibatch loss at step 8600: 0.639117 with learning rate 0.022594\n",
      "step 8600, training accuracy 0.960938\n",
      "Validation accuracy: 0.906183\n",
      "Minibatch loss at step 8700: 0.624448 with learning rate 0.022594\n",
      "step 8700, training accuracy 0.96875\n",
      "Validation accuracy: 0.906533\n",
      "Minibatch loss at step 8800: 0.660529 with learning rate 0.021464\n",
      "step 8800, training accuracy 0.945312\n",
      "Validation accuracy: 0.907183\n",
      "Minibatch loss at step 8900: 0.622697 with learning rate 0.021464\n",
      "step 8900, training accuracy 0.962891\n",
      "Validation accuracy: 0.906667\n",
      "Minibatch loss at step 9000: 0.624392 with learning rate 0.021464\n",
      "step 9000, training accuracy 0.962891\n",
      "Validation accuracy: 0.906283\n",
      "Minibatch loss at step 9100: 0.631837 with learning rate 0.020391\n",
      "step 9100, training accuracy 0.953125\n",
      "Validation accuracy: 0.908\n",
      "Minibatch loss at step 9200: 0.570817 with learning rate 0.020391\n",
      "step 9200, training accuracy 0.962891\n",
      "Validation accuracy: 0.908117\n",
      "Minibatch loss at step 9300: 0.571789 with learning rate 0.020391\n",
      "step 9300, training accuracy 0.962891\n",
      "Validation accuracy: 0.908133\n",
      "Minibatch loss at step 9400: 0.589305 with learning rate 0.019371\n",
      "step 9400, training accuracy 0.964844\n",
      "Validation accuracy: 0.90885\n",
      "Minibatch loss at step 9500: 0.519380 with learning rate 0.019371\n",
      "step 9500, training accuracy 0.964844\n",
      "Validation accuracy: 0.907067\n",
      "Minibatch loss at step 9600: 0.579069 with learning rate 0.019371\n",
      "step 9600, training accuracy 0.964844\n",
      "Validation accuracy: 0.907983\n",
      "Minibatch loss at step 9700: 0.535438 with learning rate 0.018403\n",
      "step 9700, training accuracy 0.964844\n",
      "Validation accuracy: 0.909083\n",
      "Minibatch loss at step 9800: 0.475927 with learning rate 0.018403\n",
      "step 9800, training accuracy 0.966797\n",
      "Validation accuracy: 0.90815\n",
      "Minibatch loss at step 9900: 0.475779 with learning rate 0.018403\n",
      "step 9900, training accuracy 0.96875\n",
      "Validation accuracy: 0.908917\n",
      "Minibatch loss at step 10000: 0.517845 with learning rate 0.017482\n",
      "step 10000, training accuracy 0.953125\n",
      "Validation accuracy: 0.9097\n",
      "Minibatch loss at step 10100: 0.478314 with learning rate 0.017482\n",
      "step 10100, training accuracy 0.974609\n",
      "Validation accuracy: 0.908583\n",
      "Minibatch loss at step 10200: 0.480462 with learning rate 0.017482\n",
      "step 10200, training accuracy 0.958984\n",
      "Validation accuracy: 0.909567\n",
      "Minibatch loss at step 10300: 0.462185 with learning rate 0.016608\n",
      "step 10300, training accuracy 0.96875\n",
      "Validation accuracy: 0.90935\n",
      "Minibatch loss at step 10400: 0.480129 with learning rate 0.016608\n",
      "step 10400, training accuracy 0.964844\n",
      "Validation accuracy: 0.908367\n",
      "Minibatch loss at step 10500: 0.441055 with learning rate 0.016608\n",
      "step 10500, training accuracy 0.982422\n",
      "Validation accuracy: 0.90965\n",
      "Minibatch loss at step 10600: 0.436472 with learning rate 0.015778\n",
      "step 10600, training accuracy 0.970703\n",
      "Validation accuracy: 0.90945\n",
      "Minibatch loss at step 10700: 0.427860 with learning rate 0.015778\n",
      "step 10700, training accuracy 0.960938\n",
      "Validation accuracy: 0.909667\n",
      "Minibatch loss at step 10800: 0.420727 with learning rate 0.015778\n",
      "step 10800, training accuracy 0.970703\n",
      "Validation accuracy: 0.910183\n",
      "Minibatch loss at step 10900: 0.451730 with learning rate 0.014989\n",
      "step 10900, training accuracy 0.960938\n",
      "Validation accuracy: 0.90895\n",
      "Minibatch loss at step 11000: 0.476226 with learning rate 0.014989\n",
      "step 11000, training accuracy 0.955078\n",
      "Validation accuracy: 0.909483\n",
      "Minibatch loss at step 11100: 0.444657 with learning rate 0.014989\n",
      "step 11100, training accuracy 0.951172\n",
      "Validation accuracy: 0.90965\n",
      "Minibatch loss at step 11200: 0.411951 with learning rate 0.014240\n",
      "step 11200, training accuracy 0.960938\n",
      "Validation accuracy: 0.909783\n",
      "Minibatch loss at step 11300: 0.460955 with learning rate 0.014240\n",
      "step 11300, training accuracy 0.947266\n",
      "Validation accuracy: 0.909967\n",
      "Minibatch loss at step 11400: 0.406603 with learning rate 0.014240\n",
      "step 11400, training accuracy 0.96875\n",
      "Validation accuracy: 0.9097\n",
      "Minibatch loss at step 11500: 0.464542 with learning rate 0.013528\n",
      "step 11500, training accuracy 0.943359\n",
      "Validation accuracy: 0.910867\n",
      "Minibatch loss at step 11600: 0.428620 with learning rate 0.013528\n",
      "step 11600, training accuracy 0.958984\n",
      "Validation accuracy: 0.909733\n",
      "Minibatch loss at step 11700: 0.421634 with learning rate 0.013528\n",
      "step 11700, training accuracy 0.964844\n",
      "Validation accuracy: 0.90975\n",
      "Minibatch loss at step 11800: 0.396700 with learning rate 0.012851\n",
      "step 11800, training accuracy 0.972656\n",
      "Validation accuracy: 0.910467\n",
      "Minibatch loss at step 11900: 0.394740 with learning rate 0.012851\n",
      "step 11900, training accuracy 0.972656\n",
      "Validation accuracy: 0.909817\n",
      "Minibatch loss at step 12000: 0.485540 with learning rate 0.012851\n",
      "step 12000, training accuracy 0.947266\n",
      "Validation accuracy: 0.910367\n",
      "Minibatch loss at step 12100: 0.426045 with learning rate 0.012209\n",
      "step 12100, training accuracy 0.953125\n",
      "Validation accuracy: 0.910783\n",
      "Minibatch loss at step 12200: 0.384099 with learning rate 0.012209\n",
      "step 12200, training accuracy 0.964844\n",
      "Validation accuracy: 0.910167\n",
      "Minibatch loss at step 12300: 0.399832 with learning rate 0.012209\n",
      "step 12300, training accuracy 0.962891\n",
      "Validation accuracy: 0.911633\n",
      "Minibatch loss at step 12400: 0.417892 with learning rate 0.011598\n",
      "step 12400, training accuracy 0.962891\n",
      "Validation accuracy: 0.911217\n",
      "Minibatch loss at step 12500: 0.389890 with learning rate 0.011598\n",
      "step 12500, training accuracy 0.957031\n",
      "Validation accuracy: 0.910567\n",
      "Minibatch loss at step 12600: 0.396094 with learning rate 0.011018\n",
      "step 12600, training accuracy 0.962891\n",
      "Validation accuracy: 0.91015\n",
      "Minibatch loss at step 12700: 0.353822 with learning rate 0.011018\n",
      "step 12700, training accuracy 0.964844\n",
      "Validation accuracy: 0.910517\n",
      "Minibatch loss at step 12800: 0.389704 with learning rate 0.011018\n",
      "step 12800, training accuracy 0.958984\n",
      "Validation accuracy: 0.911067\n",
      "Minibatch loss at step 12900: 0.438974 with learning rate 0.010467\n",
      "step 12900, training accuracy 0.935547\n",
      "Validation accuracy: 0.9112\n",
      "Minibatch loss at step 13000: 0.412860 with learning rate 0.010467\n",
      "step 13000, training accuracy 0.957031\n",
      "Validation accuracy: 0.911433\n",
      "Minibatch loss at step 13100: 0.359804 with learning rate 0.010467\n",
      "step 13100, training accuracy 0.962891\n",
      "Validation accuracy: 0.911483\n",
      "Minibatch loss at step 13200: 0.368828 with learning rate 0.009944\n",
      "step 13200, training accuracy 0.953125\n",
      "Validation accuracy: 0.911617\n",
      "Minibatch loss at step 13300: 0.382264 with learning rate 0.009944\n",
      "step 13300, training accuracy 0.951172\n",
      "Validation accuracy: 0.9113\n",
      "Minibatch loss at step 13400: 0.379108 with learning rate 0.009944\n",
      "step 13400, training accuracy 0.955078\n",
      "Validation accuracy: 0.910717\n",
      "Minibatch loss at step 13500: 0.396936 with learning rate 0.009447\n",
      "step 13500, training accuracy 0.945312\n",
      "Validation accuracy: 0.91175\n",
      "Minibatch loss at step 13600: 0.319294 with learning rate 0.009447\n",
      "step 13600, training accuracy 0.976562\n",
      "Validation accuracy: 0.91155\n",
      "Minibatch loss at step 13700: 0.433796 with learning rate 0.009447\n",
      "step 13700, training accuracy 0.9375\n",
      "Validation accuracy: 0.911567\n",
      "Minibatch loss at step 13800: 0.358423 with learning rate 0.008974\n",
      "step 13800, training accuracy 0.966797\n",
      "Validation accuracy: 0.9112\n",
      "Minibatch loss at step 13900: 0.443023 with learning rate 0.008974\n",
      "step 13900, training accuracy 0.9375\n",
      "Validation accuracy: 0.91085\n",
      "Minibatch loss at step 14000: 0.369621 with learning rate 0.008974\n",
      "step 14000, training accuracy 0.953125\n",
      "Validation accuracy: 0.911733\n",
      "Minibatch loss at step 14100: 0.358884 with learning rate 0.008526\n",
      "step 14100, training accuracy 0.957031\n",
      "Validation accuracy: 0.91155\n",
      "Minibatch loss at step 14200: 0.357084 with learning rate 0.008526\n",
      "step 14200, training accuracy 0.958984\n",
      "Validation accuracy: 0.912067\n",
      "Minibatch loss at step 14300: 0.367450 with learning rate 0.008526\n",
      "step 14300, training accuracy 0.949219\n",
      "Validation accuracy: 0.9112\n",
      "Minibatch loss at step 14400: 0.373747 with learning rate 0.008099\n",
      "step 14400, training accuracy 0.955078\n",
      "Validation accuracy: 0.911883\n",
      "Minibatch loss at step 14500: 0.320755 with learning rate 0.008099\n",
      "step 14500, training accuracy 0.964844\n",
      "Validation accuracy: 0.911233\n",
      "Minibatch loss at step 14600: 0.335396 with learning rate 0.008099\n",
      "step 14600, training accuracy 0.976562\n",
      "Validation accuracy: 0.9115\n",
      "Minibatch loss at step 14700: 0.362962 with learning rate 0.007694\n",
      "step 14700, training accuracy 0.951172\n",
      "Validation accuracy: 0.9121\n",
      "Minibatch loss at step 14800: 0.324191 with learning rate 0.007694\n",
      "step 14800, training accuracy 0.957031\n",
      "Validation accuracy: 0.911817\n",
      "Minibatch loss at step 14900: 0.365243 with learning rate 0.007694\n",
      "step 14900, training accuracy 0.953125\n",
      "Validation accuracy: 0.91145\n",
      "Minibatch loss at step 15000: 0.355124 with learning rate 0.007310\n",
      "step 15000, training accuracy 0.955078\n",
      "Validation accuracy: 0.912683\n",
      "Minibatch loss at step 15100: 0.371319 with learning rate 0.007310\n",
      "step 15100, training accuracy 0.957031\n",
      "Validation accuracy: 0.911683\n",
      "Minibatch loss at step 15200: 0.399911 with learning rate 0.007310\n",
      "step 15200, training accuracy 0.941406\n",
      "Validation accuracy: 0.91235\n",
      "Minibatch loss at step 15300: 0.365061 with learning rate 0.006944\n",
      "step 15300, training accuracy 0.951172\n",
      "Validation accuracy: 0.9122\n",
      "Minibatch loss at step 15400: 0.371102 with learning rate 0.006944\n",
      "step 15400, training accuracy 0.955078\n",
      "Validation accuracy: 0.912367\n",
      "Minibatch loss at step 15500: 0.362076 with learning rate 0.006944\n",
      "step 15500, training accuracy 0.955078\n",
      "Validation accuracy: 0.912533\n",
      "Minibatch loss at step 15600: 0.415356 with learning rate 0.006597\n",
      "step 15600, training accuracy 0.933594\n",
      "Validation accuracy: 0.912233\n",
      "Minibatch loss at step 15700: 0.304652 with learning rate 0.006597\n",
      "step 15700, training accuracy 0.970703\n",
      "Validation accuracy: 0.9118\n",
      "Minibatch loss at step 15800: 0.354772 with learning rate 0.006597\n",
      "step 15800, training accuracy 0.947266\n",
      "Validation accuracy: 0.912717\n",
      "Minibatch loss at step 15900: 0.337934 with learning rate 0.006267\n",
      "step 15900, training accuracy 0.949219\n",
      "Validation accuracy: 0.912483\n",
      "Minibatch loss at step 16000: 0.327403 with learning rate 0.006267\n",
      "step 16000, training accuracy 0.953125\n",
      "Validation accuracy: 0.91255\n",
      "Minibatch loss at step 16100: 0.373662 with learning rate 0.006267\n",
      "step 16100, training accuracy 0.945312\n",
      "Validation accuracy: 0.912317\n",
      "Minibatch loss at step 16200: 0.358154 with learning rate 0.005954\n",
      "step 16200, training accuracy 0.949219\n",
      "Validation accuracy: 0.9131\n",
      "Minibatch loss at step 16300: 0.333123 with learning rate 0.005954\n",
      "step 16300, training accuracy 0.951172\n",
      "Validation accuracy: 0.91245\n",
      "Minibatch loss at step 16400: 0.392826 with learning rate 0.005954\n",
      "step 16400, training accuracy 0.931641\n",
      "Validation accuracy: 0.91275\n",
      "Minibatch loss at step 16500: 0.287027 with learning rate 0.005656\n",
      "step 16500, training accuracy 0.960938\n",
      "Validation accuracy: 0.9126\n",
      "Minibatch loss at step 16600: 0.331868 with learning rate 0.005656\n",
      "step 16600, training accuracy 0.962891\n",
      "Validation accuracy: 0.9122\n",
      "Minibatch loss at step 16700: 0.304451 with learning rate 0.005373\n",
      "step 16700, training accuracy 0.964844\n",
      "Validation accuracy: 0.912867\n",
      "Minibatch loss at step 16800: 0.380349 with learning rate 0.005373\n",
      "step 16800, training accuracy 0.933594\n",
      "Validation accuracy: 0.91245\n",
      "Minibatch loss at step 16900: 0.338642 with learning rate 0.005373\n",
      "step 16900, training accuracy 0.949219\n",
      "Validation accuracy: 0.912383\n",
      "Minibatch loss at step 17000: 0.347909 with learning rate 0.005105\n",
      "step 17000, training accuracy 0.945312\n",
      "Validation accuracy: 0.9132\n",
      "Minibatch loss at step 17100: 0.356829 with learning rate 0.005105\n",
      "step 17100, training accuracy 0.933594\n",
      "Validation accuracy: 0.912483\n",
      "Minibatch loss at step 17200: 0.319076 with learning rate 0.005105\n",
      "step 17200, training accuracy 0.958984\n",
      "Validation accuracy: 0.912933\n",
      "Minibatch loss at step 17300: 0.326298 with learning rate 0.004849\n",
      "step 17300, training accuracy 0.951172\n",
      "Validation accuracy: 0.912567\n",
      "Minibatch loss at step 17400: 0.348811 with learning rate 0.004849\n",
      "step 17400, training accuracy 0.943359\n",
      "Validation accuracy: 0.9126\n",
      "Minibatch loss at step 17500: 0.361977 with learning rate 0.004849\n",
      "step 17500, training accuracy 0.939453\n",
      "Validation accuracy: 0.912717\n",
      "Minibatch loss at step 17600: 0.327118 with learning rate 0.004607\n",
      "step 17600, training accuracy 0.960938\n",
      "Validation accuracy: 0.912317\n",
      "Minibatch loss at step 17700: 0.314177 with learning rate 0.004607\n",
      "step 17700, training accuracy 0.955078\n",
      "Validation accuracy: 0.91265\n",
      "Minibatch loss at step 17800: 0.340412 with learning rate 0.004607\n",
      "step 17800, training accuracy 0.951172\n",
      "Validation accuracy: 0.91325\n",
      "Minibatch loss at step 17900: 0.341627 with learning rate 0.004377\n",
      "step 17900, training accuracy 0.947266\n",
      "Validation accuracy: 0.9125\n",
      "Minibatch loss at step 18000: 0.356809 with learning rate 0.004377\n",
      "step 18000, training accuracy 0.9375\n",
      "Validation accuracy: 0.912683\n",
      "Minibatch loss at step 18100: 0.353924 with learning rate 0.004377\n",
      "step 18100, training accuracy 0.943359\n",
      "Validation accuracy: 0.912283\n",
      "Minibatch loss at step 18200: 0.357389 with learning rate 0.004158\n",
      "step 18200, training accuracy 0.945312\n",
      "Validation accuracy: 0.913\n",
      "Minibatch loss at step 18300: 0.306890 with learning rate 0.004158\n",
      "step 18300, training accuracy 0.958984\n",
      "Validation accuracy: 0.913017\n",
      "Minibatch loss at step 18400: 0.328781 with learning rate 0.004158\n",
      "step 18400, training accuracy 0.947266\n",
      "Validation accuracy: 0.912733\n",
      "Minibatch loss at step 18500: 0.329200 with learning rate 0.003950\n",
      "step 18500, training accuracy 0.949219\n",
      "Validation accuracy: 0.913233\n",
      "Minibatch loss at step 18600: 0.345388 with learning rate 0.003950\n",
      "step 18600, training accuracy 0.957031\n",
      "Validation accuracy: 0.912583\n",
      "Minibatch loss at step 18700: 0.317516 with learning rate 0.003950\n",
      "step 18700, training accuracy 0.955078\n",
      "Validation accuracy: 0.91355\n",
      "Minibatch loss at step 18800: 0.322227 with learning rate 0.003752\n",
      "step 18800, training accuracy 0.953125\n",
      "Validation accuracy: 0.913167\n",
      "Minibatch loss at step 18900: 0.352326 with learning rate 0.003752\n",
      "step 18900, training accuracy 0.9375\n",
      "Validation accuracy: 0.913767\n",
      "Minibatch loss at step 19000: 0.348807 with learning rate 0.003752\n",
      "step 19000, training accuracy 0.951172\n",
      "Validation accuracy: 0.913167\n",
      "Minibatch loss at step 19100: 0.328111 with learning rate 0.003565\n",
      "step 19100, training accuracy 0.943359\n",
      "Validation accuracy: 0.91345\n",
      "Minibatch loss at step 19200: 0.367496 with learning rate 0.003565\n",
      "step 19200, training accuracy 0.945312\n",
      "Validation accuracy: 0.9126\n",
      "Minibatch loss at step 19300: 0.325764 with learning rate 0.003565\n",
      "step 19300, training accuracy 0.953125\n",
      "Validation accuracy: 0.9132\n",
      "Minibatch loss at step 19400: 0.341328 with learning rate 0.003387\n",
      "step 19400, training accuracy 0.962891\n",
      "Validation accuracy: 0.91275\n",
      "Minibatch loss at step 19500: 0.320194 with learning rate 0.003387\n",
      "step 19500, training accuracy 0.955078\n",
      "Validation accuracy: 0.913083\n",
      "Minibatch loss at step 19600: 0.371944 with learning rate 0.003387\n",
      "step 19600, training accuracy 0.935547\n",
      "Validation accuracy: 0.913167\n",
      "Minibatch loss at step 19700: 0.374730 with learning rate 0.003217\n",
      "step 19700, training accuracy 0.927734\n",
      "Validation accuracy: 0.913217\n",
      "Minibatch loss at step 19800: 0.328619 with learning rate 0.003217\n",
      "step 19800, training accuracy 0.951172\n",
      "Validation accuracy: 0.9135\n",
      "Minibatch loss at step 19900: 0.340490 with learning rate 0.003217\n",
      "step 19900, training accuracy 0.941406\n",
      "Validation accuracy: 0.91295\n",
      "Minibatch loss at step 20000: 0.318129 with learning rate 0.003056\n",
      "step 20000, training accuracy 0.957031\n",
      "Validation accuracy: 0.91295\n",
      "test accuracy 0.96144\n"
     ]
    }
   ],
   "source": [
    "train_model(20001, 100, 512, graph, is_saving=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 1024h, decay with 0.1 start, and l2 regu 0.01. Regu 0.1 much too much, regu 0.001 too small, much less effect\n",
    "(train_model(20001, 100, 512, graph, is_saving=True))\n",
    "\n",
    "    Minibatch loss at step 19500: 0.320194 with learning rate 0.003387\n",
    "    step 19500, training accuracy 0.955078\n",
    "    Validation accuracy: 0.913083\n",
    "    Minibatch loss at step 19600: 0.371944 with learning rate 0.003387\n",
    "    step 19600, training accuracy 0.935547\n",
    "    Validation accuracy: 0.913167\n",
    "    Minibatch loss at step 19700: 0.374730 with learning rate 0.003217\n",
    "    step 19700, training accuracy 0.927734\n",
    "    Validation accuracy: 0.913217\n",
    "    Minibatch loss at step 19800: 0.328619 with learning rate 0.003217\n",
    "    step 19800, training accuracy 0.951172\n",
    "    Validation accuracy: 0.9135\n",
    "    Minibatch loss at step 19900: 0.340490 with learning rate 0.003217\n",
    "    step 19900, training accuracy 0.941406\n",
    "    Validation accuracy: 0.91295\n",
    "    Minibatch loss at step 20000: 0.318129 with learning rate 0.003056\n",
    "    step 20000, training accuracy 0.957031\n",
    "    Validation accuracy: 0.91295\n",
    "    test accuracy 0.96144\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.96144\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU0AAAESCAYAAAB5KIfmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyMjI9ANZNGVkZGT6wf8HfThphScMZVEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc837b83090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#restore_and_display_test(graph, \"l2 reg 1024\")\n",
    "restore_and_display_random_incorrect(16, graph, \"l2 reg 1024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save old datasets and trim + replace current\n",
    "saved_train_dataset = np.copy(train_dataset)\n",
    "saved_train_labels = np.copy(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#trim to 5 batches\n",
    "train_dataset = train_dataset[:512*5,:]\n",
    "train_labels = train_labels[:512*5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3437.287598 with learning rate 0.100000\n",
      "step 0, training accuracy 0.273438\n",
      "Validation accuracy: 0.232917\n",
      "Minibatch loss at step 100: 2541.250488 with learning rate 0.100000\n",
      "step 100, training accuracy 0.970703\n",
      "Validation accuracy: 0.73705\n",
      "Minibatch loss at step 200: 2079.891113 with learning rate 0.100000\n",
      "step 200, training accuracy 1\n",
      "Validation accuracy: 0.738483\n",
      "Minibatch loss at step 300: 1703.886841 with learning rate 0.095000\n",
      "step 300, training accuracy 1\n",
      "Validation accuracy: 0.73845\n",
      "Minibatch loss at step 400: 1408.931885 with learning rate 0.095000\n",
      "step 400, training accuracy 1\n",
      "Validation accuracy: 0.738533\n",
      "Minibatch loss at step 500: 1165.021118 with learning rate 0.095000\n",
      "step 500, training accuracy 1\n",
      "Validation accuracy: 0.738567\n",
      "Minibatch loss at step 600: 964.617310 with learning rate 0.090250\n",
      "step 600, training accuracy 1\n",
      "Validation accuracy: 0.73865\n",
      "Minibatch loss at step 700: 805.244568 with learning rate 0.090250\n",
      "step 700, training accuracy 1\n",
      "Validation accuracy: 0.73855\n",
      "Minibatch loss at step 800: 672.211670 with learning rate 0.090250\n",
      "step 800, training accuracy 1\n",
      "Validation accuracy: 0.738483\n",
      "Minibatch loss at step 900: 562.217896 with learning rate 0.085737\n",
      "step 900, training accuracy 1\n",
      "Validation accuracy: 0.738533\n",
      "Minibatch loss at step 1000: 473.586060 with learning rate 0.085737\n",
      "step 1000, training accuracy 1\n",
      "Validation accuracy: 0.73865\n",
      "Minibatch loss at step 1100: 398.929749 with learning rate 0.085737\n",
      "step 1100, training accuracy 1\n",
      "Validation accuracy: 0.73865\n",
      "Minibatch loss at step 1200: 336.854431 with learning rate 0.081451\n",
      "step 1200, training accuracy 1\n",
      "Validation accuracy: 0.738683\n",
      "Minibatch loss at step 1300: 286.195374 with learning rate 0.081451\n",
      "step 1300, training accuracy 1\n",
      "Validation accuracy: 0.738733\n",
      "Minibatch loss at step 1400: 243.157791 with learning rate 0.081451\n",
      "step 1400, training accuracy 1\n",
      "Validation accuracy: 0.7387\n",
      "Minibatch loss at step 1500: 207.179871 with learning rate 0.077378\n",
      "step 1500, training accuracy 1\n",
      "Validation accuracy: 0.738783\n",
      "Minibatch loss at step 1600: 177.465744 with learning rate 0.077378\n",
      "step 1600, training accuracy 1\n",
      "Validation accuracy: 0.738817\n",
      "Minibatch loss at step 1700: 152.013702 with learning rate 0.077378\n",
      "step 1700, training accuracy 1\n",
      "Validation accuracy: 0.73895\n",
      "Minibatch loss at step 1800: 130.634338 with learning rate 0.073509\n",
      "step 1800, training accuracy 1\n",
      "Validation accuracy: 0.739267\n",
      "Minibatch loss at step 1900: 112.768066 with learning rate 0.073509\n",
      "step 1900, training accuracy 1\n",
      "Validation accuracy: 0.739633\n",
      "Minibatch loss at step 2000: 97.345383 with learning rate 0.073509\n",
      "step 2000, training accuracy 1\n",
      "Validation accuracy: 0.73975\n",
      "Minibatch loss at step 2100: 84.335037 with learning rate 0.069834\n",
      "step 2100, training accuracy 1\n",
      "Validation accuracy: 0.74035\n",
      "Minibatch loss at step 2200: 73.338623 with learning rate 0.069834\n",
      "step 2200, training accuracy 1\n",
      "Validation accuracy: 0.7407\n",
      "Minibatch loss at step 2300: 63.775852 with learning rate 0.069834\n",
      "step 2300, training accuracy 1\n",
      "Validation accuracy: 0.741183\n",
      "Minibatch loss at step 2400: 55.677597 with learning rate 0.066342\n",
      "step 2400, training accuracy 1\n",
      "Validation accuracy: 0.741933\n",
      "Minibatch loss at step 2500: 48.757233 with learning rate 0.066342\n",
      "step 2500, training accuracy 1\n",
      "Validation accuracy: 0.74265\n",
      "Minibatch loss at step 2600: 42.697586 with learning rate 0.066342\n",
      "step 2600, training accuracy 1\n",
      "Validation accuracy: 0.743367\n",
      "Minibatch loss at step 2700: 37.547485 with learning rate 0.063025\n",
      "step 2700, training accuracy 1\n",
      "Validation accuracy: 0.74405\n",
      "Minibatch loss at step 2800: 33.099842 with learning rate 0.063025\n",
      "step 2800, training accuracy 1\n",
      "Validation accuracy: 0.74515\n",
      "Minibatch loss at step 2900: 29.179035 with learning rate 0.063025\n",
      "step 2900, training accuracy 1\n",
      "Validation accuracy: 0.746233\n",
      "Minibatch loss at step 3000: 25.836563 with learning rate 0.059874\n",
      "step 3000, training accuracy 1\n",
      "Validation accuracy: 0.747317\n",
      "Minibatch loss at step 3100: 22.920517 with learning rate 0.059874\n",
      "step 3100, training accuracy 1\n",
      "Validation accuracy: 0.748533\n",
      "Minibatch loss at step 3200: 20.333757 with learning rate 0.059874\n",
      "step 3200, training accuracy 1\n",
      "Validation accuracy: 0.75005\n",
      "Minibatch loss at step 3300: 18.122198 with learning rate 0.056880\n",
      "step 3300, training accuracy 1\n",
      "Validation accuracy: 0.75165\n",
      "Minibatch loss at step 3400: 16.173588 with learning rate 0.056880\n",
      "step 3400, training accuracy 1\n",
      "Validation accuracy: 0.753567\n",
      "Minibatch loss at step 3500: 14.434608 with learning rate 0.056880\n",
      "step 3500, training accuracy 1\n",
      "Validation accuracy: 0.7552\n",
      "Minibatch loss at step 3600: 12.944242 with learning rate 0.054036\n",
      "step 3600, training accuracy 1\n",
      "Validation accuracy: 0.7566\n",
      "Minibatch loss at step 3700: 11.618565 with learning rate 0.054036\n",
      "step 3700, training accuracy 1\n",
      "Validation accuracy: 0.758533\n",
      "Minibatch loss at step 3800: 10.428768 with learning rate 0.054036\n",
      "step 3800, training accuracy 1\n",
      "Validation accuracy: 0.760783\n",
      "Minibatch loss at step 3900: 9.406910 with learning rate 0.051334\n",
      "step 3900, training accuracy 1\n",
      "Validation accuracy: 0.76225\n",
      "Minibatch loss at step 4000: 8.489439 with learning rate 0.051334\n",
      "step 4000, training accuracy 1\n",
      "Validation accuracy: 0.764233\n",
      "Minibatch loss at step 4100: 7.661556 with learning rate 0.051334\n",
      "step 4100, training accuracy 1\n",
      "Validation accuracy: 0.76625\n",
      "Minibatch loss at step 4200: 6.949359 with learning rate 0.048767\n",
      "step 4200, training accuracy 1\n",
      "Validation accuracy: 0.768167\n",
      "Minibatch loss at step 4300: 6.304028 with learning rate 0.048767\n",
      "step 4300, training accuracy 1\n",
      "Validation accuracy: 0.770067\n",
      "Minibatch loss at step 4400: 5.720108 with learning rate 0.046329\n",
      "step 4400, training accuracy 1\n",
      "Validation accuracy: 0.772233\n",
      "Minibatch loss at step 4500: 5.214509 with learning rate 0.046329\n",
      "step 4500, training accuracy 1\n",
      "Validation accuracy: 0.774333\n",
      "Minibatch loss at step 4600: 4.753623 with learning rate 0.046329\n",
      "step 4600, training accuracy 1\n",
      "Validation accuracy: 0.776567\n",
      "Minibatch loss at step 4700: 4.335915 with learning rate 0.044013\n",
      "step 4700, training accuracy 1\n",
      "Validation accuracy: 0.778583\n",
      "Minibatch loss at step 4800: 3.971152 with learning rate 0.044013\n",
      "step 4800, training accuracy 1\n",
      "Validation accuracy: 0.779933\n",
      "Minibatch loss at step 4900: 3.637175 with learning rate 0.044013\n",
      "step 4900, training accuracy 1\n",
      "Validation accuracy: 0.7821\n",
      "Minibatch loss at step 5000: 3.334103 with learning rate 0.041812\n",
      "step 5000, training accuracy 1\n",
      "Validation accuracy: 0.784083\n",
      "Minibatch loss at step 5100: 3.067263 with learning rate 0.041812\n",
      "step 5100, training accuracy 1\n",
      "Validation accuracy: 0.785933\n",
      "Minibatch loss at step 5200: 2.821862 with learning rate 0.041812\n",
      "step 5200, training accuracy 1\n",
      "Validation accuracy: 0.78765\n",
      "Minibatch loss at step 5300: 2.598980 with learning rate 0.039721\n",
      "step 5300, training accuracy 1\n",
      "Validation accuracy: 0.789517\n",
      "Minibatch loss at step 5400: 2.401145 with learning rate 0.039721\n",
      "step 5400, training accuracy 1\n",
      "Validation accuracy: 0.791083\n",
      "Minibatch loss at step 5500: 2.218427 with learning rate 0.039721\n",
      "step 5500, training accuracy 1\n",
      "Validation accuracy: 0.792467\n",
      "Minibatch loss at step 5600: 2.052362 with learning rate 0.037735\n",
      "step 5600, training accuracy 1\n",
      "Validation accuracy: 0.793967\n",
      "Minibatch loss at step 5700: 1.903826 with learning rate 0.037735\n",
      "step 5700, training accuracy 1\n",
      "Validation accuracy: 0.795633\n",
      "Minibatch loss at step 5800: 1.766108 with learning rate 0.037735\n",
      "step 5800, training accuracy 1\n",
      "Validation accuracy: 0.797167\n",
      "Minibatch loss at step 5900: 1.640863 with learning rate 0.035849\n",
      "step 5900, training accuracy 1\n",
      "Validation accuracy: 0.798417\n",
      "Minibatch loss at step 6000: 1.528001 with learning rate 0.035849\n",
      "step 6000, training accuracy 1\n",
      "Validation accuracy: 0.79935\n",
      "Minibatch loss at step 6100: 1.422962 with learning rate 0.035849\n",
      "step 6100, training accuracy 1\n",
      "Validation accuracy: 0.80075\n",
      "Minibatch loss at step 6200: 1.327422 with learning rate 0.034056\n",
      "step 6200, training accuracy 1\n",
      "Validation accuracy: 0.801667\n",
      "Minibatch loss at step 6300: 1.240681 with learning rate 0.034056\n",
      "step 6300, training accuracy 1\n",
      "Validation accuracy: 0.802867\n",
      "Minibatch loss at step 6400: 1.159649 with learning rate 0.034056\n",
      "step 6400, training accuracy 1\n",
      "Validation accuracy: 0.8039\n",
      "Minibatch loss at step 6500: 1.085955 with learning rate 0.032353\n",
      "step 6500, training accuracy 1\n",
      "Validation accuracy: 0.804433\n",
      "Minibatch loss at step 6600: 1.018560 with learning rate 0.032353\n",
      "step 6600, training accuracy 1\n",
      "Validation accuracy: 0.805317\n",
      "Minibatch loss at step 6700: 0.955404 with learning rate 0.032353\n",
      "step 6700, training accuracy 1\n",
      "Validation accuracy: 0.8063\n",
      "Minibatch loss at step 6800: 0.897952 with learning rate 0.030736\n",
      "step 6800, training accuracy 1\n",
      "Validation accuracy: 0.8071\n",
      "Minibatch loss at step 6900: 0.845053 with learning rate 0.030736\n",
      "step 6900, training accuracy 1\n",
      "Validation accuracy: 0.807617\n",
      "Minibatch loss at step 7000: 0.795315 with learning rate 0.030736\n",
      "step 7000, training accuracy 1\n",
      "Validation accuracy: 0.808333\n",
      "Minibatch loss at step 7100: 0.750089 with learning rate 0.029199\n",
      "step 7100, training accuracy 1\n",
      "Validation accuracy: 0.809\n",
      "Minibatch loss at step 7200: 0.708169 with learning rate 0.029199\n",
      "step 7200, training accuracy 1\n",
      "Validation accuracy: 0.8098\n",
      "Minibatch loss at step 7300: 0.668628 with learning rate 0.029199\n",
      "step 7300, training accuracy 1\n",
      "Validation accuracy: 0.81045\n",
      "Minibatch loss at step 7400: 0.632691 with learning rate 0.027739\n",
      "step 7400, training accuracy 1\n",
      "Validation accuracy: 0.810833\n",
      "Minibatch loss at step 7500: 0.599154 with learning rate 0.027739\n",
      "step 7500, training accuracy 1\n",
      "Validation accuracy: 0.81125\n",
      "Minibatch loss at step 7600: 0.567428 with learning rate 0.027739\n",
      "step 7600, training accuracy 1\n",
      "Validation accuracy: 0.811767\n",
      "Minibatch loss at step 7700: 0.538615 with learning rate 0.026352\n",
      "step 7700, training accuracy 1\n",
      "Validation accuracy: 0.81195\n",
      "Minibatch loss at step 7800: 0.511548 with learning rate 0.026352\n",
      "step 7800, training accuracy 1\n",
      "Validation accuracy: 0.812517\n",
      "Minibatch loss at step 7900: 0.485875 with learning rate 0.026352\n",
      "step 7900, training accuracy 1\n",
      "Validation accuracy: 0.812817\n",
      "Minibatch loss at step 8000: 0.462580 with learning rate 0.025034\n",
      "step 8000, training accuracy 1\n",
      "Validation accuracy: 0.813283\n",
      "Minibatch loss at step 8100: 0.440559 with learning rate 0.025034\n",
      "step 8100, training accuracy 1\n",
      "Validation accuracy: 0.813417\n",
      "Minibatch loss at step 8200: 0.419614 with learning rate 0.025034\n",
      "step 8200, training accuracy 1\n",
      "Validation accuracy: 0.813767\n",
      "Minibatch loss at step 8300: 0.400624 with learning rate 0.023783\n",
      "step 8300, training accuracy 1\n",
      "Validation accuracy: 0.814167\n",
      "Minibatch loss at step 8400: 0.382562 with learning rate 0.023783\n",
      "step 8400, training accuracy 1\n",
      "Validation accuracy: 0.814717\n",
      "Minibatch loss at step 8500: 0.365364 with learning rate 0.022594\n",
      "step 8500, training accuracy 1\n",
      "Validation accuracy: 0.815\n",
      "Minibatch loss at step 8600: 0.349743 with learning rate 0.022594\n",
      "step 8600, training accuracy 1\n",
      "Validation accuracy: 0.815183\n",
      "Minibatch loss at step 8700: 0.334821 with learning rate 0.022594\n",
      "step 8700, training accuracy 1\n",
      "Validation accuracy: 0.815583\n",
      "Minibatch loss at step 8800: 0.320627 with learning rate 0.021464\n",
      "step 8800, training accuracy 1\n",
      "Validation accuracy: 0.816\n",
      "Minibatch loss at step 8900: 0.307658 with learning rate 0.021464\n",
      "step 8900, training accuracy 1\n",
      "Validation accuracy: 0.816267\n",
      "Minibatch loss at step 9000: 0.295239 with learning rate 0.021464\n",
      "step 9000, training accuracy 1\n",
      "Validation accuracy: 0.816467\n",
      "Minibatch loss at step 9100: 0.283437 with learning rate 0.020391\n",
      "step 9100, training accuracy 1\n",
      "Validation accuracy: 0.81675\n",
      "Minibatch loss at step 9200: 0.272597 with learning rate 0.020391\n",
      "step 9200, training accuracy 1\n",
      "Validation accuracy: 0.817017\n",
      "Minibatch loss at step 9300: 0.262190 with learning rate 0.020391\n",
      "step 9300, training accuracy 1\n",
      "Validation accuracy: 0.817317\n",
      "Minibatch loss at step 9400: 0.252322 with learning rate 0.019371\n",
      "step 9400, training accuracy 1\n",
      "Validation accuracy: 0.8175\n",
      "Minibatch loss at step 9500: 0.243202 with learning rate 0.019371\n",
      "step 9500, training accuracy 1\n",
      "Validation accuracy: 0.81755\n",
      "Minibatch loss at step 9600: 0.234426 with learning rate 0.019371\n",
      "step 9600, training accuracy 1\n",
      "Validation accuracy: 0.8177\n",
      "Minibatch loss at step 9700: 0.226118 with learning rate 0.018403\n",
      "step 9700, training accuracy 1\n",
      "Validation accuracy: 0.817933\n",
      "Minibatch loss at step 9800: 0.218391 with learning rate 0.018403\n",
      "step 9800, training accuracy 1\n",
      "Validation accuracy: 0.818017\n",
      "Minibatch loss at step 9900: 0.210945 with learning rate 0.018403\n",
      "step 9900, training accuracy 1\n",
      "Validation accuracy: 0.81815\n",
      "Minibatch loss at step 10000: 0.203904 with learning rate 0.017482\n",
      "step 10000, training accuracy 1\n",
      "Validation accuracy: 0.818267\n",
      "test accuracy 0.884854\n"
     ]
    }
   ],
   "source": [
    "train_model(10001, 100, 512, graph, is_saving=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = saved_train_dataset\n",
    "train_labels = saved_train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_nodes = 1424\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(None, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "    tf_batch_size = tf.placeholder(tf.int32)\n",
    "    tf_keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    # input layer variables\n",
    "    weights_i = weight_variable([image_size * image_size, hidden_nodes], hidden_nodes)\n",
    "    biases_i = bias_variable([hidden_nodes])\n",
    "    \n",
    "    #hidden layer\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_i) + biases_i)\n",
    "    #drop it\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, tf_keep_prob)\n",
    "    # output layer variables\n",
    "    weights_o = weight_variable([hidden_nodes, num_labels], hidden_nodes)\n",
    "    biases_o = bias_variable([num_labels])\n",
    "    #output operation\n",
    "    logits_o = tf.matmul(h_fc1_drop, weights_o) + biases_o\n",
    "    y_conv = tf.nn.softmax(logits_o) #after softmax -> output probabilities / predictions\n",
    "    #define loss    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits_o, tf_train_labels)\n",
    "                          + 0.01*tf.nn.l2_loss(weights_i))                \n",
    "    #decay learning rate\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        0.1, global_step * tf_batch_size, train_dataset.shape[0] / 0.5, 0.95, staircase=True)\n",
    "    #Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    #learning_rate = tf.constant(0.01)\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    #define operations to measure accuracy\n",
    "    is_correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(tf_train_labels,1))\n",
    "    accuracy_l = tf.reduce_mean(tf.cast(is_correct_prediction, tf.float32))\n",
    "    #saving mode\n",
    "    saver = tf.train.Saver(max_to_keep=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.302583 with learning rate 0.100000\n",
      "step 0, training accuracy 0.130859\n",
      "Validation accuracy: 0.10205\n",
      "Minibatch loss at step 100: 2.302460 with learning rate 0.100000\n",
      "step 100, training accuracy 0.0996094\n",
      "Validation accuracy: 0.0995833\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-1f5887f9a021>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m40001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_saving\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-f5d46d69f199>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(num_steps, checkpoint_every, batch_size, graph, is_saving)\u001b[0m\n\u001b[0;32m     24\u001b[0m             feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,\n\u001b[0;32m     25\u001b[0m                          tf_batch_size: batch_size, tf_keep_prob: 0.5}\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mcheckpoint_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Minibatch loss at step %d: %f with learning rate %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munique_fetch_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m       return tf_session.TF_Run(self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 428\u001b[1;33m                                target_list)\n\u001b[0m\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(40001, 100, 512, graph, is_saving=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(None, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "    tf_batch_size = tf.placeholder(tf.int32)\n",
    "    tf_keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    # input layer variables\n",
    "    weights_i = weight_variable([image_size * image_size, hidden_nodes], image_size * image_size)\n",
    "    biases_i = bias_variable([hidden_nodes])\n",
    "    \n",
    "    #hidden layer\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_i) + biases_i)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, tf_keep_prob)\n",
    "    \n",
    "    weights_h2 = weight_variable([hidden_nodes, hidden_nodes / 4], hidden_nodes)\n",
    "    biases_h2 = bias_variable([hidden_nodes / 4])\n",
    "    h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, weights_h2) + biases_h2)\n",
    "    h_fc2_drop = tf.nn.dropout(h_fc2, tf_keep_prob)\n",
    "    \n",
    "    weights_h3 = weight_variable([hidden_nodes / 4, hidden_nodes / 20], hidden_nodes / 4)\n",
    "    biases_h3 = bias_variable([hidden_nodes / 20])\n",
    "    h_fc3 = tf.nn.relu(tf.matmul(h_fc2_drop, weights_h3) + biases_h3)        \n",
    "    h_fc3_drop = tf.nn.dropout(h_fc3, tf_keep_prob)\n",
    "    \n",
    "    # output layer variables\n",
    "    weights_o = weight_variable([hidden_nodes / 20, num_labels], hidden_nodes / 20)\n",
    "    biases_o = bias_variable([num_labels])    \n",
    "    #output operation\n",
    "    logits_o = tf.matmul(h_fc3_drop, weights_o) + biases_o\n",
    "    y_conv = tf.nn.softmax(logits_o) #after softmax -> output probabilities / predictions\n",
    "    #define loss    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits_o, tf_train_labels)\n",
    "                        + (2.0/(3*hidden_nodes))*(tf.nn.l2_loss(weights_i)\n",
    "                        + tf.nn.l2_loss(weights_h2)\n",
    "                        + tf.nn.l2_loss(weights_h3)))\n",
    "    #decay learning rate\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        0.1, global_step * tf_batch_size, train_dataset.shape[0] / 1, 0.95, staircase=True)\n",
    "    #Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    #learning_rate = tf.constant(0.01)\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    #define operations to measure accuracy\n",
    "    is_correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(tf_train_labels,1))\n",
    "    accuracy_l = tf.reduce_mean(tf.cast(is_correct_prediction, tf.float32))\n",
    "    #saving mode\n",
    "    saver = tf.train.Saver(max_to_keep=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.148685 with learning rate 0.100000\n",
      "step 0, training accuracy 0.207031\n",
      "Validation accuracy: 0.188617\n",
      "Minibatch loss at step 100: 1.461157 with learning rate 0.100000\n",
      "step 100, training accuracy 0.777344\n",
      "Validation accuracy: 0.818133\n",
      "Minibatch loss at step 200: 1.283906 with learning rate 0.100000\n",
      "step 200, training accuracy 0.863281\n",
      "Validation accuracy: 0.833333\n",
      "Minibatch loss at step 300: 1.354653 with learning rate 0.100000\n",
      "step 300, training accuracy 0.796875\n",
      "Validation accuracy: 0.84\n",
      "Minibatch loss at step 400: 1.039869 with learning rate 0.100000\n",
      "step 400, training accuracy 0.925781\n",
      "Validation accuracy: 0.843283\n",
      "Minibatch loss at step 500: 1.192185 with learning rate 0.100000\n",
      "step 500, training accuracy 0.863281\n",
      "Validation accuracy: 0.847367\n",
      "Minibatch loss at step 600: 1.291822 with learning rate 0.100000\n",
      "step 600, training accuracy 0.828125\n",
      "Validation accuracy: 0.850633\n",
      "Minibatch loss at step 700: 1.176693 with learning rate 0.100000\n",
      "step 700, training accuracy 0.855469\n",
      "Validation accuracy: 0.854767\n",
      "Minibatch loss at step 800: 1.081926 with learning rate 0.100000\n",
      "step 800, training accuracy 0.894531\n",
      "Validation accuracy: 0.856483\n",
      "Minibatch loss at step 900: 1.125577 with learning rate 0.100000\n",
      "step 900, training accuracy 0.871094\n",
      "Validation accuracy: 0.858283\n",
      "Minibatch loss at step 1000: 1.052111 with learning rate 0.100000\n",
      "step 1000, training accuracy 0.890625\n",
      "Validation accuracy: 0.859433\n",
      "Minibatch loss at step 1100: 1.068644 with learning rate 0.100000\n",
      "step 1100, training accuracy 0.871094\n",
      "Validation accuracy: 0.862283\n",
      "Minibatch loss at step 1200: 1.026774 with learning rate 0.100000\n",
      "step 1200, training accuracy 0.902344\n",
      "Validation accuracy: 0.8642\n",
      "Minibatch loss at step 1300: 0.980746 with learning rate 0.100000\n",
      "step 1300, training accuracy 0.890625\n",
      "Validation accuracy: 0.86655\n",
      "Minibatch loss at step 1400: 0.977567 with learning rate 0.100000\n",
      "step 1400, training accuracy 0.902344\n",
      "Validation accuracy: 0.868867\n",
      "Minibatch loss at step 1500: 1.059608 with learning rate 0.100000\n",
      "step 1500, training accuracy 0.871094\n",
      "Validation accuracy: 0.868567\n",
      "Minibatch loss at step 1600: 0.986022 with learning rate 0.100000\n",
      "step 1600, training accuracy 0.882812\n",
      "Validation accuracy: 0.870017\n",
      "Minibatch loss at step 1700: 1.066407 with learning rate 0.100000\n",
      "step 1700, training accuracy 0.863281\n",
      "Validation accuracy: 0.870917\n",
      "Minibatch loss at step 1800: 1.017199 with learning rate 0.100000\n",
      "step 1800, training accuracy 0.898438\n",
      "Validation accuracy: 0.873217\n",
      "Minibatch loss at step 1900: 1.157186 with learning rate 0.095000\n",
      "step 1900, training accuracy 0.859375\n",
      "Validation accuracy: 0.873433\n",
      "Minibatch loss at step 2000: 0.935242 with learning rate 0.095000\n",
      "step 2000, training accuracy 0.90625\n",
      "Validation accuracy: 0.8749\n",
      "Minibatch loss at step 2100: 0.854265 with learning rate 0.095000\n",
      "step 2100, training accuracy 0.914062\n",
      "Validation accuracy: 0.8767\n",
      "Minibatch loss at step 2200: 0.878294 with learning rate 0.095000\n",
      "step 2200, training accuracy 0.90625\n",
      "Validation accuracy: 0.877983\n",
      "Minibatch loss at step 2300: 0.957760 with learning rate 0.095000\n",
      "step 2300, training accuracy 0.878906\n",
      "Validation accuracy: 0.878917\n",
      "Minibatch loss at step 2400: 0.954598 with learning rate 0.095000\n",
      "step 2400, training accuracy 0.886719\n",
      "Validation accuracy: 0.87955\n",
      "Minibatch loss at step 2500: 0.970086 with learning rate 0.095000\n",
      "step 2500, training accuracy 0.886719\n",
      "Validation accuracy: 0.880817\n",
      "Minibatch loss at step 2600: 1.013889 with learning rate 0.095000\n",
      "step 2600, training accuracy 0.878906\n",
      "Validation accuracy: 0.880517\n",
      "Minibatch loss at step 2700: 0.941024 with learning rate 0.095000\n",
      "step 2700, training accuracy 0.882812\n",
      "Validation accuracy: 0.881767\n",
      "Minibatch loss at step 2800: 0.991257 with learning rate 0.095000\n",
      "step 2800, training accuracy 0.867188\n",
      "Validation accuracy: 0.8823\n",
      "Minibatch loss at step 2900: 0.970785 with learning rate 0.095000\n",
      "step 2900, training accuracy 0.867188\n",
      "Validation accuracy: 0.883433\n",
      "Minibatch loss at step 3000: 0.749368 with learning rate 0.095000\n",
      "step 3000, training accuracy 0.933594\n",
      "Validation accuracy: 0.88285\n",
      "Minibatch loss at step 3100: 0.770373 with learning rate 0.095000\n",
      "step 3100, training accuracy 0.925781\n",
      "Validation accuracy: 0.885183\n",
      "Minibatch loss at step 3200: 0.774212 with learning rate 0.095000\n",
      "step 3200, training accuracy 0.929688\n",
      "Validation accuracy: 0.885283\n",
      "Minibatch loss at step 3300: 0.808649 with learning rate 0.095000\n",
      "step 3300, training accuracy 0.929688\n",
      "Validation accuracy: 0.88565\n",
      "Minibatch loss at step 3400: 0.872778 with learning rate 0.095000\n",
      "step 3400, training accuracy 0.890625\n",
      "Validation accuracy: 0.887167\n",
      "Minibatch loss at step 3500: 0.833555 with learning rate 0.095000\n",
      "step 3500, training accuracy 0.875\n",
      "Validation accuracy: 0.887717\n",
      "Minibatch loss at step 3600: 0.939350 with learning rate 0.095000\n",
      "step 3600, training accuracy 0.878906\n",
      "Validation accuracy: 0.887817\n",
      "Minibatch loss at step 3700: 0.987784 with learning rate 0.090250\n",
      "step 3700, training accuracy 0.839844\n",
      "Validation accuracy: 0.88895\n",
      "Minibatch loss at step 3800: 0.842104 with learning rate 0.090250\n",
      "step 3800, training accuracy 0.902344\n",
      "Validation accuracy: 0.890117\n",
      "Minibatch loss at step 3900: 0.809940 with learning rate 0.090250\n",
      "step 3900, training accuracy 0.902344\n",
      "Validation accuracy: 0.890333\n",
      "Minibatch loss at step 4000: 0.799362 with learning rate 0.090250\n",
      "step 4000, training accuracy 0.90625\n",
      "Validation accuracy: 0.89035\n",
      "Minibatch loss at step 4100: 0.886364 with learning rate 0.090250\n",
      "step 4100, training accuracy 0.882812\n",
      "Validation accuracy: 0.89105\n",
      "Minibatch loss at step 4200: 0.913322 with learning rate 0.090250\n",
      "step 4200, training accuracy 0.886719\n",
      "Validation accuracy: 0.890917\n",
      "Minibatch loss at step 4300: 0.730769 with learning rate 0.090250\n",
      "step 4300, training accuracy 0.914062\n",
      "Validation accuracy: 0.892183\n",
      "Minibatch loss at step 4400: 0.880946 with learning rate 0.090250\n",
      "step 4400, training accuracy 0.894531\n",
      "Validation accuracy: 0.891783\n",
      "Minibatch loss at step 4500: 0.775747 with learning rate 0.090250\n",
      "step 4500, training accuracy 0.90625\n",
      "Validation accuracy: 0.892683\n",
      "Minibatch loss at step 4600: 0.787813 with learning rate 0.090250\n",
      "step 4600, training accuracy 0.902344\n",
      "Validation accuracy: 0.891617\n",
      "Minibatch loss at step 4700: 0.715295 with learning rate 0.090250\n",
      "step 4700, training accuracy 0.933594\n",
      "Validation accuracy: 0.892967\n",
      "Minibatch loss at step 4800: 0.649876 with learning rate 0.090250\n",
      "step 4800, training accuracy 0.9375\n",
      "Validation accuracy: 0.893683\n",
      "Minibatch loss at step 4900: 0.841709 with learning rate 0.090250\n",
      "step 4900, training accuracy 0.914062\n",
      "Validation accuracy: 0.895033\n",
      "Minibatch loss at step 5000: 0.745683 with learning rate 0.090250\n",
      "step 5000, training accuracy 0.902344\n",
      "Validation accuracy: 0.894667\n",
      "Minibatch loss at step 5100: 0.716955 with learning rate 0.090250\n",
      "step 5100, training accuracy 0.914062\n",
      "Validation accuracy: 0.894867\n",
      "Minibatch loss at step 5200: 0.771648 with learning rate 0.090250\n",
      "step 5200, training accuracy 0.90625\n",
      "Validation accuracy: 0.894533\n",
      "Minibatch loss at step 5300: 0.872240 with learning rate 0.090250\n",
      "step 5300, training accuracy 0.890625\n",
      "Validation accuracy: 0.894917\n",
      "Minibatch loss at step 5400: 0.757858 with learning rate 0.090250\n",
      "step 5400, training accuracy 0.914062\n",
      "Validation accuracy: 0.8957\n",
      "Minibatch loss at step 5500: 0.778244 with learning rate 0.085737\n",
      "step 5500, training accuracy 0.894531\n",
      "Validation accuracy: 0.896783\n",
      "Minibatch loss at step 5600: 0.707615 with learning rate 0.085737\n",
      "step 5600, training accuracy 0.898438\n",
      "Validation accuracy: 0.896733\n",
      "Minibatch loss at step 5700: 0.692761 with learning rate 0.085737\n",
      "step 5700, training accuracy 0.933594\n",
      "Validation accuracy: 0.89685\n",
      "Minibatch loss at step 5800: 0.654280 with learning rate 0.085737\n",
      "step 5800, training accuracy 0.9375\n",
      "Validation accuracy: 0.896817\n",
      "Minibatch loss at step 5900: 0.783164 with learning rate 0.085737\n",
      "step 5900, training accuracy 0.890625\n",
      "Validation accuracy: 0.897417\n",
      "Minibatch loss at step 6000: 0.706718 with learning rate 0.085737\n",
      "step 6000, training accuracy 0.925781\n",
      "Validation accuracy: 0.898783\n",
      "Minibatch loss at step 6100: 0.656783 with learning rate 0.085737\n",
      "step 6100, training accuracy 0.9375\n",
      "Validation accuracy: 0.898367\n",
      "Minibatch loss at step 6200: 0.710890 with learning rate 0.085737\n",
      "step 6200, training accuracy 0.902344\n",
      "Validation accuracy: 0.898433\n",
      "Minibatch loss at step 6300: 0.720969 with learning rate 0.085737\n",
      "step 6300, training accuracy 0.917969\n",
      "Validation accuracy: 0.89855\n",
      "Minibatch loss at step 6400: 0.650543 with learning rate 0.085737\n",
      "step 6400, training accuracy 0.921875\n",
      "Validation accuracy: 0.898867\n",
      "Minibatch loss at step 6500: 0.733312 with learning rate 0.085737\n",
      "step 6500, training accuracy 0.882812\n",
      "Validation accuracy: 0.898333\n",
      "Minibatch loss at step 6600: 0.666459 with learning rate 0.085737\n",
      "step 6600, training accuracy 0.917969\n",
      "Validation accuracy: 0.90035\n",
      "Minibatch loss at step 6700: 0.649508 with learning rate 0.085737\n",
      "step 6700, training accuracy 0.925781\n",
      "Validation accuracy: 0.8999\n",
      "Minibatch loss at step 6800: 0.659177 with learning rate 0.085737\n",
      "step 6800, training accuracy 0.921875\n",
      "Validation accuracy: 0.90015\n",
      "Minibatch loss at step 6900: 0.711630 with learning rate 0.085737\n",
      "step 6900, training accuracy 0.902344\n",
      "Validation accuracy: 0.900167\n",
      "Minibatch loss at step 7000: 0.644720 with learning rate 0.085737\n",
      "step 7000, training accuracy 0.910156\n",
      "Validation accuracy: 0.90155\n",
      "Minibatch loss at step 7100: 0.698959 with learning rate 0.085737\n",
      "step 7100, training accuracy 0.898438\n",
      "Validation accuracy: 0.8999\n",
      "Minibatch loss at step 7200: 0.746137 with learning rate 0.085737\n",
      "step 7200, training accuracy 0.886719\n",
      "Validation accuracy: 0.90045\n",
      "Minibatch loss at step 7300: 0.671859 with learning rate 0.085737\n",
      "step 7300, training accuracy 0.890625\n",
      "Validation accuracy: 0.90145\n",
      "Minibatch loss at step 7400: 0.626946 with learning rate 0.081451\n",
      "step 7400, training accuracy 0.929688\n",
      "Validation accuracy: 0.901583\n",
      "Minibatch loss at step 7500: 0.704237 with learning rate 0.081451\n",
      "step 7500, training accuracy 0.890625\n",
      "Validation accuracy: 0.902317\n",
      "Minibatch loss at step 7600: 0.600534 with learning rate 0.081451\n",
      "step 7600, training accuracy 0.925781\n",
      "Validation accuracy: 0.902817\n",
      "Minibatch loss at step 7700: 0.638251 with learning rate 0.081451\n",
      "step 7700, training accuracy 0.921875\n",
      "Validation accuracy: 0.902733\n",
      "Minibatch loss at step 7800: 0.581538 with learning rate 0.081451\n",
      "step 7800, training accuracy 0.929688\n",
      "Validation accuracy: 0.902717\n",
      "Minibatch loss at step 7900: 0.610324 with learning rate 0.081451\n",
      "step 7900, training accuracy 0.910156\n",
      "Validation accuracy: 0.902333\n",
      "Minibatch loss at step 8000: 0.529684 with learning rate 0.081451\n",
      "step 8000, training accuracy 0.945312\n",
      "Validation accuracy: 0.9029\n",
      "Minibatch loss at step 8100: 0.658302 with learning rate 0.081451\n",
      "step 8100, training accuracy 0.941406\n",
      "Validation accuracy: 0.903467\n",
      "Minibatch loss at step 8200: 0.647740 with learning rate 0.081451\n",
      "step 8200, training accuracy 0.925781\n",
      "Validation accuracy: 0.9036\n",
      "Minibatch loss at step 8300: 0.602677 with learning rate 0.081451\n",
      "step 8300, training accuracy 0.933594\n",
      "Validation accuracy: 0.904217\n",
      "Minibatch loss at step 8400: 0.644096 with learning rate 0.081451\n",
      "step 8400, training accuracy 0.902344\n",
      "Validation accuracy: 0.903033\n",
      "Minibatch loss at step 8500: 0.574338 with learning rate 0.081451\n",
      "step 8500, training accuracy 0.933594\n",
      "Validation accuracy: 0.904\n",
      "Minibatch loss at step 8600: 0.617807 with learning rate 0.081451\n",
      "step 8600, training accuracy 0.921875\n",
      "Validation accuracy: 0.90315\n",
      "Minibatch loss at step 8700: 0.675896 with learning rate 0.081451\n",
      "step 8700, training accuracy 0.886719\n",
      "Validation accuracy: 0.903417\n",
      "Minibatch loss at step 8800: 0.661497 with learning rate 0.081451\n",
      "step 8800, training accuracy 0.917969\n",
      "Validation accuracy: 0.9047\n",
      "Minibatch loss at step 8900: 0.721482 with learning rate 0.081451\n",
      "step 8900, training accuracy 0.890625\n",
      "Validation accuracy: 0.904617\n",
      "Minibatch loss at step 9000: 0.594655 with learning rate 0.081451\n",
      "step 9000, training accuracy 0.929688\n",
      "Validation accuracy: 0.905417\n",
      "Minibatch loss at step 9100: 0.719003 with learning rate 0.081451\n",
      "step 9100, training accuracy 0.902344\n",
      "Validation accuracy: 0.905483\n",
      "Minibatch loss at step 9200: 0.532889 with learning rate 0.077378\n",
      "step 9200, training accuracy 0.941406\n",
      "Validation accuracy: 0.90505\n",
      "Minibatch loss at step 9300: 0.490509 with learning rate 0.077378\n",
      "step 9300, training accuracy 0.949219\n",
      "Validation accuracy: 0.90545\n",
      "Minibatch loss at step 9400: 0.528857 with learning rate 0.077378\n",
      "step 9400, training accuracy 0.925781\n",
      "Validation accuracy: 0.906817\n",
      "Minibatch loss at step 9500: 0.499443 with learning rate 0.077378\n",
      "step 9500, training accuracy 0.957031\n",
      "Validation accuracy: 0.906217\n",
      "Minibatch loss at step 9600: 0.705424 with learning rate 0.077378\n",
      "step 9600, training accuracy 0.882812\n",
      "Validation accuracy: 0.906417\n",
      "Minibatch loss at step 9700: 0.457062 with learning rate 0.077378\n",
      "step 9700, training accuracy 0.9375\n",
      "Validation accuracy: 0.906867\n",
      "Minibatch loss at step 9800: 0.486277 with learning rate 0.077378\n",
      "step 9800, training accuracy 0.9375\n",
      "Validation accuracy: 0.906667\n",
      "Minibatch loss at step 9900: 0.590177 with learning rate 0.077378\n",
      "step 9900, training accuracy 0.933594\n",
      "Validation accuracy: 0.9067\n",
      "Minibatch loss at step 10000: 0.578395 with learning rate 0.077378\n",
      "step 10000, training accuracy 0.910156\n",
      "Validation accuracy: 0.907733\n",
      "Minibatch loss at step 10100: 0.581525 with learning rate 0.077378\n",
      "step 10100, training accuracy 0.90625\n",
      "Validation accuracy: 0.907067\n",
      "Minibatch loss at step 10200: 0.522389 with learning rate 0.077378\n",
      "step 10200, training accuracy 0.933594\n",
      "Validation accuracy: 0.908033\n",
      "Minibatch loss at step 10300: 0.628446 with learning rate 0.077378\n",
      "step 10300, training accuracy 0.925781\n",
      "Validation accuracy: 0.907467\n",
      "Minibatch loss at step 10400: 0.566013 with learning rate 0.077378\n",
      "step 10400, training accuracy 0.914062\n",
      "Validation accuracy: 0.90845\n",
      "Minibatch loss at step 10500: 0.514715 with learning rate 0.077378\n",
      "step 10500, training accuracy 0.945312\n",
      "Validation accuracy: 0.907217\n",
      "Minibatch loss at step 10600: 0.614190 with learning rate 0.077378\n",
      "step 10600, training accuracy 0.902344\n",
      "Validation accuracy: 0.9074\n",
      "Minibatch loss at step 10700: 0.516693 with learning rate 0.077378\n",
      "step 10700, training accuracy 0.929688\n",
      "Validation accuracy: 0.908067\n",
      "Minibatch loss at step 10800: 0.463706 with learning rate 0.077378\n",
      "step 10800, training accuracy 0.953125\n",
      "Validation accuracy: 0.907633\n",
      "Minibatch loss at step 10900: 0.539205 with learning rate 0.077378\n",
      "step 10900, training accuracy 0.921875\n",
      "Validation accuracy: 0.907167\n",
      "Minibatch loss at step 11000: 0.576390 with learning rate 0.073509\n",
      "step 11000, training accuracy 0.925781\n",
      "Validation accuracy: 0.908433\n",
      "Minibatch loss at step 11100: 0.535373 with learning rate 0.073509\n",
      "step 11100, training accuracy 0.933594\n",
      "Validation accuracy: 0.907783\n",
      "Minibatch loss at step 11200: 0.525925 with learning rate 0.073509\n",
      "step 11200, training accuracy 0.933594\n",
      "Validation accuracy: 0.908333\n",
      "Minibatch loss at step 11300: 0.563752 with learning rate 0.073509\n",
      "step 11300, training accuracy 0.929688\n",
      "Validation accuracy: 0.908317\n",
      "Minibatch loss at step 11400: 0.542660 with learning rate 0.073509\n",
      "step 11400, training accuracy 0.917969\n",
      "Validation accuracy: 0.908383\n",
      "Minibatch loss at step 11500: 0.587088 with learning rate 0.073509\n",
      "step 11500, training accuracy 0.910156\n",
      "Validation accuracy: 0.90965\n",
      "Minibatch loss at step 11600: 0.566269 with learning rate 0.073509\n",
      "step 11600, training accuracy 0.917969\n",
      "Validation accuracy: 0.90965\n",
      "Minibatch loss at step 11700: 0.475766 with learning rate 0.073509\n",
      "step 11700, training accuracy 0.933594\n",
      "Validation accuracy: 0.908733\n",
      "Minibatch loss at step 11800: 0.453941 with learning rate 0.073509\n",
      "step 11800, training accuracy 0.957031\n",
      "Validation accuracy: 0.909167\n",
      "Minibatch loss at step 11900: 0.528228 with learning rate 0.073509\n",
      "step 11900, training accuracy 0.929688\n",
      "Validation accuracy: 0.909433\n",
      "Minibatch loss at step 12000: 0.404354 with learning rate 0.073509\n",
      "step 12000, training accuracy 0.96875\n",
      "Validation accuracy: 0.909417\n",
      "Minibatch loss at step 12100: 0.520871 with learning rate 0.073509\n",
      "step 12100, training accuracy 0.929688\n",
      "Validation accuracy: 0.9093\n",
      "Minibatch loss at step 12200: 0.484568 with learning rate 0.073509\n",
      "step 12200, training accuracy 0.945312\n",
      "Validation accuracy: 0.9094\n",
      "Minibatch loss at step 12300: 0.493764 with learning rate 0.073509\n",
      "step 12300, training accuracy 0.917969\n",
      "Validation accuracy: 0.910033\n",
      "Minibatch loss at step 12400: 0.492023 with learning rate 0.073509\n",
      "step 12400, training accuracy 0.945312\n",
      "Validation accuracy: 0.910533\n",
      "Minibatch loss at step 12500: 0.550908 with learning rate 0.073509\n",
      "step 12500, training accuracy 0.917969\n",
      "Validation accuracy: 0.909867\n",
      "Minibatch loss at step 12600: 0.550170 with learning rate 0.073509\n",
      "step 12600, training accuracy 0.945312\n",
      "Validation accuracy: 0.909167\n",
      "Minibatch loss at step 12700: 0.569687 with learning rate 0.073509\n",
      "step 12700, training accuracy 0.917969\n",
      "Validation accuracy: 0.910667\n",
      "Minibatch loss at step 12800: 0.612925 with learning rate 0.073509\n",
      "step 12800, training accuracy 0.902344\n",
      "Validation accuracy: 0.91005\n",
      "Minibatch loss at step 12900: 0.456439 with learning rate 0.069834\n",
      "step 12900, training accuracy 0.945312\n",
      "Validation accuracy: 0.9103\n",
      "Minibatch loss at step 13000: 0.517868 with learning rate 0.069834\n",
      "step 13000, training accuracy 0.90625\n",
      "Validation accuracy: 0.910883\n",
      "Minibatch loss at step 13100: 0.486732 with learning rate 0.069834\n",
      "step 13100, training accuracy 0.933594\n",
      "Validation accuracy: 0.910917\n",
      "Minibatch loss at step 13200: 0.484849 with learning rate 0.069834\n",
      "step 13200, training accuracy 0.941406\n",
      "Validation accuracy: 0.911767\n",
      "Minibatch loss at step 13300: 0.518491 with learning rate 0.069834\n",
      "step 13300, training accuracy 0.917969\n",
      "Validation accuracy: 0.910817\n",
      "Minibatch loss at step 13400: 0.500549 with learning rate 0.069834\n",
      "step 13400, training accuracy 0.941406\n",
      "Validation accuracy: 0.911583\n",
      "Minibatch loss at step 13500: 0.437237 with learning rate 0.069834\n",
      "step 13500, training accuracy 0.941406\n",
      "Validation accuracy: 0.910433\n",
      "Minibatch loss at step 13600: 0.497089 with learning rate 0.069834\n",
      "step 13600, training accuracy 0.933594\n",
      "Validation accuracy: 0.911867\n",
      "Minibatch loss at step 13700: 0.468462 with learning rate 0.069834\n",
      "step 13700, training accuracy 0.933594\n",
      "Validation accuracy: 0.91125\n",
      "Minibatch loss at step 13800: 0.554835 with learning rate 0.069834\n",
      "step 13800, training accuracy 0.914062\n",
      "Validation accuracy: 0.911983\n",
      "Minibatch loss at step 13900: 0.500501 with learning rate 0.069834\n",
      "step 13900, training accuracy 0.941406\n",
      "Validation accuracy: 0.91085\n",
      "Minibatch loss at step 14000: 0.552635 with learning rate 0.069834\n",
      "step 14000, training accuracy 0.914062\n",
      "Validation accuracy: 0.911933\n",
      "Minibatch loss at step 14100: 0.513628 with learning rate 0.069834\n",
      "step 14100, training accuracy 0.929688\n",
      "Validation accuracy: 0.910683\n",
      "Minibatch loss at step 14200: 0.518224 with learning rate 0.069834\n",
      "step 14200, training accuracy 0.9375\n",
      "Validation accuracy: 0.911517\n",
      "Minibatch loss at step 14300: 0.521617 with learning rate 0.069834\n",
      "step 14300, training accuracy 0.917969\n",
      "Validation accuracy: 0.911667\n",
      "Minibatch loss at step 14400: 0.462446 with learning rate 0.069834\n",
      "step 14400, training accuracy 0.925781\n",
      "Validation accuracy: 0.912417\n",
      "Minibatch loss at step 14500: 0.581014 with learning rate 0.069834\n",
      "step 14500, training accuracy 0.902344\n",
      "Validation accuracy: 0.91205\n",
      "Minibatch loss at step 14600: 0.462479 with learning rate 0.069834\n",
      "step 14600, training accuracy 0.945312\n",
      "Validation accuracy: 0.9126\n",
      "Minibatch loss at step 14700: 0.460208 with learning rate 0.066342\n",
      "step 14700, training accuracy 0.929688\n",
      "Validation accuracy: 0.911983\n",
      "Minibatch loss at step 14800: 0.467166 with learning rate 0.066342\n",
      "step 14800, training accuracy 0.9375\n",
      "Validation accuracy: 0.911933\n",
      "Minibatch loss at step 14900: 0.464137 with learning rate 0.066342\n",
      "step 14900, training accuracy 0.9375\n",
      "Validation accuracy: 0.912883\n",
      "Minibatch loss at step 15000: 0.401098 with learning rate 0.066342\n",
      "step 15000, training accuracy 0.957031\n",
      "Validation accuracy: 0.91185\n",
      "Minibatch loss at step 15100: 0.560710 with learning rate 0.066342\n",
      "step 15100, training accuracy 0.902344\n",
      "Validation accuracy: 0.912817\n",
      "Minibatch loss at step 15200: 0.539743 with learning rate 0.066342\n",
      "step 15200, training accuracy 0.921875\n",
      "Validation accuracy: 0.9119\n",
      "Minibatch loss at step 15300: 0.432785 with learning rate 0.066342\n",
      "step 15300, training accuracy 0.9375\n",
      "Validation accuracy: 0.912817\n",
      "Minibatch loss at step 15400: 0.520418 with learning rate 0.066342\n",
      "step 15400, training accuracy 0.925781\n",
      "Validation accuracy: 0.912567\n",
      "Minibatch loss at step 15500: 0.549994 with learning rate 0.066342\n",
      "step 15500, training accuracy 0.921875\n",
      "Validation accuracy: 0.912783\n",
      "Minibatch loss at step 15600: 0.484000 with learning rate 0.066342\n",
      "step 15600, training accuracy 0.925781\n",
      "Validation accuracy: 0.91305\n",
      "Minibatch loss at step 15700: 0.506236 with learning rate 0.066342\n",
      "step 15700, training accuracy 0.9375\n",
      "Validation accuracy: 0.913117\n",
      "Minibatch loss at step 15800: 0.509241 with learning rate 0.066342\n",
      "step 15800, training accuracy 0.914062\n",
      "Validation accuracy: 0.91325\n",
      "Minibatch loss at step 15900: 0.436175 with learning rate 0.066342\n",
      "step 15900, training accuracy 0.949219\n",
      "Validation accuracy: 0.9138\n",
      "Minibatch loss at step 16000: 0.420880 with learning rate 0.066342\n",
      "step 16000, training accuracy 0.953125\n",
      "Validation accuracy: 0.912633\n",
      "Minibatch loss at step 16100: 0.414956 with learning rate 0.066342\n",
      "step 16100, training accuracy 0.941406\n",
      "Validation accuracy: 0.912967\n",
      "Minibatch loss at step 16200: 0.466800 with learning rate 0.066342\n",
      "step 16200, training accuracy 0.929688\n",
      "Validation accuracy: 0.913433\n",
      "Minibatch loss at step 16300: 0.453409 with learning rate 0.066342\n",
      "step 16300, training accuracy 0.929688\n",
      "Validation accuracy: 0.913983\n",
      "Minibatch loss at step 16400: 0.507933 with learning rate 0.066342\n",
      "step 16400, training accuracy 0.929688\n",
      "Validation accuracy: 0.914167\n",
      "Minibatch loss at step 16500: 0.464118 with learning rate 0.063025\n",
      "step 16500, training accuracy 0.925781\n",
      "Validation accuracy: 0.913383\n",
      "Minibatch loss at step 16600: 0.431159 with learning rate 0.063025\n",
      "step 16600, training accuracy 0.933594\n",
      "Validation accuracy: 0.914517\n",
      "Minibatch loss at step 16700: 0.408182 with learning rate 0.063025\n",
      "step 16700, training accuracy 0.957031\n",
      "Validation accuracy: 0.912783\n",
      "Minibatch loss at step 16800: 0.428767 with learning rate 0.063025\n",
      "step 16800, training accuracy 0.9375\n",
      "Validation accuracy: 0.914567\n",
      "Minibatch loss at step 16900: 0.444780 with learning rate 0.063025\n",
      "step 16900, training accuracy 0.925781\n",
      "Validation accuracy: 0.91325\n",
      "Minibatch loss at step 17000: 0.472542 with learning rate 0.063025\n",
      "step 17000, training accuracy 0.925781\n",
      "Validation accuracy: 0.914217\n",
      "Minibatch loss at step 17100: 0.408910 with learning rate 0.063025\n",
      "step 17100, training accuracy 0.929688\n",
      "Validation accuracy: 0.915317\n",
      "Minibatch loss at step 17200: 0.515712 with learning rate 0.063025\n",
      "step 17200, training accuracy 0.914062\n",
      "Validation accuracy: 0.914533\n",
      "Minibatch loss at step 17300: 0.460375 with learning rate 0.063025\n",
      "step 17300, training accuracy 0.9375\n",
      "Validation accuracy: 0.914867\n",
      "Minibatch loss at step 17400: 0.438965 with learning rate 0.063025\n",
      "step 17400, training accuracy 0.949219\n",
      "Validation accuracy: 0.914583\n",
      "Minibatch loss at step 17500: 0.543872 with learning rate 0.063025\n",
      "step 17500, training accuracy 0.902344\n",
      "Validation accuracy: 0.913967\n",
      "Minibatch loss at step 17600: 0.420489 with learning rate 0.063025\n",
      "step 17600, training accuracy 0.933594\n",
      "Validation accuracy: 0.914917\n",
      "Minibatch loss at step 17700: 0.447774 with learning rate 0.063025\n",
      "step 17700, training accuracy 0.917969\n",
      "Validation accuracy: 0.914917\n",
      "Minibatch loss at step 17800: 0.454343 with learning rate 0.063025\n",
      "step 17800, training accuracy 0.929688\n",
      "Validation accuracy: 0.9143\n",
      "Minibatch loss at step 17900: 0.381362 with learning rate 0.063025\n",
      "step 17900, training accuracy 0.957031\n",
      "Validation accuracy: 0.915283\n",
      "Minibatch loss at step 18000: 0.389785 with learning rate 0.063025\n",
      "step 18000, training accuracy 0.953125\n",
      "Validation accuracy: 0.915817\n",
      "Minibatch loss at step 18100: 0.362278 with learning rate 0.063025\n",
      "step 18100, training accuracy 0.960938\n",
      "Validation accuracy: 0.91595\n",
      "Minibatch loss at step 18200: 0.382345 with learning rate 0.063025\n",
      "step 18200, training accuracy 0.953125\n",
      "Validation accuracy: 0.915683\n",
      "Minibatch loss at step 18300: 0.322587 with learning rate 0.063025\n",
      "step 18300, training accuracy 0.964844\n",
      "Validation accuracy: 0.9155\n",
      "Minibatch loss at step 18400: 0.452645 with learning rate 0.059874\n",
      "step 18400, training accuracy 0.949219\n",
      "Validation accuracy: 0.916333\n",
      "Minibatch loss at step 18500: 0.435475 with learning rate 0.059874\n",
      "step 18500, training accuracy 0.933594\n",
      "Validation accuracy: 0.9158\n",
      "Minibatch loss at step 18600: 0.448693 with learning rate 0.059874\n",
      "step 18600, training accuracy 0.957031\n",
      "Validation accuracy: 0.915117\n",
      "Minibatch loss at step 18700: 0.458030 with learning rate 0.059874\n",
      "step 18700, training accuracy 0.929688\n",
      "Validation accuracy: 0.9159\n",
      "Minibatch loss at step 18800: 0.477545 with learning rate 0.059874\n",
      "step 18800, training accuracy 0.925781\n",
      "Validation accuracy: 0.91475\n",
      "Minibatch loss at step 18900: 0.475989 with learning rate 0.059874\n",
      "step 18900, training accuracy 0.914062\n",
      "Validation accuracy: 0.9161\n",
      "Minibatch loss at step 19000: 0.475389 with learning rate 0.059874\n",
      "step 19000, training accuracy 0.925781\n",
      "Validation accuracy: 0.91645\n",
      "Minibatch loss at step 19100: 0.417905 with learning rate 0.059874\n",
      "step 19100, training accuracy 0.960938\n",
      "Validation accuracy: 0.916483\n",
      "Minibatch loss at step 19200: 0.420237 with learning rate 0.059874\n",
      "step 19200, training accuracy 0.9375\n",
      "Validation accuracy: 0.915667\n",
      "Minibatch loss at step 19300: 0.409502 with learning rate 0.059874\n",
      "step 19300, training accuracy 0.957031\n",
      "Validation accuracy: 0.91545\n",
      "Minibatch loss at step 19400: 0.451624 with learning rate 0.059874\n",
      "step 19400, training accuracy 0.9375\n",
      "Validation accuracy: 0.9164\n",
      "Minibatch loss at step 19500: 0.452017 with learning rate 0.059874\n",
      "step 19500, training accuracy 0.925781\n",
      "Validation accuracy: 0.916083\n",
      "Minibatch loss at step 19600: 0.338686 with learning rate 0.059874\n",
      "step 19600, training accuracy 0.964844\n",
      "Validation accuracy: 0.916083\n",
      "Minibatch loss at step 19700: 0.441850 with learning rate 0.059874\n",
      "step 19700, training accuracy 0.917969\n",
      "Validation accuracy: 0.916533\n",
      "Minibatch loss at step 19800: 0.410432 with learning rate 0.059874\n",
      "step 19800, training accuracy 0.921875\n",
      "Validation accuracy: 0.915467\n",
      "Minibatch loss at step 19900: 0.383010 with learning rate 0.059874\n",
      "step 19900, training accuracy 0.953125\n",
      "Validation accuracy: 0.9164\n",
      "Minibatch loss at step 20000: 0.462049 with learning rate 0.059874\n",
      "step 20000, training accuracy 0.929688\n",
      "Validation accuracy: 0.916083\n",
      "Minibatch loss at step 20100: 0.380026 with learning rate 0.059874\n",
      "step 20100, training accuracy 0.941406\n",
      "Validation accuracy: 0.91695\n",
      "Minibatch loss at step 20200: 0.425617 with learning rate 0.056880\n",
      "step 20200, training accuracy 0.933594\n",
      "Validation accuracy: 0.915833\n",
      "Minibatch loss at step 20300: 0.414584 with learning rate 0.056880\n",
      "step 20300, training accuracy 0.941406\n",
      "Validation accuracy: 0.91705\n",
      "Minibatch loss at step 20400: 0.380805 with learning rate 0.056880\n",
      "step 20400, training accuracy 0.953125\n",
      "Validation accuracy: 0.916983\n",
      "Minibatch loss at step 20500: 0.387849 with learning rate 0.056880\n",
      "step 20500, training accuracy 0.9375\n",
      "Validation accuracy: 0.916717\n",
      "Minibatch loss at step 20600: 0.350283 with learning rate 0.056880\n",
      "step 20600, training accuracy 0.949219\n",
      "Validation accuracy: 0.916883\n",
      "Minibatch loss at step 20700: 0.375230 with learning rate 0.056880\n",
      "step 20700, training accuracy 0.949219\n",
      "Validation accuracy: 0.91685\n",
      "Minibatch loss at step 20800: 0.451078 with learning rate 0.056880\n",
      "step 20800, training accuracy 0.933594\n",
      "Validation accuracy: 0.917017\n",
      "Minibatch loss at step 20900: 0.436785 with learning rate 0.056880\n",
      "step 20900, training accuracy 0.933594\n",
      "Validation accuracy: 0.915967\n",
      "Minibatch loss at step 21000: 0.363564 with learning rate 0.056880\n",
      "step 21000, training accuracy 0.957031\n",
      "Validation accuracy: 0.917117\n",
      "Minibatch loss at step 21100: 0.429744 with learning rate 0.056880\n",
      "step 21100, training accuracy 0.914062\n",
      "Validation accuracy: 0.916283\n",
      "Minibatch loss at step 21200: 0.465458 with learning rate 0.056880\n",
      "step 21200, training accuracy 0.9375\n",
      "Validation accuracy: 0.916667\n",
      "Minibatch loss at step 21300: 0.454784 with learning rate 0.056880\n",
      "step 21300, training accuracy 0.9375\n",
      "Validation accuracy: 0.916867\n",
      "Minibatch loss at step 21400: 0.407174 with learning rate 0.056880\n",
      "step 21400, training accuracy 0.949219\n",
      "Validation accuracy: 0.9175\n",
      "Minibatch loss at step 21500: 0.458942 with learning rate 0.056880\n",
      "step 21500, training accuracy 0.917969\n",
      "Validation accuracy: 0.917017\n",
      "Minibatch loss at step 21600: 0.393394 with learning rate 0.056880\n",
      "step 21600, training accuracy 0.957031\n",
      "Validation accuracy: 0.916733\n",
      "Minibatch loss at step 21700: 0.362851 with learning rate 0.056880\n",
      "step 21700, training accuracy 0.960938\n",
      "Validation accuracy: 0.916917\n",
      "Minibatch loss at step 21800: 0.453626 with learning rate 0.056880\n",
      "step 21800, training accuracy 0.910156\n",
      "Validation accuracy: 0.917483\n",
      "Minibatch loss at step 21900: 0.397600 with learning rate 0.056880\n",
      "step 21900, training accuracy 0.941406\n",
      "Validation accuracy: 0.9177\n",
      "Minibatch loss at step 22000: 0.418648 with learning rate 0.054036\n",
      "step 22000, training accuracy 0.945312\n",
      "Validation accuracy: 0.9174\n",
      "Minibatch loss at step 22100: 0.422469 with learning rate 0.054036\n",
      "step 22100, training accuracy 0.933594\n",
      "Validation accuracy: 0.917417\n",
      "Minibatch loss at step 22200: 0.384137 with learning rate 0.054036\n",
      "step 22200, training accuracy 0.945312\n",
      "Validation accuracy: 0.917383\n",
      "Minibatch loss at step 22300: 0.405071 with learning rate 0.054036\n",
      "step 22300, training accuracy 0.957031\n",
      "Validation accuracy: 0.917283\n",
      "Minibatch loss at step 22400: 0.405216 with learning rate 0.054036\n",
      "step 22400, training accuracy 0.929688\n",
      "Validation accuracy: 0.917633\n",
      "Minibatch loss at step 22500: 0.411795 with learning rate 0.054036\n",
      "step 22500, training accuracy 0.957031\n",
      "Validation accuracy: 0.917583\n",
      "Minibatch loss at step 22600: 0.388711 with learning rate 0.054036\n",
      "step 22600, training accuracy 0.945312\n",
      "Validation accuracy: 0.917933\n",
      "Minibatch loss at step 22700: 0.390662 with learning rate 0.054036\n",
      "step 22700, training accuracy 0.9375\n",
      "Validation accuracy: 0.9177\n",
      "Minibatch loss at step 22800: 0.362096 with learning rate 0.054036\n",
      "step 22800, training accuracy 0.941406\n",
      "Validation accuracy: 0.917433\n",
      "Minibatch loss at step 22900: 0.386427 with learning rate 0.054036\n",
      "step 22900, training accuracy 0.933594\n",
      "Validation accuracy: 0.918283\n",
      "Minibatch loss at step 23000: 0.373117 with learning rate 0.054036\n",
      "step 23000, training accuracy 0.949219\n",
      "Validation accuracy: 0.917183\n",
      "Minibatch loss at step 23100: 0.292501 with learning rate 0.054036\n",
      "step 23100, training accuracy 0.980469\n",
      "Validation accuracy: 0.9177\n",
      "Minibatch loss at step 23200: 0.368751 with learning rate 0.054036\n",
      "step 23200, training accuracy 0.949219\n",
      "Validation accuracy: 0.91765\n",
      "Minibatch loss at step 23300: 0.373518 with learning rate 0.054036\n",
      "step 23300, training accuracy 0.949219\n",
      "Validation accuracy: 0.917633\n",
      "Minibatch loss at step 23400: 0.423538 with learning rate 0.054036\n",
      "step 23400, training accuracy 0.945312\n",
      "Validation accuracy: 0.917717\n",
      "Minibatch loss at step 23500: 0.533386 with learning rate 0.054036\n",
      "step 23500, training accuracy 0.921875\n",
      "Validation accuracy: 0.9172\n",
      "Minibatch loss at step 23600: 0.424011 with learning rate 0.054036\n",
      "step 23600, training accuracy 0.933594\n",
      "Validation accuracy: 0.918767\n",
      "Minibatch loss at step 23700: 0.298765 with learning rate 0.054036\n",
      "step 23700, training accuracy 0.964844\n",
      "Validation accuracy: 0.918783\n",
      "Minibatch loss at step 23800: 0.377034 with learning rate 0.054036\n",
      "step 23800, training accuracy 0.949219\n",
      "Validation accuracy: 0.918483\n",
      "Minibatch loss at step 23900: 0.458595 with learning rate 0.051334\n",
      "step 23900, training accuracy 0.929688\n",
      "Validation accuracy: 0.9179\n",
      "Minibatch loss at step 24000: 0.467566 with learning rate 0.051334\n",
      "step 24000, training accuracy 0.929688\n",
      "Validation accuracy: 0.91855\n",
      "Minibatch loss at step 24100: 0.409593 with learning rate 0.051334\n",
      "step 24100, training accuracy 0.929688\n",
      "Validation accuracy: 0.9188\n",
      "Minibatch loss at step 24200: 0.383636 with learning rate 0.051334\n",
      "step 24200, training accuracy 0.929688\n",
      "Validation accuracy: 0.919483\n",
      "Minibatch loss at step 24300: 0.360935 with learning rate 0.051334\n",
      "step 24300, training accuracy 0.9375\n",
      "Validation accuracy: 0.918667\n",
      "Minibatch loss at step 24400: 0.399033 with learning rate 0.051334\n",
      "step 24400, training accuracy 0.933594\n",
      "Validation accuracy: 0.919267\n",
      "Minibatch loss at step 24500: 0.346032 with learning rate 0.051334\n",
      "step 24500, training accuracy 0.96875\n",
      "Validation accuracy: 0.918417\n",
      "Minibatch loss at step 24600: 0.374046 with learning rate 0.051334\n",
      "step 24600, training accuracy 0.945312\n",
      "Validation accuracy: 0.918667\n",
      "Minibatch loss at step 24700: 0.418756 with learning rate 0.051334\n",
      "step 24700, training accuracy 0.929688\n",
      "Validation accuracy: 0.918183\n",
      "Minibatch loss at step 24800: 0.390092 with learning rate 0.051334\n",
      "step 24800, training accuracy 0.941406\n",
      "Validation accuracy: 0.918733\n",
      "Minibatch loss at step 24900: 0.379480 with learning rate 0.051334\n",
      "step 24900, training accuracy 0.949219\n",
      "Validation accuracy: 0.91755\n",
      "Minibatch loss at step 25000: 0.411427 with learning rate 0.051334\n",
      "step 25000, training accuracy 0.929688\n",
      "Validation accuracy: 0.917667\n",
      "Minibatch loss at step 25100: 0.379027 with learning rate 0.051334\n",
      "step 25100, training accuracy 0.929688\n",
      "Validation accuracy: 0.91815\n",
      "Minibatch loss at step 25200: 0.397352 with learning rate 0.051334\n",
      "step 25200, training accuracy 0.945312\n",
      "Validation accuracy: 0.918033\n",
      "Minibatch loss at step 25300: 0.368101 with learning rate 0.051334\n",
      "step 25300, training accuracy 0.945312\n",
      "Validation accuracy: 0.918433\n",
      "Minibatch loss at step 25400: 0.480448 with learning rate 0.051334\n",
      "step 25400, training accuracy 0.910156\n",
      "Validation accuracy: 0.918867\n",
      "Minibatch loss at step 25500: 0.402282 with learning rate 0.051334\n",
      "step 25500, training accuracy 0.9375\n",
      "Validation accuracy: 0.918583\n",
      "Minibatch loss at step 25600: 0.372716 with learning rate 0.051334\n",
      "step 25600, training accuracy 0.9375\n",
      "Validation accuracy: 0.91855\n",
      "Minibatch loss at step 25700: 0.461872 with learning rate 0.048767\n",
      "step 25700, training accuracy 0.929688\n",
      "Validation accuracy: 0.918667\n",
      "Minibatch loss at step 25800: 0.440439 with learning rate 0.048767\n",
      "step 25800, training accuracy 0.933594\n",
      "Validation accuracy: 0.918933\n",
      "Minibatch loss at step 25900: 0.384756 with learning rate 0.048767\n",
      "step 25900, training accuracy 0.949219\n",
      "Validation accuracy: 0.91875\n",
      "Minibatch loss at step 26000: 0.300489 with learning rate 0.048767\n",
      "step 26000, training accuracy 0.949219\n",
      "Validation accuracy: 0.91875\n",
      "Minibatch loss at step 26100: 0.444874 with learning rate 0.048767\n",
      "step 26100, training accuracy 0.925781\n",
      "Validation accuracy: 0.919533\n",
      "Minibatch loss at step 26200: 0.405684 with learning rate 0.048767\n",
      "step 26200, training accuracy 0.9375\n",
      "Validation accuracy: 0.91855\n",
      "Minibatch loss at step 26300: 0.403576 with learning rate 0.048767\n",
      "step 26300, training accuracy 0.929688\n",
      "Validation accuracy: 0.9198\n",
      "Minibatch loss at step 26400: 0.405430 with learning rate 0.048767\n",
      "step 26400, training accuracy 0.933594\n",
      "Validation accuracy: 0.9189\n",
      "Minibatch loss at step 26500: 0.360680 with learning rate 0.048767\n",
      "step 26500, training accuracy 0.949219\n",
      "Validation accuracy: 0.918367\n",
      "Minibatch loss at step 26600: 0.437273 with learning rate 0.048767\n",
      "step 26600, training accuracy 0.929688\n",
      "Validation accuracy: 0.9182\n",
      "Minibatch loss at step 26700: 0.319294 with learning rate 0.048767\n",
      "step 26700, training accuracy 0.964844\n",
      "Validation accuracy: 0.918367\n",
      "Minibatch loss at step 26800: 0.345886 with learning rate 0.048767\n",
      "step 26800, training accuracy 0.941406\n",
      "Validation accuracy: 0.918517\n",
      "Minibatch loss at step 26900: 0.346599 with learning rate 0.048767\n",
      "step 26900, training accuracy 0.949219\n",
      "Validation accuracy: 0.918917\n",
      "Minibatch loss at step 27000: 0.415091 with learning rate 0.048767\n",
      "step 27000, training accuracy 0.9375\n",
      "Validation accuracy: 0.917783\n",
      "Minibatch loss at step 27100: 0.388564 with learning rate 0.048767\n",
      "step 27100, training accuracy 0.933594\n",
      "Validation accuracy: 0.9185\n",
      "Minibatch loss at step 27200: 0.429778 with learning rate 0.048767\n",
      "step 27200, training accuracy 0.9375\n",
      "Validation accuracy: 0.918917\n",
      "Minibatch loss at step 27300: 0.365598 with learning rate 0.048767\n",
      "step 27300, training accuracy 0.949219\n",
      "Validation accuracy: 0.91735\n",
      "Minibatch loss at step 27400: 0.338330 with learning rate 0.048767\n",
      "step 27400, training accuracy 0.941406\n",
      "Validation accuracy: 0.919083\n",
      "Minibatch loss at step 27500: 0.415391 with learning rate 0.046329\n",
      "step 27500, training accuracy 0.9375\n",
      "Validation accuracy: 0.9168\n",
      "Minibatch loss at step 27600: 0.391517 with learning rate 0.046329\n",
      "step 27600, training accuracy 0.957031\n",
      "Validation accuracy: 0.918833\n",
      "Minibatch loss at step 27700: 0.363593 with learning rate 0.046329\n",
      "step 27700, training accuracy 0.953125\n",
      "Validation accuracy: 0.9197\n",
      "Minibatch loss at step 27800: 0.391413 with learning rate 0.046329\n",
      "step 27800, training accuracy 0.941406\n",
      "Validation accuracy: 0.919917\n",
      "Minibatch loss at step 27900: 0.308419 with learning rate 0.046329\n",
      "step 27900, training accuracy 0.953125\n",
      "Validation accuracy: 0.91885\n",
      "Minibatch loss at step 28000: 0.372800 with learning rate 0.046329\n",
      "step 28000, training accuracy 0.949219\n",
      "Validation accuracy: 0.919383\n",
      "Minibatch loss at step 28100: 0.307018 with learning rate 0.046329\n",
      "step 28100, training accuracy 0.960938\n",
      "Validation accuracy: 0.91975\n",
      "Minibatch loss at step 28200: 0.256183 with learning rate 0.046329\n",
      "step 28200, training accuracy 0.960938\n",
      "Validation accuracy: 0.918817\n",
      "Minibatch loss at step 28300: 0.399367 with learning rate 0.046329\n",
      "step 28300, training accuracy 0.945312\n",
      "Validation accuracy: 0.919167\n",
      "Minibatch loss at step 28400: 0.358465 with learning rate 0.046329\n",
      "step 28400, training accuracy 0.957031\n",
      "Validation accuracy: 0.919767\n",
      "Minibatch loss at step 28500: 0.373713 with learning rate 0.046329\n",
      "step 28500, training accuracy 0.945312\n",
      "Validation accuracy: 0.918317\n",
      "Minibatch loss at step 28600: 0.430075 with learning rate 0.046329\n",
      "step 28600, training accuracy 0.9375\n",
      "Validation accuracy: 0.920383\n",
      "Minibatch loss at step 28700: 0.309948 with learning rate 0.046329\n",
      "step 28700, training accuracy 0.980469\n",
      "Validation accuracy: 0.920067\n",
      "Minibatch loss at step 28800: 0.294628 with learning rate 0.046329\n",
      "step 28800, training accuracy 0.953125\n",
      "Validation accuracy: 0.919083\n",
      "Minibatch loss at step 28900: 0.307345 with learning rate 0.046329\n",
      "step 28900, training accuracy 0.960938\n",
      "Validation accuracy: 0.91865\n",
      "Minibatch loss at step 29000: 0.303995 with learning rate 0.046329\n",
      "step 29000, training accuracy 0.957031\n",
      "Validation accuracy: 0.91965\n",
      "Minibatch loss at step 29100: 0.465165 with learning rate 0.046329\n",
      "step 29100, training accuracy 0.933594\n",
      "Validation accuracy: 0.920617\n",
      "Minibatch loss at step 29200: 0.411005 with learning rate 0.046329\n",
      "step 29200, training accuracy 0.929688\n",
      "Validation accuracy: 0.91945\n",
      "Minibatch loss at step 29300: 0.366692 with learning rate 0.046329\n",
      "step 29300, training accuracy 0.945312\n",
      "Validation accuracy: 0.92025\n",
      "Minibatch loss at step 29400: 0.312988 with learning rate 0.044013\n",
      "step 29400, training accuracy 0.972656\n",
      "Validation accuracy: 0.9199\n",
      "Minibatch loss at step 29500: 0.426350 with learning rate 0.044013\n",
      "step 29500, training accuracy 0.933594\n",
      "Validation accuracy: 0.919767\n",
      "Minibatch loss at step 29600: 0.432281 with learning rate 0.044013\n",
      "step 29600, training accuracy 0.933594\n",
      "Validation accuracy: 0.920783\n",
      "Minibatch loss at step 29700: 0.418919 with learning rate 0.044013\n",
      "step 29700, training accuracy 0.941406\n",
      "Validation accuracy: 0.920267\n",
      "Minibatch loss at step 29800: 0.307157 with learning rate 0.044013\n",
      "step 29800, training accuracy 0.96875\n",
      "Validation accuracy: 0.920717\n",
      "Minibatch loss at step 29900: 0.385392 with learning rate 0.044013\n",
      "step 29900, training accuracy 0.9375\n",
      "Validation accuracy: 0.920083\n",
      "Minibatch loss at step 30000: 0.317590 with learning rate 0.044013\n",
      "step 30000, training accuracy 0.957031\n",
      "Validation accuracy: 0.92005\n",
      "Minibatch loss at step 30100: 0.382553 with learning rate 0.044013\n",
      "step 30100, training accuracy 0.949219\n",
      "Validation accuracy: 0.920567\n",
      "Minibatch loss at step 30200: 0.435313 with learning rate 0.044013\n",
      "step 30200, training accuracy 0.933594\n",
      "Validation accuracy: 0.919917\n",
      "Minibatch loss at step 30300: 0.384205 with learning rate 0.044013\n",
      "step 30300, training accuracy 0.9375\n",
      "Validation accuracy: 0.9211\n",
      "Minibatch loss at step 30400: 0.357314 with learning rate 0.044013\n",
      "step 30400, training accuracy 0.945312\n",
      "Validation accuracy: 0.920117\n",
      "Minibatch loss at step 30500: 0.453915 with learning rate 0.044013\n",
      "step 30500, training accuracy 0.914062\n",
      "Validation accuracy: 0.9195\n",
      "Minibatch loss at step 30600: 0.325768 with learning rate 0.044013\n",
      "step 30600, training accuracy 0.953125\n",
      "Validation accuracy: 0.920467\n",
      "Minibatch loss at step 30700: 0.355302 with learning rate 0.044013\n",
      "step 30700, training accuracy 0.957031\n",
      "Validation accuracy: 0.919233\n",
      "Minibatch loss at step 30800: 0.413808 with learning rate 0.044013\n",
      "step 30800, training accuracy 0.9375\n",
      "Validation accuracy: 0.920267\n",
      "Minibatch loss at step 30900: 0.321980 with learning rate 0.044013\n",
      "step 30900, training accuracy 0.941406\n",
      "Validation accuracy: 0.9199\n",
      "Minibatch loss at step 31000: 0.321150 with learning rate 0.044013\n",
      "step 31000, training accuracy 0.949219\n",
      "Validation accuracy: 0.919817\n",
      "Minibatch loss at step 31100: 0.288055 with learning rate 0.044013\n",
      "step 31100, training accuracy 0.964844\n",
      "Validation accuracy: 0.91995\n",
      "Minibatch loss at step 31200: 0.296386 with learning rate 0.041812\n",
      "step 31200, training accuracy 0.949219\n",
      "Validation accuracy: 0.9211\n",
      "Minibatch loss at step 31300: 0.288072 with learning rate 0.041812\n",
      "step 31300, training accuracy 0.964844\n",
      "Validation accuracy: 0.921333\n",
      "Minibatch loss at step 31400: 0.297167 with learning rate 0.041812\n",
      "step 31400, training accuracy 0.957031\n",
      "Validation accuracy: 0.920483\n",
      "Minibatch loss at step 31500: 0.397800 with learning rate 0.041812\n",
      "step 31500, training accuracy 0.933594\n",
      "Validation accuracy: 0.92175\n",
      "Minibatch loss at step 31600: 0.283563 with learning rate 0.041812\n",
      "step 31600, training accuracy 0.960938\n",
      "Validation accuracy: 0.920483\n",
      "Minibatch loss at step 31700: 0.332783 with learning rate 0.041812\n",
      "step 31700, training accuracy 0.957031\n",
      "Validation accuracy: 0.9216\n",
      "Minibatch loss at step 31800: 0.275751 with learning rate 0.041812\n",
      "step 31800, training accuracy 0.976562\n",
      "Validation accuracy: 0.920967\n",
      "Minibatch loss at step 31900: 0.305583 with learning rate 0.041812\n",
      "step 31900, training accuracy 0.960938\n",
      "Validation accuracy: 0.920417\n",
      "Minibatch loss at step 32000: 0.316360 with learning rate 0.041812\n",
      "step 32000, training accuracy 0.960938\n",
      "Validation accuracy: 0.920133\n",
      "Minibatch loss at step 32100: 0.414443 with learning rate 0.041812\n",
      "step 32100, training accuracy 0.949219\n",
      "Validation accuracy: 0.919033\n",
      "Minibatch loss at step 32200: 0.398842 with learning rate 0.041812\n",
      "step 32200, training accuracy 0.933594\n",
      "Validation accuracy: 0.920367\n",
      "Minibatch loss at step 32300: 0.308876 with learning rate 0.041812\n",
      "step 32300, training accuracy 0.949219\n",
      "Validation accuracy: 0.920233\n",
      "Minibatch loss at step 32400: 0.304612 with learning rate 0.041812\n",
      "step 32400, training accuracy 0.96875\n",
      "Validation accuracy: 0.92125\n",
      "Minibatch loss at step 32500: 0.413573 with learning rate 0.041812\n",
      "step 32500, training accuracy 0.933594\n",
      "Validation accuracy: 0.920317\n",
      "Minibatch loss at step 32600: 0.327529 with learning rate 0.041812\n",
      "step 32600, training accuracy 0.96875\n",
      "Validation accuracy: 0.921317\n",
      "Minibatch loss at step 32700: 0.375440 with learning rate 0.041812\n",
      "step 32700, training accuracy 0.9375\n",
      "Validation accuracy: 0.920933\n",
      "Minibatch loss at step 32800: 0.390057 with learning rate 0.041812\n",
      "step 32800, training accuracy 0.9375\n",
      "Validation accuracy: 0.921483\n",
      "Minibatch loss at step 32900: 0.351124 with learning rate 0.041812\n",
      "step 32900, training accuracy 0.949219\n",
      "Validation accuracy: 0.920617\n",
      "Minibatch loss at step 33000: 0.278701 with learning rate 0.039721\n",
      "step 33000, training accuracy 0.964844\n",
      "Validation accuracy: 0.921383\n",
      "Minibatch loss at step 33100: 0.329694 with learning rate 0.039721\n",
      "step 33100, training accuracy 0.960938\n",
      "Validation accuracy: 0.920483\n",
      "Minibatch loss at step 33200: 0.315241 with learning rate 0.039721\n",
      "step 33200, training accuracy 0.949219\n",
      "Validation accuracy: 0.921967\n",
      "Minibatch loss at step 33300: 0.373177 with learning rate 0.039721\n",
      "step 33300, training accuracy 0.941406\n",
      "Validation accuracy: 0.921633\n",
      "Minibatch loss at step 33400: 0.364284 with learning rate 0.039721\n",
      "step 33400, training accuracy 0.949219\n",
      "Validation accuracy: 0.921433\n",
      "Minibatch loss at step 33500: 0.374329 with learning rate 0.039721\n",
      "step 33500, training accuracy 0.949219\n",
      "Validation accuracy: 0.920467\n",
      "Minibatch loss at step 33600: 0.369724 with learning rate 0.039721\n",
      "step 33600, training accuracy 0.949219\n",
      "Validation accuracy: 0.921183\n",
      "Minibatch loss at step 33700: 0.418711 with learning rate 0.039721\n",
      "step 33700, training accuracy 0.929688\n",
      "Validation accuracy: 0.9203\n",
      "Minibatch loss at step 33800: 0.272517 with learning rate 0.039721\n",
      "step 33800, training accuracy 0.96875\n",
      "Validation accuracy: 0.921267\n",
      "Minibatch loss at step 33900: 0.285347 with learning rate 0.039721\n",
      "step 33900, training accuracy 0.957031\n",
      "Validation accuracy: 0.921883\n",
      "Minibatch loss at step 34000: 0.342465 with learning rate 0.039721\n",
      "step 34000, training accuracy 0.929688\n",
      "Validation accuracy: 0.92065\n",
      "Minibatch loss at step 34100: 0.386651 with learning rate 0.039721\n",
      "step 34100, training accuracy 0.949219\n",
      "Validation accuracy: 0.9206\n",
      "Minibatch loss at step 34200: 0.316142 with learning rate 0.039721\n",
      "step 34200, training accuracy 0.96875\n",
      "Validation accuracy: 0.921717\n",
      "Minibatch loss at step 34300: 0.393277 with learning rate 0.039721\n",
      "step 34300, training accuracy 0.945312\n",
      "Validation accuracy: 0.9219\n",
      "Minibatch loss at step 34400: 0.314975 with learning rate 0.039721\n",
      "step 34400, training accuracy 0.957031\n",
      "Validation accuracy: 0.920983\n",
      "Minibatch loss at step 34500: 0.345662 with learning rate 0.039721\n",
      "step 34500, training accuracy 0.957031\n",
      "Validation accuracy: 0.920867\n",
      "Minibatch loss at step 34600: 0.319459 with learning rate 0.039721\n",
      "step 34600, training accuracy 0.964844\n",
      "Validation accuracy: 0.92165\n",
      "Minibatch loss at step 34700: 0.323978 with learning rate 0.039721\n",
      "step 34700, training accuracy 0.964844\n",
      "Validation accuracy: 0.92055\n",
      "Minibatch loss at step 34800: 0.349738 with learning rate 0.039721\n",
      "step 34800, training accuracy 0.949219\n",
      "Validation accuracy: 0.921433\n",
      "Minibatch loss at step 34900: 0.279566 with learning rate 0.037735\n",
      "step 34900, training accuracy 0.96875\n",
      "Validation accuracy: 0.921883\n",
      "Minibatch loss at step 35000: 0.307029 with learning rate 0.037735\n",
      "step 35000, training accuracy 0.960938\n",
      "Validation accuracy: 0.921267\n",
      "Minibatch loss at step 35100: 0.346126 with learning rate 0.037735\n",
      "step 35100, training accuracy 0.953125\n",
      "Validation accuracy: 0.922033\n",
      "Minibatch loss at step 35200: 0.326804 with learning rate 0.037735\n",
      "step 35200, training accuracy 0.953125\n",
      "Validation accuracy: 0.920567\n",
      "Minibatch loss at step 35300: 0.337929 with learning rate 0.037735\n",
      "step 35300, training accuracy 0.957031\n",
      "Validation accuracy: 0.9214\n",
      "Minibatch loss at step 35400: 0.371373 with learning rate 0.037735\n",
      "step 35400, training accuracy 0.945312\n",
      "Validation accuracy: 0.921567\n",
      "Minibatch loss at step 35500: 0.369082 with learning rate 0.037735\n",
      "step 35500, training accuracy 0.953125\n",
      "Validation accuracy: 0.922217\n",
      "Minibatch loss at step 35600: 0.386135 with learning rate 0.037735\n",
      "step 35600, training accuracy 0.945312\n",
      "Validation accuracy: 0.921783\n",
      "Minibatch loss at step 35700: 0.287226 with learning rate 0.037735\n",
      "step 35700, training accuracy 0.960938\n",
      "Validation accuracy: 0.921533\n",
      "Minibatch loss at step 35800: 0.307506 with learning rate 0.037735\n",
      "step 35800, training accuracy 0.957031\n",
      "Validation accuracy: 0.922317\n",
      "Minibatch loss at step 35900: 0.370668 with learning rate 0.037735\n",
      "step 35900, training accuracy 0.941406\n",
      "Validation accuracy: 0.9216\n",
      "Minibatch loss at step 36000: 0.378634 with learning rate 0.037735\n",
      "step 36000, training accuracy 0.953125\n",
      "Validation accuracy: 0.921217\n",
      "Minibatch loss at step 36100: 0.303173 with learning rate 0.037735\n",
      "step 36100, training accuracy 0.957031\n",
      "Validation accuracy: 0.921317\n",
      "Minibatch loss at step 36200: 0.347965 with learning rate 0.037735\n",
      "step 36200, training accuracy 0.953125\n",
      "Validation accuracy: 0.9215\n",
      "Minibatch loss at step 36300: 0.354826 with learning rate 0.037735\n",
      "step 36300, training accuracy 0.949219\n",
      "Validation accuracy: 0.921117\n",
      "Minibatch loss at step 36400: 0.363629 with learning rate 0.037735\n",
      "step 36400, training accuracy 0.949219\n",
      "Validation accuracy: 0.921\n",
      "Minibatch loss at step 36500: 0.320594 with learning rate 0.037735\n",
      "step 36500, training accuracy 0.960938\n",
      "Validation accuracy: 0.920867\n",
      "Minibatch loss at step 36600: 0.320287 with learning rate 0.037735\n",
      "step 36600, training accuracy 0.972656\n",
      "Validation accuracy: 0.921433\n",
      "Minibatch loss at step 36700: 0.315290 with learning rate 0.035849\n",
      "step 36700, training accuracy 0.964844\n",
      "Validation accuracy: 0.922333\n",
      "Minibatch loss at step 36800: 0.283950 with learning rate 0.035849\n",
      "step 36800, training accuracy 0.96875\n",
      "Validation accuracy: 0.921483\n",
      "Minibatch loss at step 36900: 0.369672 with learning rate 0.035849\n",
      "step 36900, training accuracy 0.957031\n",
      "Validation accuracy: 0.9219\n",
      "Minibatch loss at step 37000: 0.364730 with learning rate 0.035849\n",
      "step 37000, training accuracy 0.960938\n",
      "Validation accuracy: 0.92145\n",
      "Minibatch loss at step 37100: 0.395963 with learning rate 0.035849\n",
      "step 37100, training accuracy 0.941406\n",
      "Validation accuracy: 0.9225\n",
      "Minibatch loss at step 37200: 0.264164 with learning rate 0.035849\n",
      "step 37200, training accuracy 0.96875\n",
      "Validation accuracy: 0.922267\n",
      "Minibatch loss at step 37300: 0.308190 with learning rate 0.035849\n",
      "step 37300, training accuracy 0.96875\n",
      "Validation accuracy: 0.921333\n",
      "Minibatch loss at step 37400: 0.426926 with learning rate 0.035849\n",
      "step 37400, training accuracy 0.9375\n",
      "Validation accuracy: 0.92305\n",
      "Minibatch loss at step 37500: 0.301798 with learning rate 0.035849\n",
      "step 37500, training accuracy 0.960938\n",
      "Validation accuracy: 0.922683\n",
      "Minibatch loss at step 37600: 0.360734 with learning rate 0.035849\n",
      "step 37600, training accuracy 0.941406\n",
      "Validation accuracy: 0.922667\n",
      "Minibatch loss at step 37700: 0.305905 with learning rate 0.035849\n",
      "step 37700, training accuracy 0.960938\n",
      "Validation accuracy: 0.922383\n",
      "Minibatch loss at step 37800: 0.343161 with learning rate 0.035849\n",
      "step 37800, training accuracy 0.949219\n",
      "Validation accuracy: 0.9223\n",
      "Minibatch loss at step 37900: 0.300222 with learning rate 0.035849\n",
      "step 37900, training accuracy 0.964844\n",
      "Validation accuracy: 0.921917\n",
      "Minibatch loss at step 38000: 0.281104 with learning rate 0.035849\n",
      "step 38000, training accuracy 0.957031\n",
      "Validation accuracy: 0.920917\n",
      "Minibatch loss at step 38100: 0.339323 with learning rate 0.035849\n",
      "step 38100, training accuracy 0.945312\n",
      "Validation accuracy: 0.922517\n",
      "Minibatch loss at step 38200: 0.347574 with learning rate 0.035849\n",
      "step 38200, training accuracy 0.957031\n",
      "Validation accuracy: 0.921917\n",
      "Minibatch loss at step 38300: 0.286433 with learning rate 0.035849\n",
      "step 38300, training accuracy 0.953125\n",
      "Validation accuracy: 0.92205\n",
      "Minibatch loss at step 38400: 0.308383 with learning rate 0.035849\n",
      "step 38400, training accuracy 0.941406\n",
      "Validation accuracy: 0.922783\n",
      "Minibatch loss at step 38500: 0.316511 with learning rate 0.034056\n",
      "step 38500, training accuracy 0.960938\n",
      "Validation accuracy: 0.922933\n",
      "Minibatch loss at step 38600: 0.370366 with learning rate 0.034056\n",
      "step 38600, training accuracy 0.945312\n",
      "Validation accuracy: 0.922767\n",
      "Minibatch loss at step 38700: 0.249262 with learning rate 0.034056\n",
      "step 38700, training accuracy 0.976562\n",
      "Validation accuracy: 0.922683\n",
      "Minibatch loss at step 38800: 0.396213 with learning rate 0.034056\n",
      "step 38800, training accuracy 0.941406\n",
      "Validation accuracy: 0.922967\n",
      "Minibatch loss at step 38900: 0.401485 with learning rate 0.034056\n",
      "step 38900, training accuracy 0.925781\n",
      "Validation accuracy: 0.9235\n",
      "Minibatch loss at step 39000: 0.357895 with learning rate 0.034056\n",
      "step 39000, training accuracy 0.964844\n",
      "Validation accuracy: 0.9227\n",
      "Minibatch loss at step 39100: 0.294486 with learning rate 0.034056\n",
      "step 39100, training accuracy 0.96875\n",
      "Validation accuracy: 0.921917\n",
      "Minibatch loss at step 39200: 0.295936 with learning rate 0.034056\n",
      "step 39200, training accuracy 0.953125\n",
      "Validation accuracy: 0.922267\n",
      "Minibatch loss at step 39300: 0.290966 with learning rate 0.034056\n",
      "step 39300, training accuracy 0.96875\n",
      "Validation accuracy: 0.923383\n",
      "Minibatch loss at step 39400: 0.261798 with learning rate 0.034056\n",
      "step 39400, training accuracy 0.960938\n",
      "Validation accuracy: 0.922533\n",
      "Minibatch loss at step 39500: 0.319003 with learning rate 0.034056\n",
      "step 39500, training accuracy 0.953125\n",
      "Validation accuracy: 0.922283\n",
      "Minibatch loss at step 39600: 0.335832 with learning rate 0.034056\n",
      "step 39600, training accuracy 0.972656\n",
      "Validation accuracy: 0.922317\n",
      "Minibatch loss at step 39700: 0.264989 with learning rate 0.034056\n",
      "step 39700, training accuracy 0.957031\n",
      "Validation accuracy: 0.922417\n",
      "Minibatch loss at step 39800: 0.354118 with learning rate 0.034056\n",
      "step 39800, training accuracy 0.953125\n",
      "Validation accuracy: 0.92215\n",
      "Minibatch loss at step 39900: 0.246937 with learning rate 0.034056\n",
      "step 39900, training accuracy 0.960938\n",
      "Validation accuracy: 0.921633\n",
      "Minibatch loss at step 40000: 0.320041 with learning rate 0.034056\n",
      "step 40000, training accuracy 0.957031\n",
      "Validation accuracy: 0.922683\n",
      "Minibatch loss at step 40100: 0.290458 with learning rate 0.034056\n",
      "step 40100, training accuracy 0.957031\n",
      "Validation accuracy: 0.922867\n",
      "Minibatch loss at step 40200: 0.362775 with learning rate 0.034056\n",
      "step 40200, training accuracy 0.9375\n",
      "Validation accuracy: 0.922383\n",
      "Minibatch loss at step 40300: 0.335988 with learning rate 0.034056\n",
      "step 40300, training accuracy 0.945312\n",
      "Validation accuracy: 0.922783\n",
      "Minibatch loss at step 40400: 0.293386 with learning rate 0.032353\n",
      "step 40400, training accuracy 0.960938\n",
      "Validation accuracy: 0.923067\n",
      "Minibatch loss at step 40500: 0.320396 with learning rate 0.032353\n",
      "step 40500, training accuracy 0.964844\n",
      "Validation accuracy: 0.9234\n",
      "Minibatch loss at step 40600: 0.293570 with learning rate 0.032353\n",
      "step 40600, training accuracy 0.972656\n",
      "Validation accuracy: 0.922933\n",
      "Minibatch loss at step 40700: 0.328204 with learning rate 0.032353\n",
      "step 40700, training accuracy 0.945312\n",
      "Validation accuracy: 0.922333\n",
      "Minibatch loss at step 40800: 0.357454 with learning rate 0.032353\n",
      "step 40800, training accuracy 0.933594\n",
      "Validation accuracy: 0.921983\n",
      "Minibatch loss at step 40900: 0.275730 with learning rate 0.032353\n",
      "step 40900, training accuracy 0.964844\n",
      "Validation accuracy: 0.923583\n",
      "Minibatch loss at step 41000: 0.379201 with learning rate 0.032353\n",
      "step 41000, training accuracy 0.945312\n",
      "Validation accuracy: 0.923333\n",
      "Minibatch loss at step 41100: 0.281094 with learning rate 0.032353\n",
      "step 41100, training accuracy 0.96875\n",
      "Validation accuracy: 0.9232\n",
      "Minibatch loss at step 41200: 0.284919 with learning rate 0.032353\n",
      "step 41200, training accuracy 0.960938\n",
      "Validation accuracy: 0.922\n",
      "Minibatch loss at step 41300: 0.374047 with learning rate 0.032353\n",
      "step 41300, training accuracy 0.941406\n",
      "Validation accuracy: 0.923633\n",
      "Minibatch loss at step 41400: 0.487304 with learning rate 0.032353\n",
      "step 41400, training accuracy 0.910156\n",
      "Validation accuracy: 0.923317\n",
      "Minibatch loss at step 41500: 0.192737 with learning rate 0.032353\n",
      "step 41500, training accuracy 0.976562\n",
      "Validation accuracy: 0.922933\n",
      "Minibatch loss at step 41600: 0.310187 with learning rate 0.032353\n",
      "step 41600, training accuracy 0.945312\n",
      "Validation accuracy: 0.9231\n",
      "Minibatch loss at step 41700: 0.327632 with learning rate 0.032353\n",
      "step 41700, training accuracy 0.9375\n",
      "Validation accuracy: 0.923517\n",
      "Minibatch loss at step 41800: 0.364078 with learning rate 0.032353\n",
      "step 41800, training accuracy 0.941406\n",
      "Validation accuracy: 0.922567\n",
      "Minibatch loss at step 41900: 0.311184 with learning rate 0.032353\n",
      "step 41900, training accuracy 0.96875\n",
      "Validation accuracy: 0.9231\n",
      "Minibatch loss at step 42000: 0.361546 with learning rate 0.032353\n",
      "step 42000, training accuracy 0.941406\n",
      "Validation accuracy: 0.922633\n",
      "Minibatch loss at step 42100: 0.307072 with learning rate 0.032353\n",
      "step 42100, training accuracy 0.960938\n",
      "Validation accuracy: 0.92215\n",
      "Minibatch loss at step 42200: 0.330715 with learning rate 0.030736\n",
      "step 42200, training accuracy 0.953125\n",
      "Validation accuracy: 0.92375\n",
      "Minibatch loss at step 42300: 0.262938 with learning rate 0.030736\n",
      "step 42300, training accuracy 0.964844\n",
      "Validation accuracy: 0.923217\n",
      "Minibatch loss at step 42400: 0.356466 with learning rate 0.030736\n",
      "step 42400, training accuracy 0.933594\n",
      "Validation accuracy: 0.92375\n",
      "Minibatch loss at step 42500: 0.245680 with learning rate 0.030736\n",
      "step 42500, training accuracy 0.960938\n",
      "Validation accuracy: 0.923717\n",
      "Minibatch loss at step 42600: 0.294987 with learning rate 0.030736\n",
      "step 42600, training accuracy 0.976562\n",
      "Validation accuracy: 0.923783\n",
      "Minibatch loss at step 42700: 0.327690 with learning rate 0.030736\n",
      "step 42700, training accuracy 0.949219\n",
      "Validation accuracy: 0.922633\n",
      "Minibatch loss at step 42800: 0.212164 with learning rate 0.030736\n",
      "step 42800, training accuracy 0.976562\n",
      "Validation accuracy: 0.92355\n",
      "Minibatch loss at step 42900: 0.334948 with learning rate 0.030736\n",
      "step 42900, training accuracy 0.953125\n",
      "Validation accuracy: 0.924067\n",
      "Minibatch loss at step 43000: 0.275539 with learning rate 0.030736\n",
      "step 43000, training accuracy 0.964844\n",
      "Validation accuracy: 0.923217\n",
      "Minibatch loss at step 43100: 0.319115 with learning rate 0.030736\n",
      "step 43100, training accuracy 0.933594\n",
      "Validation accuracy: 0.923667\n",
      "Minibatch loss at step 43200: 0.295279 with learning rate 0.030736\n",
      "step 43200, training accuracy 0.960938\n",
      "Validation accuracy: 0.923133\n",
      "Minibatch loss at step 43300: 0.321577 with learning rate 0.030736\n",
      "step 43300, training accuracy 0.972656\n",
      "Validation accuracy: 0.923033\n",
      "Minibatch loss at step 43400: 0.319916 with learning rate 0.030736\n",
      "step 43400, training accuracy 0.953125\n",
      "Validation accuracy: 0.922967\n",
      "Minibatch loss at step 43500: 0.372951 with learning rate 0.030736\n",
      "step 43500, training accuracy 0.941406\n",
      "Validation accuracy: 0.923383\n",
      "Minibatch loss at step 43600: 0.281756 with learning rate 0.030736\n",
      "step 43600, training accuracy 0.972656\n",
      "Validation accuracy: 0.923267\n",
      "Minibatch loss at step 43700: 0.266167 with learning rate 0.030736\n",
      "step 43700, training accuracy 0.957031\n",
      "Validation accuracy: 0.92375\n",
      "Minibatch loss at step 43800: 0.232152 with learning rate 0.030736\n",
      "step 43800, training accuracy 0.976562\n",
      "Validation accuracy: 0.923917\n",
      "Minibatch loss at step 43900: 0.256066 with learning rate 0.030736\n",
      "step 43900, training accuracy 0.976562\n",
      "Validation accuracy: 0.923817\n",
      "Minibatch loss at step 44000: 0.362422 with learning rate 0.029199\n",
      "step 44000, training accuracy 0.929688\n",
      "Validation accuracy: 0.9234\n",
      "Minibatch loss at step 44100: 0.308058 with learning rate 0.029199\n",
      "step 44100, training accuracy 0.957031\n",
      "Validation accuracy: 0.923617\n",
      "Minibatch loss at step 44200: 0.224261 with learning rate 0.029199\n",
      "step 44200, training accuracy 0.980469\n",
      "Validation accuracy: 0.9239\n",
      "Minibatch loss at step 44300: 0.270465 with learning rate 0.029199\n",
      "step 44300, training accuracy 0.96875\n",
      "Validation accuracy: 0.923733\n",
      "Minibatch loss at step 44400: 0.310141 with learning rate 0.029199\n",
      "step 44400, training accuracy 0.976562\n",
      "Validation accuracy: 0.92405\n",
      "Minibatch loss at step 44500: 0.336906 with learning rate 0.029199\n",
      "step 44500, training accuracy 0.9375\n",
      "Validation accuracy: 0.923833\n",
      "Minibatch loss at step 44600: 0.219625 with learning rate 0.029199\n",
      "step 44600, training accuracy 0.976562\n",
      "Validation accuracy: 0.92345\n",
      "Minibatch loss at step 44700: 0.271456 with learning rate 0.029199\n",
      "step 44700, training accuracy 0.972656\n",
      "Validation accuracy: 0.923683\n",
      "Minibatch loss at step 44800: 0.374116 with learning rate 0.029199\n",
      "step 44800, training accuracy 0.949219\n",
      "Validation accuracy: 0.923033\n",
      "Minibatch loss at step 44900: 0.360651 with learning rate 0.029199\n",
      "step 44900, training accuracy 0.957031\n",
      "Validation accuracy: 0.9232\n",
      "Minibatch loss at step 45000: 0.302498 with learning rate 0.029199\n",
      "step 45000, training accuracy 0.96875\n",
      "Validation accuracy: 0.92335\n",
      "Minibatch loss at step 45100: 0.289634 with learning rate 0.029199\n",
      "step 45100, training accuracy 0.964844\n",
      "Validation accuracy: 0.92375\n",
      "Minibatch loss at step 45200: 0.303437 with learning rate 0.029199\n",
      "step 45200, training accuracy 0.949219\n",
      "Validation accuracy: 0.923967\n",
      "Minibatch loss at step 45300: 0.293075 with learning rate 0.029199\n",
      "step 45300, training accuracy 0.964844\n",
      "Validation accuracy: 0.922917\n",
      "Minibatch loss at step 45400: 0.247063 with learning rate 0.029199\n",
      "step 45400, training accuracy 0.972656\n",
      "Validation accuracy: 0.92325\n",
      "Minibatch loss at step 45500: 0.283192 with learning rate 0.029199\n",
      "step 45500, training accuracy 0.960938\n",
      "Validation accuracy: 0.92425\n",
      "Minibatch loss at step 45600: 0.301120 with learning rate 0.029199\n",
      "step 45600, training accuracy 0.960938\n",
      "Validation accuracy: 0.9238\n",
      "Minibatch loss at step 45700: 0.278185 with learning rate 0.029199\n",
      "step 45700, training accuracy 0.984375\n",
      "Validation accuracy: 0.9232\n",
      "Minibatch loss at step 45800: 0.262030 with learning rate 0.029199\n",
      "step 45800, training accuracy 0.972656\n",
      "Validation accuracy: 0.923233\n",
      "Minibatch loss at step 45900: 0.277211 with learning rate 0.027739\n",
      "step 45900, training accuracy 0.960938\n",
      "Validation accuracy: 0.923467\n",
      "Minibatch loss at step 46000: 0.275900 with learning rate 0.027739\n",
      "step 46000, training accuracy 0.949219\n",
      "Validation accuracy: 0.923267\n",
      "Minibatch loss at step 46100: 0.295584 with learning rate 0.027739\n",
      "step 46100, training accuracy 0.960938\n",
      "Validation accuracy: 0.9241\n",
      "Minibatch loss at step 46200: 0.277516 with learning rate 0.027739\n",
      "step 46200, training accuracy 0.945312\n",
      "Validation accuracy: 0.924217\n",
      "Minibatch loss at step 46300: 0.291195 with learning rate 0.027739\n",
      "step 46300, training accuracy 0.96875\n",
      "Validation accuracy: 0.92375\n",
      "Minibatch loss at step 46400: 0.362135 with learning rate 0.027739\n",
      "step 46400, training accuracy 0.941406\n",
      "Validation accuracy: 0.924467\n",
      "Minibatch loss at step 46500: 0.297252 with learning rate 0.027739\n",
      "step 46500, training accuracy 0.96875\n",
      "Validation accuracy: 0.924083\n",
      "Minibatch loss at step 46600: 0.268600 with learning rate 0.027739\n",
      "step 46600, training accuracy 0.972656\n",
      "Validation accuracy: 0.924017\n",
      "Minibatch loss at step 46700: 0.289247 with learning rate 0.027739\n",
      "step 46700, training accuracy 0.96875\n",
      "Validation accuracy: 0.92435\n",
      "Minibatch loss at step 46800: 0.333675 with learning rate 0.027739\n",
      "step 46800, training accuracy 0.945312\n",
      "Validation accuracy: 0.924617\n",
      "Minibatch loss at step 46900: 0.259714 with learning rate 0.027739\n",
      "step 46900, training accuracy 0.980469\n",
      "Validation accuracy: 0.923883\n",
      "Minibatch loss at step 47000: 0.306598 with learning rate 0.027739\n",
      "step 47000, training accuracy 0.976562\n",
      "Validation accuracy: 0.923933\n",
      "Minibatch loss at step 47100: 0.296443 with learning rate 0.027739\n",
      "step 47100, training accuracy 0.964844\n",
      "Validation accuracy: 0.923917\n",
      "Minibatch loss at step 47200: 0.277057 with learning rate 0.027739\n",
      "step 47200, training accuracy 0.96875\n",
      "Validation accuracy: 0.923933\n",
      "Minibatch loss at step 47300: 0.285741 with learning rate 0.027739\n",
      "step 47300, training accuracy 0.964844\n",
      "Validation accuracy: 0.923783\n",
      "Minibatch loss at step 47400: 0.341494 with learning rate 0.027739\n",
      "step 47400, training accuracy 0.960938\n",
      "Validation accuracy: 0.924283\n",
      "Minibatch loss at step 47500: 0.288436 with learning rate 0.027739\n",
      "step 47500, training accuracy 0.960938\n",
      "Validation accuracy: 0.923967\n",
      "Minibatch loss at step 47600: 0.379568 with learning rate 0.027739\n",
      "step 47600, training accuracy 0.941406\n",
      "Validation accuracy: 0.924\n",
      "Minibatch loss at step 47700: 0.378209 with learning rate 0.026352\n",
      "step 47700, training accuracy 0.9375\n",
      "Validation accuracy: 0.924433\n",
      "Minibatch loss at step 47800: 0.217497 with learning rate 0.026352\n",
      "step 47800, training accuracy 0.972656\n",
      "Validation accuracy: 0.924017\n",
      "Minibatch loss at step 47900: 0.307953 with learning rate 0.026352\n",
      "step 47900, training accuracy 0.945312\n",
      "Validation accuracy: 0.923733\n",
      "Minibatch loss at step 48000: 0.333590 with learning rate 0.026352\n",
      "step 48000, training accuracy 0.953125\n",
      "Validation accuracy: 0.923533\n",
      "Minibatch loss at step 48100: 0.303354 with learning rate 0.026352\n",
      "step 48100, training accuracy 0.972656\n",
      "Validation accuracy: 0.924217\n",
      "Minibatch loss at step 48200: 0.279458 with learning rate 0.026352\n",
      "step 48200, training accuracy 0.960938\n",
      "Validation accuracy: 0.923167\n",
      "Minibatch loss at step 48300: 0.310276 with learning rate 0.026352\n",
      "step 48300, training accuracy 0.960938\n",
      "Validation accuracy: 0.924967\n",
      "Minibatch loss at step 48400: 0.279151 with learning rate 0.026352\n",
      "step 48400, training accuracy 0.964844\n",
      "Validation accuracy: 0.92425\n",
      "Minibatch loss at step 48500: 0.235978 with learning rate 0.026352\n",
      "step 48500, training accuracy 0.984375\n",
      "Validation accuracy: 0.924\n",
      "Minibatch loss at step 48600: 0.340073 with learning rate 0.026352\n",
      "step 48600, training accuracy 0.960938\n",
      "Validation accuracy: 0.923617\n",
      "Minibatch loss at step 48700: 0.252748 with learning rate 0.026352\n",
      "step 48700, training accuracy 0.96875\n",
      "Validation accuracy: 0.924133\n",
      "Minibatch loss at step 48800: 0.233314 with learning rate 0.026352\n",
      "step 48800, training accuracy 0.984375\n",
      "Validation accuracy: 0.924033\n",
      "Minibatch loss at step 48900: 0.284334 with learning rate 0.026352\n",
      "step 48900, training accuracy 0.96875\n",
      "Validation accuracy: 0.923767\n",
      "Minibatch loss at step 49000: 0.259277 with learning rate 0.026352\n",
      "step 49000, training accuracy 0.960938\n",
      "Validation accuracy: 0.923767\n",
      "Minibatch loss at step 49100: 0.384287 with learning rate 0.026352\n",
      "step 49100, training accuracy 0.949219\n",
      "Validation accuracy: 0.923717\n",
      "Minibatch loss at step 49200: 0.309674 with learning rate 0.026352\n",
      "step 49200, training accuracy 0.960938\n",
      "Validation accuracy: 0.92485\n",
      "Minibatch loss at step 49300: 0.346789 with learning rate 0.026352\n",
      "step 49300, training accuracy 0.949219\n",
      "Validation accuracy: 0.924067\n",
      "Minibatch loss at step 49400: 0.321077 with learning rate 0.026352\n",
      "step 49400, training accuracy 0.949219\n",
      "Validation accuracy: 0.9241\n",
      "Minibatch loss at step 49500: 0.314160 with learning rate 0.025034\n",
      "step 49500, training accuracy 0.949219\n",
      "Validation accuracy: 0.925233\n",
      "Minibatch loss at step 49600: 0.367707 with learning rate 0.025034\n",
      "step 49600, training accuracy 0.945312\n",
      "Validation accuracy: 0.924117\n",
      "Minibatch loss at step 49700: 0.297870 with learning rate 0.025034\n",
      "step 49700, training accuracy 0.957031\n",
      "Validation accuracy: 0.924133\n",
      "Minibatch loss at step 49800: 0.316166 with learning rate 0.025034\n",
      "step 49800, training accuracy 0.953125\n",
      "Validation accuracy: 0.9236\n",
      "Minibatch loss at step 49900: 0.323212 with learning rate 0.025034\n",
      "step 49900, training accuracy 0.957031\n",
      "Validation accuracy: 0.924667\n",
      "Minibatch loss at step 50000: 0.282079 with learning rate 0.025034\n",
      "step 50000, training accuracy 0.960938\n",
      "Validation accuracy: 0.925017\n",
      "Minibatch loss at step 50100: 0.245857 with learning rate 0.025034\n",
      "step 50100, training accuracy 0.976562\n",
      "Validation accuracy: 0.9244\n",
      "Minibatch loss at step 50200: 0.328148 with learning rate 0.025034\n",
      "step 50200, training accuracy 0.949219\n",
      "Validation accuracy: 0.924517\n",
      "Minibatch loss at step 50300: 0.362725 with learning rate 0.025034\n",
      "step 50300, training accuracy 0.941406\n",
      "Validation accuracy: 0.924767\n",
      "Minibatch loss at step 50400: 0.293139 with learning rate 0.025034\n",
      "step 50400, training accuracy 0.957031\n",
      "Validation accuracy: 0.924433\n",
      "Minibatch loss at step 50500: 0.297233 with learning rate 0.025034\n",
      "step 50500, training accuracy 0.960938\n",
      "Validation accuracy: 0.92355\n",
      "Minibatch loss at step 50600: 0.306144 with learning rate 0.025034\n",
      "step 50600, training accuracy 0.953125\n",
      "Validation accuracy: 0.923783\n",
      "Minibatch loss at step 50700: 0.380302 with learning rate 0.025034\n",
      "step 50700, training accuracy 0.929688\n",
      "Validation accuracy: 0.925083\n",
      "Minibatch loss at step 50800: 0.262694 with learning rate 0.025034\n",
      "step 50800, training accuracy 0.957031\n",
      "Validation accuracy: 0.92435\n",
      "Minibatch loss at step 50900: 0.315988 with learning rate 0.025034\n",
      "step 50900, training accuracy 0.960938\n",
      "Validation accuracy: 0.92435\n",
      "Minibatch loss at step 51000: 0.335762 with learning rate 0.025034\n",
      "step 51000, training accuracy 0.945312\n",
      "Validation accuracy: 0.924583\n",
      "Minibatch loss at step 51100: 0.261179 with learning rate 0.025034\n",
      "step 51100, training accuracy 0.964844\n",
      "Validation accuracy: 0.9247\n",
      "Minibatch loss at step 51200: 0.201624 with learning rate 0.025034\n",
      "step 51200, training accuracy 0.988281\n",
      "Validation accuracy: 0.9251\n",
      "Minibatch loss at step 51300: 0.347856 with learning rate 0.025034\n",
      "step 51300, training accuracy 0.957031\n",
      "Validation accuracy: 0.92465\n",
      "Minibatch loss at step 51400: 0.314697 with learning rate 0.023783\n",
      "step 51400, training accuracy 0.949219\n",
      "Validation accuracy: 0.9249\n",
      "Minibatch loss at step 51500: 0.355257 with learning rate 0.023783\n",
      "step 51500, training accuracy 0.957031\n",
      "Validation accuracy: 0.92505\n",
      "Minibatch loss at step 51600: 0.342547 with learning rate 0.023783\n",
      "step 51600, training accuracy 0.941406\n",
      "Validation accuracy: 0.925\n",
      "Minibatch loss at step 51700: 0.355820 with learning rate 0.023783\n",
      "step 51700, training accuracy 0.9375\n",
      "Validation accuracy: 0.923917\n",
      "Minibatch loss at step 51800: 0.308119 with learning rate 0.023783\n",
      "step 51800, training accuracy 0.964844\n",
      "Validation accuracy: 0.924583\n",
      "Minibatch loss at step 51900: 0.285371 with learning rate 0.023783\n",
      "step 51900, training accuracy 0.957031\n",
      "Validation accuracy: 0.925633\n",
      "Minibatch loss at step 52000: 0.276970 with learning rate 0.023783\n",
      "step 52000, training accuracy 0.972656\n",
      "Validation accuracy: 0.92515\n",
      "Minibatch loss at step 52100: 0.316425 with learning rate 0.023783\n",
      "step 52100, training accuracy 0.953125\n",
      "Validation accuracy: 0.924483\n",
      "Minibatch loss at step 52200: 0.292478 with learning rate 0.023783\n",
      "step 52200, training accuracy 0.953125\n",
      "Validation accuracy: 0.925067\n",
      "Minibatch loss at step 52300: 0.292118 with learning rate 0.023783\n",
      "step 52300, training accuracy 0.960938\n",
      "Validation accuracy: 0.92425\n",
      "Minibatch loss at step 52400: 0.309143 with learning rate 0.023783\n",
      "step 52400, training accuracy 0.945312\n",
      "Validation accuracy: 0.924917\n",
      "Minibatch loss at step 52500: 0.258512 with learning rate 0.023783\n",
      "step 52500, training accuracy 0.972656\n",
      "Validation accuracy: 0.925083\n",
      "Minibatch loss at step 52600: 0.300617 with learning rate 0.023783\n",
      "step 52600, training accuracy 0.960938\n",
      "Validation accuracy: 0.925333\n",
      "Minibatch loss at step 52700: 0.338515 with learning rate 0.023783\n",
      "step 52700, training accuracy 0.960938\n",
      "Validation accuracy: 0.924367\n",
      "Minibatch loss at step 52800: 0.256058 with learning rate 0.023783\n",
      "step 52800, training accuracy 0.972656\n",
      "Validation accuracy: 0.924617\n",
      "Minibatch loss at step 52900: 0.335817 with learning rate 0.023783\n",
      "step 52900, training accuracy 0.941406\n",
      "Validation accuracy: 0.9249\n",
      "Minibatch loss at step 53000: 0.256470 with learning rate 0.023783\n",
      "step 53000, training accuracy 0.957031\n",
      "Validation accuracy: 0.925017\n",
      "Minibatch loss at step 53100: 0.242797 with learning rate 0.023783\n",
      "step 53100, training accuracy 0.96875\n",
      "Validation accuracy: 0.925183\n",
      "Minibatch loss at step 53200: 0.328250 with learning rate 0.022594\n",
      "step 53200, training accuracy 0.957031\n",
      "Validation accuracy: 0.925667\n",
      "Minibatch loss at step 53300: 0.274614 with learning rate 0.022594\n",
      "step 53300, training accuracy 0.96875\n",
      "Validation accuracy: 0.924967\n",
      "Minibatch loss at step 53400: 0.325123 with learning rate 0.022594\n",
      "step 53400, training accuracy 0.953125\n",
      "Validation accuracy: 0.9249\n",
      "Minibatch loss at step 53500: 0.282439 with learning rate 0.022594\n",
      "step 53500, training accuracy 0.972656\n",
      "Validation accuracy: 0.925117\n",
      "Minibatch loss at step 53600: 0.307452 with learning rate 0.022594\n",
      "step 53600, training accuracy 0.964844\n",
      "Validation accuracy: 0.92525\n",
      "Minibatch loss at step 53700: 0.325956 with learning rate 0.022594\n",
      "step 53700, training accuracy 0.949219\n",
      "Validation accuracy: 0.9253\n",
      "Minibatch loss at step 53800: 0.275204 with learning rate 0.022594\n",
      "step 53800, training accuracy 0.964844\n",
      "Validation accuracy: 0.925217\n",
      "Minibatch loss at step 53900: 0.250983 with learning rate 0.022594\n",
      "step 53900, training accuracy 0.972656\n",
      "Validation accuracy: 0.925567\n",
      "Minibatch loss at step 54000: 0.314841 with learning rate 0.022594\n",
      "step 54000, training accuracy 0.964844\n",
      "Validation accuracy: 0.925133\n",
      "Minibatch loss at step 54100: 0.302883 with learning rate 0.022594\n",
      "step 54100, training accuracy 0.964844\n",
      "Validation accuracy: 0.9252\n",
      "Minibatch loss at step 54200: 0.253097 with learning rate 0.022594\n",
      "step 54200, training accuracy 0.972656\n",
      "Validation accuracy: 0.925017\n",
      "Minibatch loss at step 54300: 0.320492 with learning rate 0.022594\n",
      "step 54300, training accuracy 0.949219\n",
      "Validation accuracy: 0.9251\n",
      "Minibatch loss at step 54400: 0.318258 with learning rate 0.022594\n",
      "step 54400, training accuracy 0.957031\n",
      "Validation accuracy: 0.92495\n",
      "Minibatch loss at step 54500: 0.301264 with learning rate 0.022594\n",
      "step 54500, training accuracy 0.96875\n",
      "Validation accuracy: 0.92385\n",
      "Minibatch loss at step 54600: 0.337315 with learning rate 0.022594\n",
      "step 54600, training accuracy 0.9375\n",
      "Validation accuracy: 0.92415\n",
      "Minibatch loss at step 54700: 0.271811 with learning rate 0.022594\n",
      "step 54700, training accuracy 0.964844\n",
      "Validation accuracy: 0.92575\n",
      "Minibatch loss at step 54800: 0.323139 with learning rate 0.022594\n",
      "step 54800, training accuracy 0.957031\n",
      "Validation accuracy: 0.92495\n",
      "Minibatch loss at step 54900: 0.408319 with learning rate 0.022594\n",
      "step 54900, training accuracy 0.949219\n",
      "Validation accuracy: 0.92495\n",
      "Minibatch loss at step 55000: 0.345088 with learning rate 0.021464\n",
      "step 55000, training accuracy 0.953125\n",
      "Validation accuracy: 0.925533\n",
      "Minibatch loss at step 55100: 0.348000 with learning rate 0.021464\n",
      "step 55100, training accuracy 0.933594\n",
      "Validation accuracy: 0.9255\n",
      "Minibatch loss at step 55200: 0.275515 with learning rate 0.021464\n",
      "step 55200, training accuracy 0.976562\n",
      "Validation accuracy: 0.92605\n",
      "Minibatch loss at step 55300: 0.285088 with learning rate 0.021464\n",
      "step 55300, training accuracy 0.949219\n",
      "Validation accuracy: 0.92625\n",
      "Minibatch loss at step 55400: 0.290805 with learning rate 0.021464\n",
      "step 55400, training accuracy 0.957031\n",
      "Validation accuracy: 0.92555\n",
      "Minibatch loss at step 55500: 0.315476 with learning rate 0.021464\n",
      "step 55500, training accuracy 0.957031\n",
      "Validation accuracy: 0.926217\n",
      "Minibatch loss at step 55600: 0.338185 with learning rate 0.021464\n",
      "step 55600, training accuracy 0.941406\n",
      "Validation accuracy: 0.92505\n",
      "Minibatch loss at step 55700: 0.235398 with learning rate 0.021464\n",
      "step 55700, training accuracy 0.980469\n",
      "Validation accuracy: 0.925067\n",
      "Minibatch loss at step 55800: 0.332499 with learning rate 0.021464\n",
      "step 55800, training accuracy 0.945312\n",
      "Validation accuracy: 0.925217\n",
      "Minibatch loss at step 55900: 0.207101 with learning rate 0.021464\n",
      "step 55900, training accuracy 0.980469\n",
      "Validation accuracy: 0.925267\n",
      "Minibatch loss at step 56000: 0.218732 with learning rate 0.021464\n",
      "step 56000, training accuracy 0.984375\n",
      "Validation accuracy: 0.924833\n",
      "Minibatch loss at step 56100: 0.297099 with learning rate 0.021464\n",
      "step 56100, training accuracy 0.960938\n",
      "Validation accuracy: 0.9238\n",
      "Minibatch loss at step 56200: 0.226688 with learning rate 0.021464\n",
      "step 56200, training accuracy 0.96875\n",
      "Validation accuracy: 0.925183\n",
      "Minibatch loss at step 56300: 0.249777 with learning rate 0.021464\n",
      "step 56300, training accuracy 0.96875\n",
      "Validation accuracy: 0.925317\n",
      "Minibatch loss at step 56400: 0.289323 with learning rate 0.021464\n",
      "step 56400, training accuracy 0.964844\n",
      "Validation accuracy: 0.925467\n",
      "Minibatch loss at step 56500: 0.289802 with learning rate 0.021464\n",
      "step 56500, training accuracy 0.96875\n",
      "Validation accuracy: 0.924633\n",
      "Minibatch loss at step 56600: 0.246901 with learning rate 0.021464\n",
      "step 56600, training accuracy 0.972656\n",
      "Validation accuracy: 0.925533\n",
      "Minibatch loss at step 56700: 0.298734 with learning rate 0.021464\n",
      "step 56700, training accuracy 0.964844\n",
      "Validation accuracy: 0.925483\n",
      "Minibatch loss at step 56800: 0.300115 with learning rate 0.021464\n",
      "step 56800, training accuracy 0.957031\n",
      "Validation accuracy: 0.924867\n",
      "Minibatch loss at step 56900: 0.248055 with learning rate 0.020391\n",
      "step 56900, training accuracy 0.976562\n",
      "Validation accuracy: 0.925367\n",
      "Minibatch loss at step 57000: 0.258024 with learning rate 0.020391\n",
      "step 57000, training accuracy 0.957031\n",
      "Validation accuracy: 0.925483\n",
      "Minibatch loss at step 57100: 0.326414 with learning rate 0.020391\n",
      "step 57100, training accuracy 0.953125\n",
      "Validation accuracy: 0.925233\n",
      "Minibatch loss at step 57200: 0.313483 with learning rate 0.020391\n",
      "step 57200, training accuracy 0.945312\n",
      "Validation accuracy: 0.92555\n",
      "Minibatch loss at step 57300: 0.266324 with learning rate 0.020391\n",
      "step 57300, training accuracy 0.972656\n",
      "Validation accuracy: 0.925233\n",
      "Minibatch loss at step 57400: 0.381436 with learning rate 0.020391\n",
      "step 57400, training accuracy 0.929688\n",
      "Validation accuracy: 0.925383\n",
      "Minibatch loss at step 57500: 0.224790 with learning rate 0.020391\n",
      "step 57500, training accuracy 0.984375\n",
      "Validation accuracy: 0.925433\n",
      "Minibatch loss at step 57600: 0.283587 with learning rate 0.020391\n",
      "step 57600, training accuracy 0.972656\n",
      "Validation accuracy: 0.9257\n",
      "Minibatch loss at step 57700: 0.221660 with learning rate 0.020391\n",
      "step 57700, training accuracy 0.972656\n",
      "Validation accuracy: 0.9253\n",
      "Minibatch loss at step 57800: 0.324495 with learning rate 0.020391\n",
      "step 57800, training accuracy 0.96875\n",
      "Validation accuracy: 0.926017\n",
      "Minibatch loss at step 57900: 0.272180 with learning rate 0.020391\n",
      "step 57900, training accuracy 0.964844\n",
      "Validation accuracy: 0.924633\n",
      "Minibatch loss at step 58000: 0.220344 with learning rate 0.020391\n",
      "step 58000, training accuracy 0.980469\n",
      "Validation accuracy: 0.925183\n",
      "Minibatch loss at step 58100: 0.271797 with learning rate 0.020391\n",
      "step 58100, training accuracy 0.960938\n",
      "Validation accuracy: 0.925333\n",
      "Minibatch loss at step 58200: 0.296573 with learning rate 0.020391\n",
      "step 58200, training accuracy 0.960938\n",
      "Validation accuracy: 0.9254\n",
      "Minibatch loss at step 58300: 0.240244 with learning rate 0.020391\n",
      "step 58300, training accuracy 0.972656\n",
      "Validation accuracy: 0.924983\n",
      "Minibatch loss at step 58400: 0.235810 with learning rate 0.020391\n",
      "step 58400, training accuracy 0.96875\n",
      "Validation accuracy: 0.925217\n",
      "Minibatch loss at step 58500: 0.252567 with learning rate 0.020391\n",
      "step 58500, training accuracy 0.976562\n",
      "Validation accuracy: 0.925183\n",
      "Minibatch loss at step 58600: 0.295564 with learning rate 0.020391\n",
      "step 58600, training accuracy 0.964844\n",
      "Validation accuracy: 0.92595\n",
      "Minibatch loss at step 58700: 0.301791 with learning rate 0.019371\n",
      "step 58700, training accuracy 0.96875\n",
      "Validation accuracy: 0.926367\n",
      "Minibatch loss at step 58800: 0.258803 with learning rate 0.019371\n",
      "step 58800, training accuracy 0.96875\n",
      "Validation accuracy: 0.925817\n",
      "Minibatch loss at step 58900: 0.236490 with learning rate 0.019371\n",
      "step 58900, training accuracy 0.972656\n",
      "Validation accuracy: 0.925583\n",
      "Minibatch loss at step 59000: 0.323955 with learning rate 0.019371\n",
      "step 59000, training accuracy 0.941406\n",
      "Validation accuracy: 0.926233\n",
      "Minibatch loss at step 59100: 0.343042 with learning rate 0.019371\n",
      "step 59100, training accuracy 0.953125\n",
      "Validation accuracy: 0.9258\n",
      "Minibatch loss at step 59200: 0.292788 with learning rate 0.019371\n",
      "step 59200, training accuracy 0.949219\n",
      "Validation accuracy: 0.925717\n",
      "Minibatch loss at step 59300: 0.214636 with learning rate 0.019371\n",
      "step 59300, training accuracy 0.976562\n",
      "Validation accuracy: 0.925583\n",
      "Minibatch loss at step 59400: 0.386861 with learning rate 0.019371\n",
      "step 59400, training accuracy 0.949219\n",
      "Validation accuracy: 0.92545\n",
      "Minibatch loss at step 59500: 0.350074 with learning rate 0.019371\n",
      "step 59500, training accuracy 0.945312\n",
      "Validation accuracy: 0.9251\n",
      "Minibatch loss at step 59600: 0.322713 with learning rate 0.019371\n",
      "step 59600, training accuracy 0.957031\n",
      "Validation accuracy: 0.9252\n",
      "Minibatch loss at step 59700: 0.319876 with learning rate 0.019371\n",
      "step 59700, training accuracy 0.941406\n",
      "Validation accuracy: 0.925017\n",
      "Minibatch loss at step 59800: 0.311781 with learning rate 0.019371\n",
      "step 59800, training accuracy 0.949219\n",
      "Validation accuracy: 0.926267\n",
      "Minibatch loss at step 59900: 0.337093 with learning rate 0.019371\n",
      "step 59900, training accuracy 0.960938\n",
      "Validation accuracy: 0.92595\n",
      "Minibatch loss at step 60000: 0.317075 with learning rate 0.019371\n",
      "step 60000, training accuracy 0.960938\n",
      "Validation accuracy: 0.92565\n",
      "Minibatch loss at step 60100: 0.302677 with learning rate 0.019371\n",
      "step 60100, training accuracy 0.960938\n",
      "Validation accuracy: 0.9258\n",
      "Minibatch loss at step 60200: 0.331908 with learning rate 0.019371\n",
      "step 60200, training accuracy 0.957031\n",
      "Validation accuracy: 0.925567\n",
      "Minibatch loss at step 60300: 0.296516 with learning rate 0.019371\n",
      "step 60300, training accuracy 0.957031\n",
      "Validation accuracy: 0.92565\n",
      "Minibatch loss at step 60400: 0.360196 with learning rate 0.019371\n",
      "step 60400, training accuracy 0.9375\n",
      "Validation accuracy: 0.926117\n",
      "Minibatch loss at step 60500: 0.234748 with learning rate 0.018403\n",
      "step 60500, training accuracy 0.972656\n",
      "Validation accuracy: 0.925883\n",
      "Minibatch loss at step 60600: 0.286623 with learning rate 0.018403\n",
      "step 60600, training accuracy 0.957031\n",
      "Validation accuracy: 0.92575\n",
      "Minibatch loss at step 60700: 0.290802 with learning rate 0.018403\n",
      "step 60700, training accuracy 0.96875\n",
      "Validation accuracy: 0.926533\n",
      "Minibatch loss at step 60800: 0.332339 with learning rate 0.018403\n",
      "step 60800, training accuracy 0.972656\n",
      "Validation accuracy: 0.926783\n",
      "Minibatch loss at step 60900: 0.298181 with learning rate 0.018403\n",
      "step 60900, training accuracy 0.96875\n",
      "Validation accuracy: 0.926667\n",
      "Minibatch loss at step 61000: 0.300544 with learning rate 0.018403\n",
      "step 61000, training accuracy 0.96875\n",
      "Validation accuracy: 0.926517\n",
      "Minibatch loss at step 61100: 0.247939 with learning rate 0.018403\n",
      "step 61100, training accuracy 0.96875\n",
      "Validation accuracy: 0.9264\n",
      "Minibatch loss at step 61200: 0.230898 with learning rate 0.018403\n",
      "step 61200, training accuracy 0.976562\n",
      "Validation accuracy: 0.925967\n",
      "Minibatch loss at step 61300: 0.285375 with learning rate 0.018403\n",
      "step 61300, training accuracy 0.964844\n",
      "Validation accuracy: 0.92665\n",
      "Minibatch loss at step 61400: 0.313952 with learning rate 0.018403\n",
      "step 61400, training accuracy 0.945312\n",
      "Validation accuracy: 0.9261\n",
      "Minibatch loss at step 61500: 0.238012 with learning rate 0.018403\n",
      "step 61500, training accuracy 0.972656\n",
      "Validation accuracy: 0.9256\n",
      "Minibatch loss at step 61600: 0.258226 with learning rate 0.018403\n",
      "step 61600, training accuracy 0.964844\n",
      "Validation accuracy: 0.9255\n",
      "Minibatch loss at step 61700: 0.321315 with learning rate 0.018403\n",
      "step 61700, training accuracy 0.953125\n",
      "Validation accuracy: 0.925633\n",
      "Minibatch loss at step 61800: 0.247289 with learning rate 0.018403\n",
      "step 61800, training accuracy 0.964844\n",
      "Validation accuracy: 0.92545\n",
      "Minibatch loss at step 61900: 0.308422 with learning rate 0.018403\n",
      "step 61900, training accuracy 0.941406\n",
      "Validation accuracy: 0.925533\n",
      "Minibatch loss at step 62000: 0.224975 with learning rate 0.018403\n",
      "step 62000, training accuracy 0.964844\n",
      "Validation accuracy: 0.925717\n",
      "Minibatch loss at step 62100: 0.365473 with learning rate 0.018403\n",
      "step 62100, training accuracy 0.945312\n",
      "Validation accuracy: 0.926717\n",
      "Minibatch loss at step 62200: 0.257871 with learning rate 0.018403\n",
      "step 62200, training accuracy 0.964844\n",
      "Validation accuracy: 0.92635\n",
      "Minibatch loss at step 62300: 0.310218 with learning rate 0.018403\n",
      "step 62300, training accuracy 0.964844\n",
      "Validation accuracy: 0.926183\n",
      "Minibatch loss at step 62400: 0.330991 with learning rate 0.017482\n",
      "step 62400, training accuracy 0.949219\n",
      "Validation accuracy: 0.926517\n",
      "Minibatch loss at step 62500: 0.294176 with learning rate 0.017482\n",
      "step 62500, training accuracy 0.957031\n",
      "Validation accuracy: 0.92625\n",
      "Minibatch loss at step 62600: 0.207699 with learning rate 0.017482\n",
      "step 62600, training accuracy 0.976562\n",
      "Validation accuracy: 0.92695\n",
      "Minibatch loss at step 62700: 0.278346 with learning rate 0.017482\n",
      "step 62700, training accuracy 0.964844\n",
      "Validation accuracy: 0.925417\n",
      "Minibatch loss at step 62800: 0.313182 with learning rate 0.017482\n",
      "step 62800, training accuracy 0.96875\n",
      "Validation accuracy: 0.926067\n",
      "Minibatch loss at step 62900: 0.258076 with learning rate 0.017482\n",
      "step 62900, training accuracy 0.960938\n",
      "Validation accuracy: 0.926567\n",
      "Minibatch loss at step 63000: 0.243337 with learning rate 0.017482\n",
      "step 63000, training accuracy 0.957031\n",
      "Validation accuracy: 0.926483\n",
      "Minibatch loss at step 63100: 0.319215 with learning rate 0.017482\n",
      "step 63100, training accuracy 0.953125\n",
      "Validation accuracy: 0.926683\n",
      "Minibatch loss at step 63200: 0.246669 with learning rate 0.017482\n",
      "step 63200, training accuracy 0.964844\n",
      "Validation accuracy: 0.92615\n",
      "Minibatch loss at step 63300: 0.273382 with learning rate 0.017482\n",
      "step 63300, training accuracy 0.964844\n",
      "Validation accuracy: 0.92465\n",
      "Minibatch loss at step 63400: 0.281802 with learning rate 0.017482\n",
      "step 63400, training accuracy 0.957031\n",
      "Validation accuracy: 0.926133\n",
      "Minibatch loss at step 63500: 0.286118 with learning rate 0.017482\n",
      "step 63500, training accuracy 0.957031\n",
      "Validation accuracy: 0.926567\n",
      "Minibatch loss at step 63600: 0.357737 with learning rate 0.017482\n",
      "step 63600, training accuracy 0.953125\n",
      "Validation accuracy: 0.9261\n",
      "Minibatch loss at step 63700: 0.293466 with learning rate 0.017482\n",
      "step 63700, training accuracy 0.957031\n",
      "Validation accuracy: 0.925733\n",
      "Minibatch loss at step 63800: 0.257451 with learning rate 0.017482\n",
      "step 63800, training accuracy 0.960938\n",
      "Validation accuracy: 0.9261\n",
      "Minibatch loss at step 63900: 0.224922 with learning rate 0.017482\n",
      "step 63900, training accuracy 0.980469\n",
      "Validation accuracy: 0.9255\n",
      "Minibatch loss at step 64000: 0.334322 with learning rate 0.017482\n",
      "step 64000, training accuracy 0.945312\n",
      "Validation accuracy: 0.926133\n",
      "Minibatch loss at step 64100: 0.265384 with learning rate 0.017482\n",
      "step 64100, training accuracy 0.96875\n",
      "Validation accuracy: 0.925983\n",
      "Minibatch loss at step 64200: 0.329303 with learning rate 0.016608\n",
      "step 64200, training accuracy 0.945312\n",
      "Validation accuracy: 0.926983\n",
      "Minibatch loss at step 64300: 0.287087 with learning rate 0.016608\n",
      "step 64300, training accuracy 0.949219\n",
      "Validation accuracy: 0.926433\n",
      "Minibatch loss at step 64400: 0.273916 with learning rate 0.016608\n",
      "step 64400, training accuracy 0.960938\n",
      "Validation accuracy: 0.926617\n",
      "Minibatch loss at step 64500: 0.258372 with learning rate 0.016608\n",
      "step 64500, training accuracy 0.976562\n",
      "Validation accuracy: 0.926867\n",
      "Minibatch loss at step 64600: 0.261565 with learning rate 0.016608\n",
      "step 64600, training accuracy 0.949219\n",
      "Validation accuracy: 0.926267\n",
      "Minibatch loss at step 64700: 0.319897 with learning rate 0.016608\n",
      "step 64700, training accuracy 0.941406\n",
      "Validation accuracy: 0.9265\n",
      "Minibatch loss at step 64800: 0.257875 with learning rate 0.016608\n",
      "step 64800, training accuracy 0.972656\n",
      "Validation accuracy: 0.926267\n",
      "Minibatch loss at step 64900: 0.295166 with learning rate 0.016608\n",
      "step 64900, training accuracy 0.949219\n",
      "Validation accuracy: 0.926917\n",
      "Minibatch loss at step 65000: 0.242374 with learning rate 0.016608\n",
      "step 65000, training accuracy 0.980469\n",
      "Validation accuracy: 0.927217\n",
      "Minibatch loss at step 65100: 0.287035 with learning rate 0.016608\n",
      "step 65100, training accuracy 0.96875\n",
      "Validation accuracy: 0.9264\n",
      "Minibatch loss at step 65200: 0.348495 with learning rate 0.016608\n",
      "step 65200, training accuracy 0.964844\n",
      "Validation accuracy: 0.926767\n",
      "Minibatch loss at step 65300: 0.288743 with learning rate 0.016608\n",
      "step 65300, training accuracy 0.964844\n",
      "Validation accuracy: 0.926783\n",
      "Minibatch loss at step 65400: 0.304007 with learning rate 0.016608\n",
      "step 65400, training accuracy 0.96875\n",
      "Validation accuracy: 0.9264\n",
      "Minibatch loss at step 65500: 0.314080 with learning rate 0.016608\n",
      "step 65500, training accuracy 0.960938\n",
      "Validation accuracy: 0.925333\n",
      "Minibatch loss at step 65600: 0.359069 with learning rate 0.016608\n",
      "step 65600, training accuracy 0.945312\n",
      "Validation accuracy: 0.926133\n",
      "Minibatch loss at step 65700: 0.274113 with learning rate 0.016608\n",
      "step 65700, training accuracy 0.96875\n",
      "Validation accuracy: 0.925567\n",
      "Minibatch loss at step 65800: 0.208342 with learning rate 0.016608\n",
      "step 65800, training accuracy 0.972656\n",
      "Validation accuracy: 0.925317\n",
      "Minibatch loss at step 65900: 0.339279 with learning rate 0.016608\n",
      "step 65900, training accuracy 0.957031\n",
      "Validation accuracy: 0.9262\n",
      "Minibatch loss at step 66000: 0.259616 with learning rate 0.015778\n",
      "step 66000, training accuracy 0.957031\n",
      "Validation accuracy: 0.926917\n",
      "Minibatch loss at step 66100: 0.252692 with learning rate 0.015778\n",
      "step 66100, training accuracy 0.976562\n",
      "Validation accuracy: 0.926617\n",
      "Minibatch loss at step 66200: 0.300991 with learning rate 0.015778\n",
      "step 66200, training accuracy 0.957031\n",
      "Validation accuracy: 0.92655\n",
      "Minibatch loss at step 66300: 0.241241 with learning rate 0.015778\n",
      "step 66300, training accuracy 0.984375\n",
      "Validation accuracy: 0.926483\n",
      "Minibatch loss at step 66400: 0.235394 with learning rate 0.015778\n",
      "step 66400, training accuracy 0.976562\n",
      "Validation accuracy: 0.926883\n",
      "Minibatch loss at step 66500: 0.254708 with learning rate 0.015778\n",
      "step 66500, training accuracy 0.972656\n",
      "Validation accuracy: 0.926717\n",
      "Minibatch loss at step 66600: 0.221427 with learning rate 0.015778\n",
      "step 66600, training accuracy 0.980469\n",
      "Validation accuracy: 0.926767\n",
      "Minibatch loss at step 66700: 0.321603 with learning rate 0.015778\n",
      "step 66700, training accuracy 0.957031\n",
      "Validation accuracy: 0.927017\n",
      "Minibatch loss at step 66800: 0.289617 with learning rate 0.015778\n",
      "step 66800, training accuracy 0.964844\n",
      "Validation accuracy: 0.926767\n",
      "Minibatch loss at step 66900: 0.306122 with learning rate 0.015778\n",
      "step 66900, training accuracy 0.945312\n",
      "Validation accuracy: 0.926467\n",
      "Minibatch loss at step 67000: 0.304817 with learning rate 0.015778\n",
      "step 67000, training accuracy 0.964844\n",
      "Validation accuracy: 0.926233\n",
      "Minibatch loss at step 67100: 0.306355 with learning rate 0.015778\n",
      "step 67100, training accuracy 0.964844\n",
      "Validation accuracy: 0.927033\n",
      "Minibatch loss at step 67200: 0.252139 with learning rate 0.015778\n",
      "step 67200, training accuracy 0.964844\n",
      "Validation accuracy: 0.9265\n",
      "Minibatch loss at step 67300: 0.257605 with learning rate 0.015778\n",
      "step 67300, training accuracy 0.984375\n",
      "Validation accuracy: 0.926617\n",
      "Minibatch loss at step 67400: 0.246149 with learning rate 0.015778\n",
      "step 67400, training accuracy 0.964844\n",
      "Validation accuracy: 0.92605\n",
      "Minibatch loss at step 67500: 0.248401 with learning rate 0.015778\n",
      "step 67500, training accuracy 0.960938\n",
      "Validation accuracy: 0.926567\n",
      "Minibatch loss at step 67600: 0.233235 with learning rate 0.015778\n",
      "step 67600, training accuracy 0.980469\n",
      "Validation accuracy: 0.926733\n",
      "Minibatch loss at step 67700: 0.286619 with learning rate 0.015778\n",
      "step 67700, training accuracy 0.988281\n",
      "Validation accuracy: 0.92635\n",
      "Minibatch loss at step 67800: 0.280891 with learning rate 0.015778\n",
      "step 67800, training accuracy 0.957031\n",
      "Validation accuracy: 0.927033\n",
      "Minibatch loss at step 67900: 0.276951 with learning rate 0.014989\n",
      "step 67900, training accuracy 0.976562\n",
      "Validation accuracy: 0.926383\n",
      "Minibatch loss at step 68000: 0.345361 with learning rate 0.014989\n",
      "step 68000, training accuracy 0.929688\n",
      "Validation accuracy: 0.927183\n",
      "Minibatch loss at step 68100: 0.222985 with learning rate 0.014989\n",
      "step 68100, training accuracy 0.972656\n",
      "Validation accuracy: 0.927133\n",
      "Minibatch loss at step 68200: 0.239551 with learning rate 0.014989\n",
      "step 68200, training accuracy 0.980469\n",
      "Validation accuracy: 0.926483\n",
      "Minibatch loss at step 68300: 0.249228 with learning rate 0.014989\n",
      "step 68300, training accuracy 0.960938\n",
      "Validation accuracy: 0.926133\n",
      "Minibatch loss at step 68400: 0.245767 with learning rate 0.014989\n",
      "step 68400, training accuracy 0.976562\n",
      "Validation accuracy: 0.926783\n",
      "Minibatch loss at step 68500: 0.317010 with learning rate 0.014989\n",
      "step 68500, training accuracy 0.960938\n",
      "Validation accuracy: 0.9262\n",
      "Minibatch loss at step 68600: 0.238327 with learning rate 0.014989\n",
      "step 68600, training accuracy 0.972656\n",
      "Validation accuracy: 0.927467\n",
      "Minibatch loss at step 68700: 0.266235 with learning rate 0.014989\n",
      "step 68700, training accuracy 0.957031\n",
      "Validation accuracy: 0.926783\n",
      "Minibatch loss at step 68800: 0.279630 with learning rate 0.014989\n",
      "step 68800, training accuracy 0.976562\n",
      "Validation accuracy: 0.926817\n",
      "Minibatch loss at step 68900: 0.211344 with learning rate 0.014989\n",
      "step 68900, training accuracy 0.984375\n",
      "Validation accuracy: 0.9269\n",
      "Minibatch loss at step 69000: 0.274843 with learning rate 0.014989\n",
      "step 69000, training accuracy 0.964844\n",
      "Validation accuracy: 0.926733\n",
      "Minibatch loss at step 69100: 0.217954 with learning rate 0.014989\n",
      "step 69100, training accuracy 0.976562\n",
      "Validation accuracy: 0.92665\n",
      "Minibatch loss at step 69200: 0.290211 with learning rate 0.014989\n",
      "step 69200, training accuracy 0.953125\n",
      "Validation accuracy: 0.926583\n",
      "Minibatch loss at step 69300: 0.327318 with learning rate 0.014989\n",
      "step 69300, training accuracy 0.949219\n",
      "Validation accuracy: 0.926633\n",
      "Minibatch loss at step 69400: 0.280910 with learning rate 0.014989\n",
      "step 69400, training accuracy 0.945312\n",
      "Validation accuracy: 0.926633\n",
      "Minibatch loss at step 69500: 0.250306 with learning rate 0.014989\n",
      "step 69500, training accuracy 0.972656\n",
      "Validation accuracy: 0.926333\n",
      "Minibatch loss at step 69600: 0.305300 with learning rate 0.014989\n",
      "step 69600, training accuracy 0.964844\n",
      "Validation accuracy: 0.92665\n",
      "Minibatch loss at step 69700: 0.250193 with learning rate 0.014240\n",
      "step 69700, training accuracy 0.980469\n",
      "Validation accuracy: 0.92635\n",
      "Minibatch loss at step 69800: 0.249387 with learning rate 0.014240\n",
      "step 69800, training accuracy 0.976562\n",
      "Validation accuracy: 0.926567\n",
      "Minibatch loss at step 69900: 0.271949 with learning rate 0.014240\n",
      "step 69900, training accuracy 0.964844\n",
      "Validation accuracy: 0.927167\n",
      "Minibatch loss at step 70000: 0.399339 with learning rate 0.014240\n",
      "step 70000, training accuracy 0.933594\n",
      "Validation accuracy: 0.926233\n",
      "Minibatch loss at step 70100: 0.227035 with learning rate 0.014240\n",
      "step 70100, training accuracy 0.972656\n",
      "Validation accuracy: 0.926467\n",
      "Minibatch loss at step 70200: 0.384367 with learning rate 0.014240\n",
      "step 70200, training accuracy 0.953125\n",
      "Validation accuracy: 0.927017\n",
      "Minibatch loss at step 70300: 0.266616 with learning rate 0.014240\n",
      "step 70300, training accuracy 0.972656\n",
      "Validation accuracy: 0.927067\n",
      "Minibatch loss at step 70400: 0.262669 with learning rate 0.014240\n",
      "step 70400, training accuracy 0.957031\n",
      "Validation accuracy: 0.927117\n",
      "Minibatch loss at step 70500: 0.302248 with learning rate 0.014240\n",
      "step 70500, training accuracy 0.941406\n",
      "Validation accuracy: 0.926867\n",
      "Minibatch loss at step 70600: 0.212472 with learning rate 0.014240\n",
      "step 70600, training accuracy 0.980469\n",
      "Validation accuracy: 0.927367\n",
      "Minibatch loss at step 70700: 0.286201 with learning rate 0.014240\n",
      "step 70700, training accuracy 0.957031\n",
      "Validation accuracy: 0.927083\n",
      "Minibatch loss at step 70800: 0.280085 with learning rate 0.014240\n",
      "step 70800, training accuracy 0.960938\n",
      "Validation accuracy: 0.9276\n",
      "Minibatch loss at step 70900: 0.226133 with learning rate 0.014240\n",
      "step 70900, training accuracy 0.972656\n",
      "Validation accuracy: 0.927383\n",
      "Minibatch loss at step 71000: 0.245648 with learning rate 0.014240\n",
      "step 71000, training accuracy 0.960938\n",
      "Validation accuracy: 0.927\n",
      "Minibatch loss at step 71100: 0.281596 with learning rate 0.014240\n",
      "step 71100, training accuracy 0.96875\n",
      "Validation accuracy: 0.92695\n",
      "Minibatch loss at step 71200: 0.246152 with learning rate 0.014240\n",
      "step 71200, training accuracy 0.96875\n",
      "Validation accuracy: 0.926433\n",
      "Minibatch loss at step 71300: 0.362875 with learning rate 0.014240\n",
      "step 71300, training accuracy 0.945312\n",
      "Validation accuracy: 0.926767\n",
      "Minibatch loss at step 71400: 0.277412 with learning rate 0.014240\n",
      "step 71400, training accuracy 0.972656\n",
      "Validation accuracy: 0.92705\n",
      "Minibatch loss at step 71500: 0.244070 with learning rate 0.013528\n",
      "step 71500, training accuracy 0.96875\n",
      "Validation accuracy: 0.927433\n",
      "Minibatch loss at step 71600: 0.239371 with learning rate 0.013528\n",
      "step 71600, training accuracy 0.972656\n",
      "Validation accuracy: 0.926917\n",
      "Minibatch loss at step 71700: 0.285572 with learning rate 0.013528\n",
      "step 71700, training accuracy 0.957031\n",
      "Validation accuracy: 0.9279\n",
      "Minibatch loss at step 71800: 0.257284 with learning rate 0.013528\n",
      "step 71800, training accuracy 0.964844\n",
      "Validation accuracy: 0.927517\n",
      "Minibatch loss at step 71900: 0.250862 with learning rate 0.013528\n",
      "step 71900, training accuracy 0.980469\n",
      "Validation accuracy: 0.92755\n",
      "Minibatch loss at step 72000: 0.231639 with learning rate 0.013528\n",
      "step 72000, training accuracy 0.972656\n",
      "Validation accuracy: 0.92685\n",
      "Minibatch loss at step 72100: 0.228054 with learning rate 0.013528\n",
      "step 72100, training accuracy 0.976562\n",
      "Validation accuracy: 0.92705\n",
      "Minibatch loss at step 72200: 0.329352 with learning rate 0.013528\n",
      "step 72200, training accuracy 0.953125\n",
      "Validation accuracy: 0.92715\n",
      "Minibatch loss at step 72300: 0.253155 with learning rate 0.013528\n",
      "step 72300, training accuracy 0.964844\n",
      "Validation accuracy: 0.927417\n",
      "Minibatch loss at step 72400: 0.266245 with learning rate 0.013528\n",
      "step 72400, training accuracy 0.964844\n",
      "Validation accuracy: 0.927217\n",
      "Minibatch loss at step 72500: 0.266030 with learning rate 0.013528\n",
      "step 72500, training accuracy 0.949219\n",
      "Validation accuracy: 0.9276\n",
      "Minibatch loss at step 72600: 0.215007 with learning rate 0.013528\n",
      "step 72600, training accuracy 0.976562\n",
      "Validation accuracy: 0.927683\n",
      "Minibatch loss at step 72700: 0.277459 with learning rate 0.013528\n",
      "step 72700, training accuracy 0.96875\n",
      "Validation accuracy: 0.92705\n",
      "Minibatch loss at step 72800: 0.258999 with learning rate 0.013528\n",
      "step 72800, training accuracy 0.976562\n",
      "Validation accuracy: 0.927517\n",
      "Minibatch loss at step 72900: 0.304403 with learning rate 0.013528\n",
      "step 72900, training accuracy 0.960938\n",
      "Validation accuracy: 0.926783\n",
      "Minibatch loss at step 73000: 0.218722 with learning rate 0.013528\n",
      "step 73000, training accuracy 0.976562\n",
      "Validation accuracy: 0.92675\n",
      "Minibatch loss at step 73100: 0.209655 with learning rate 0.013528\n",
      "step 73100, training accuracy 0.980469\n",
      "Validation accuracy: 0.927367\n",
      "Minibatch loss at step 73200: 0.217518 with learning rate 0.013528\n",
      "step 73200, training accuracy 0.976562\n",
      "Validation accuracy: 0.9267\n",
      "Minibatch loss at step 73300: 0.303480 with learning rate 0.012851\n",
      "step 73300, training accuracy 0.960938\n",
      "Validation accuracy: 0.927217\n",
      "Minibatch loss at step 73400: 0.275637 with learning rate 0.012851\n",
      "step 73400, training accuracy 0.949219\n",
      "Validation accuracy: 0.927217\n",
      "Minibatch loss at step 73500: 0.356644 with learning rate 0.012851\n",
      "step 73500, training accuracy 0.953125\n",
      "Validation accuracy: 0.927133\n",
      "Minibatch loss at step 73600: 0.278309 with learning rate 0.012851\n",
      "step 73600, training accuracy 0.976562\n",
      "Validation accuracy: 0.9275\n",
      "Minibatch loss at step 73700: 0.223324 with learning rate 0.012851\n",
      "step 73700, training accuracy 0.972656\n",
      "Validation accuracy: 0.927683\n",
      "Minibatch loss at step 73800: 0.346857 with learning rate 0.012851\n",
      "step 73800, training accuracy 0.941406\n",
      "Validation accuracy: 0.92785\n",
      "Minibatch loss at step 73900: 0.319814 with learning rate 0.012851\n",
      "step 73900, training accuracy 0.960938\n",
      "Validation accuracy: 0.927033\n",
      "Minibatch loss at step 74000: 0.266840 with learning rate 0.012851\n",
      "step 74000, training accuracy 0.96875\n",
      "Validation accuracy: 0.927467\n",
      "Minibatch loss at step 74100: 0.249487 with learning rate 0.012851\n",
      "step 74100, training accuracy 0.960938\n",
      "Validation accuracy: 0.927483\n",
      "Minibatch loss at step 74200: 0.204147 with learning rate 0.012851\n",
      "step 74200, training accuracy 0.988281\n",
      "Validation accuracy: 0.9274\n",
      "Minibatch loss at step 74300: 0.266248 with learning rate 0.012851\n",
      "step 74300, training accuracy 0.960938\n",
      "Validation accuracy: 0.927417\n",
      "Minibatch loss at step 74400: 0.235212 with learning rate 0.012851\n",
      "step 74400, training accuracy 0.984375\n",
      "Validation accuracy: 0.92705\n",
      "Minibatch loss at step 74500: 0.276862 with learning rate 0.012851\n",
      "step 74500, training accuracy 0.972656\n",
      "Validation accuracy: 0.927367\n",
      "Minibatch loss at step 74600: 0.326852 with learning rate 0.012851\n",
      "step 74600, training accuracy 0.953125\n",
      "Validation accuracy: 0.926917\n",
      "Minibatch loss at step 74700: 0.246123 with learning rate 0.012851\n",
      "step 74700, training accuracy 0.984375\n",
      "Validation accuracy: 0.927567\n",
      "Minibatch loss at step 74800: 0.250954 with learning rate 0.012851\n",
      "step 74800, training accuracy 0.96875\n",
      "Validation accuracy: 0.926433\n",
      "Minibatch loss at step 74900: 0.312412 with learning rate 0.012851\n",
      "step 74900, training accuracy 0.949219\n",
      "Validation accuracy: 0.927833\n",
      "Minibatch loss at step 75000: 0.305385 with learning rate 0.012851\n",
      "step 75000, training accuracy 0.953125\n",
      "Validation accuracy: 0.927317\n",
      "Minibatch loss at step 75100: 0.262298 with learning rate 0.012851\n",
      "step 75100, training accuracy 0.96875\n",
      "Validation accuracy: 0.927533\n",
      "Minibatch loss at step 75200: 0.233410 with learning rate 0.012209\n",
      "step 75200, training accuracy 0.96875\n",
      "Validation accuracy: 0.927217\n",
      "Minibatch loss at step 75300: 0.328888 with learning rate 0.012209\n",
      "step 75300, training accuracy 0.949219\n",
      "Validation accuracy: 0.9279\n",
      "Minibatch loss at step 75400: 0.306537 with learning rate 0.012209\n",
      "step 75400, training accuracy 0.960938\n",
      "Validation accuracy: 0.927533\n",
      "Minibatch loss at step 75500: 0.323252 with learning rate 0.012209\n",
      "step 75500, training accuracy 0.953125\n",
      "Validation accuracy: 0.9272\n",
      "Minibatch loss at step 75600: 0.240771 with learning rate 0.012209\n",
      "step 75600, training accuracy 0.972656\n",
      "Validation accuracy: 0.9273\n",
      "Minibatch loss at step 75700: 0.256640 with learning rate 0.012209\n",
      "step 75700, training accuracy 0.96875\n",
      "Validation accuracy: 0.927517\n",
      "Minibatch loss at step 75800: 0.217996 with learning rate 0.012209\n",
      "step 75800, training accuracy 0.96875\n",
      "Validation accuracy: 0.927383\n",
      "Minibatch loss at step 75900: 0.233349 with learning rate 0.012209\n",
      "step 75900, training accuracy 0.980469\n",
      "Validation accuracy: 0.92795\n",
      "Minibatch loss at step 76000: 0.291239 with learning rate 0.012209\n",
      "step 76000, training accuracy 0.953125\n",
      "Validation accuracy: 0.92745\n",
      "Minibatch loss at step 76100: 0.277566 with learning rate 0.012209\n",
      "step 76100, training accuracy 0.960938\n",
      "Validation accuracy: 0.927467\n",
      "Minibatch loss at step 76200: 0.238585 with learning rate 0.012209\n",
      "step 76200, training accuracy 0.972656\n",
      "Validation accuracy: 0.927567\n",
      "Minibatch loss at step 76300: 0.233229 with learning rate 0.012209\n",
      "step 76300, training accuracy 0.976562\n",
      "Validation accuracy: 0.927517\n",
      "Minibatch loss at step 76400: 0.303441 with learning rate 0.012209\n",
      "step 76400, training accuracy 0.960938\n",
      "Validation accuracy: 0.927467\n",
      "Minibatch loss at step 76500: 0.196067 with learning rate 0.012209\n",
      "step 76500, training accuracy 0.984375\n",
      "Validation accuracy: 0.927733\n",
      "Minibatch loss at step 76600: 0.271856 with learning rate 0.012209\n",
      "step 76600, training accuracy 0.96875\n",
      "Validation accuracy: 0.92715\n",
      "Minibatch loss at step 76700: 0.243617 with learning rate 0.012209\n",
      "step 76700, training accuracy 0.980469\n",
      "Validation accuracy: 0.926317\n",
      "Minibatch loss at step 76800: 0.338922 with learning rate 0.012209\n",
      "step 76800, training accuracy 0.949219\n",
      "Validation accuracy: 0.927083\n",
      "Minibatch loss at step 76900: 0.252485 with learning rate 0.012209\n",
      "step 76900, training accuracy 0.960938\n",
      "Validation accuracy: 0.926967\n",
      "Minibatch loss at step 77000: 0.320230 with learning rate 0.011598\n",
      "step 77000, training accuracy 0.949219\n",
      "Validation accuracy: 0.927983\n",
      "Minibatch loss at step 77100: 0.269446 with learning rate 0.011598\n",
      "step 77100, training accuracy 0.953125\n",
      "Validation accuracy: 0.927183\n",
      "Minibatch loss at step 77200: 0.244080 with learning rate 0.011598\n",
      "step 77200, training accuracy 0.96875\n",
      "Validation accuracy: 0.928067\n",
      "Minibatch loss at step 77300: 0.279276 with learning rate 0.011598\n",
      "step 77300, training accuracy 0.960938\n",
      "Validation accuracy: 0.92795\n",
      "Minibatch loss at step 77400: 0.270478 with learning rate 0.011598\n",
      "step 77400, training accuracy 0.953125\n",
      "Validation accuracy: 0.927633\n",
      "Minibatch loss at step 77500: 0.259445 with learning rate 0.011598\n",
      "step 77500, training accuracy 0.972656\n",
      "Validation accuracy: 0.927817\n",
      "Minibatch loss at step 77600: 0.299631 with learning rate 0.011598\n",
      "step 77600, training accuracy 0.945312\n",
      "Validation accuracy: 0.927867\n",
      "Minibatch loss at step 77700: 0.265603 with learning rate 0.011598\n",
      "step 77700, training accuracy 0.96875\n",
      "Validation accuracy: 0.92745\n",
      "Minibatch loss at step 77800: 0.216109 with learning rate 0.011598\n",
      "step 77800, training accuracy 0.96875\n",
      "Validation accuracy: 0.927417\n",
      "Minibatch loss at step 77900: 0.248783 with learning rate 0.011598\n",
      "step 77900, training accuracy 0.96875\n",
      "Validation accuracy: 0.927283\n",
      "Minibatch loss at step 78000: 0.223946 with learning rate 0.011598\n",
      "step 78000, training accuracy 0.976562\n",
      "Validation accuracy: 0.926883\n",
      "Minibatch loss at step 78100: 0.253069 with learning rate 0.011598\n",
      "step 78100, training accuracy 0.96875\n",
      "Validation accuracy: 0.928083\n",
      "Minibatch loss at step 78200: 0.283997 with learning rate 0.011598\n",
      "step 78200, training accuracy 0.957031\n",
      "Validation accuracy: 0.927533\n",
      "Minibatch loss at step 78300: 0.222688 with learning rate 0.011598\n",
      "step 78300, training accuracy 0.976562\n",
      "Validation accuracy: 0.927417\n",
      "Minibatch loss at step 78400: 0.275291 with learning rate 0.011598\n",
      "step 78400, training accuracy 0.964844\n",
      "Validation accuracy: 0.92755\n",
      "Minibatch loss at step 78500: 0.276283 with learning rate 0.011598\n",
      "step 78500, training accuracy 0.964844\n",
      "Validation accuracy: 0.927717\n",
      "Minibatch loss at step 78600: 0.325637 with learning rate 0.011598\n",
      "step 78600, training accuracy 0.945312\n",
      "Validation accuracy: 0.927633\n",
      "Minibatch loss at step 78700: 0.326752 with learning rate 0.011598\n",
      "step 78700, training accuracy 0.945312\n",
      "Validation accuracy: 0.92775\n",
      "Minibatch loss at step 78800: 0.265812 with learning rate 0.011018\n",
      "step 78800, training accuracy 0.957031\n",
      "Validation accuracy: 0.927617\n",
      "Minibatch loss at step 78900: 0.228589 with learning rate 0.011018\n",
      "step 78900, training accuracy 0.964844\n",
      "Validation accuracy: 0.927117\n",
      "Minibatch loss at step 79000: 0.220169 with learning rate 0.011018\n",
      "step 79000, training accuracy 0.976562\n",
      "Validation accuracy: 0.928183\n",
      "Minibatch loss at step 79100: 0.260767 with learning rate 0.011018\n",
      "step 79100, training accuracy 0.96875\n",
      "Validation accuracy: 0.9275\n",
      "Minibatch loss at step 79200: 0.324514 with learning rate 0.011018\n",
      "step 79200, training accuracy 0.953125\n",
      "Validation accuracy: 0.92765\n",
      "Minibatch loss at step 79300: 0.193372 with learning rate 0.011018\n",
      "step 79300, training accuracy 0.972656\n",
      "Validation accuracy: 0.927917\n",
      "Minibatch loss at step 79400: 0.369674 with learning rate 0.011018\n",
      "step 79400, training accuracy 0.941406\n",
      "Validation accuracy: 0.927867\n",
      "Minibatch loss at step 79500: 0.282892 with learning rate 0.011018\n",
      "step 79500, training accuracy 0.964844\n",
      "Validation accuracy: 0.928083\n",
      "Minibatch loss at step 79600: 0.275673 with learning rate 0.011018\n",
      "step 79600, training accuracy 0.960938\n",
      "Validation accuracy: 0.927833\n",
      "Minibatch loss at step 79700: 0.185675 with learning rate 0.011018\n",
      "step 79700, training accuracy 0.988281\n",
      "Validation accuracy: 0.927883\n",
      "Minibatch loss at step 79800: 0.288591 with learning rate 0.011018\n",
      "step 79800, training accuracy 0.960938\n",
      "Validation accuracy: 0.927617\n",
      "Minibatch loss at step 79900: 0.269353 with learning rate 0.011018\n",
      "step 79900, training accuracy 0.96875\n",
      "Validation accuracy: 0.928767\n",
      "Minibatch loss at step 80000: 0.285802 with learning rate 0.011018\n",
      "step 80000, training accuracy 0.960938\n",
      "Validation accuracy: 0.927617\n",
      "Minibatch loss at step 80100: 0.188625 with learning rate 0.011018\n",
      "step 80100, training accuracy 0.992188\n",
      "Validation accuracy: 0.927617\n",
      "Minibatch loss at step 80200: 0.283401 with learning rate 0.011018\n",
      "step 80200, training accuracy 0.957031\n",
      "Validation accuracy: 0.926883\n",
      "Minibatch loss at step 80300: 0.273094 with learning rate 0.011018\n",
      "step 80300, training accuracy 0.960938\n",
      "Validation accuracy: 0.927267\n",
      "Minibatch loss at step 80400: 0.294863 with learning rate 0.011018\n",
      "step 80400, training accuracy 0.964844\n",
      "Validation accuracy: 0.927683\n",
      "Minibatch loss at step 80500: 0.222645 with learning rate 0.011018\n",
      "step 80500, training accuracy 0.984375\n",
      "Validation accuracy: 0.927917\n",
      "Minibatch loss at step 80600: 0.337535 with learning rate 0.011018\n",
      "step 80600, training accuracy 0.949219\n",
      "Validation accuracy: 0.927883\n",
      "Minibatch loss at step 80700: 0.278685 with learning rate 0.010467\n",
      "step 80700, training accuracy 0.960938\n",
      "Validation accuracy: 0.927883\n",
      "Minibatch loss at step 80800: 0.194842 with learning rate 0.010467\n",
      "step 80800, training accuracy 0.984375\n",
      "Validation accuracy: 0.927683\n",
      "Minibatch loss at step 80900: 0.227796 with learning rate 0.010467\n",
      "step 80900, training accuracy 0.972656\n",
      "Validation accuracy: 0.927967\n",
      "Minibatch loss at step 81000: 0.331397 with learning rate 0.010467\n",
      "step 81000, training accuracy 0.957031\n",
      "Validation accuracy: 0.92775\n",
      "Minibatch loss at step 81100: 0.251559 with learning rate 0.010467\n",
      "step 81100, training accuracy 0.96875\n",
      "Validation accuracy: 0.927983\n",
      "Minibatch loss at step 81200: 0.261913 with learning rate 0.010467\n",
      "step 81200, training accuracy 0.953125\n",
      "Validation accuracy: 0.9281\n",
      "Minibatch loss at step 81300: 0.310250 with learning rate 0.010467\n",
      "step 81300, training accuracy 0.949219\n",
      "Validation accuracy: 0.927483\n",
      "Minibatch loss at step 81400: 0.268401 with learning rate 0.010467\n",
      "step 81400, training accuracy 0.96875\n",
      "Validation accuracy: 0.92825\n",
      "Minibatch loss at step 81500: 0.234514 with learning rate 0.010467\n",
      "step 81500, training accuracy 0.972656\n",
      "Validation accuracy: 0.9275\n",
      "Minibatch loss at step 81600: 0.270904 with learning rate 0.010467\n",
      "step 81600, training accuracy 0.960938\n",
      "Validation accuracy: 0.928267\n",
      "Minibatch loss at step 81700: 0.211468 with learning rate 0.010467\n",
      "step 81700, training accuracy 0.980469\n",
      "Validation accuracy: 0.928167\n",
      "Minibatch loss at step 81800: 0.270862 with learning rate 0.010467\n",
      "step 81800, training accuracy 0.957031\n",
      "Validation accuracy: 0.9282\n",
      "Minibatch loss at step 81900: 0.264210 with learning rate 0.010467\n",
      "step 81900, training accuracy 0.96875\n",
      "Validation accuracy: 0.92805\n",
      "Minibatch loss at step 82000: 0.221483 with learning rate 0.010467\n",
      "step 82000, training accuracy 0.972656\n",
      "Validation accuracy: 0.928217\n",
      "Minibatch loss at step 82100: 0.293565 with learning rate 0.010467\n",
      "step 82100, training accuracy 0.964844\n",
      "Validation accuracy: 0.9277\n",
      "Minibatch loss at step 82200: 0.250573 with learning rate 0.010467\n",
      "step 82200, training accuracy 0.960938\n",
      "Validation accuracy: 0.927883\n",
      "Minibatch loss at step 82300: 0.338334 with learning rate 0.010467\n",
      "step 82300, training accuracy 0.953125\n",
      "Validation accuracy: 0.9279\n",
      "Minibatch loss at step 82400: 0.322277 with learning rate 0.010467\n",
      "step 82400, training accuracy 0.964844\n",
      "Validation accuracy: 0.92785\n",
      "Minibatch loss at step 82500: 0.295109 with learning rate 0.009944\n",
      "step 82500, training accuracy 0.964844\n",
      "Validation accuracy: 0.928033\n",
      "Minibatch loss at step 82600: 0.283189 with learning rate 0.009944\n",
      "step 82600, training accuracy 0.972656\n",
      "Validation accuracy: 0.927717\n",
      "Minibatch loss at step 82700: 0.251434 with learning rate 0.009944\n",
      "step 82700, training accuracy 0.96875\n",
      "Validation accuracy: 0.927967\n",
      "Minibatch loss at step 82800: 0.330726 with learning rate 0.009944\n",
      "step 82800, training accuracy 0.949219\n",
      "Validation accuracy: 0.928483\n",
      "Minibatch loss at step 82900: 0.269834 with learning rate 0.009944\n",
      "step 82900, training accuracy 0.964844\n",
      "Validation accuracy: 0.9281\n",
      "Minibatch loss at step 83000: 0.271297 with learning rate 0.009944\n",
      "step 83000, training accuracy 0.964844\n",
      "Validation accuracy: 0.928017\n",
      "Minibatch loss at step 83100: 0.321830 with learning rate 0.009944\n",
      "step 83100, training accuracy 0.949219\n",
      "Validation accuracy: 0.928167\n",
      "Minibatch loss at step 83200: 0.348410 with learning rate 0.009944\n",
      "step 83200, training accuracy 0.949219\n",
      "Validation accuracy: 0.9283\n",
      "Minibatch loss at step 83300: 0.226010 with learning rate 0.009944\n",
      "step 83300, training accuracy 0.96875\n",
      "Validation accuracy: 0.928433\n",
      "Minibatch loss at step 83400: 0.243045 with learning rate 0.009944\n",
      "step 83400, training accuracy 0.96875\n",
      "Validation accuracy: 0.928717\n",
      "Minibatch loss at step 83500: 0.258398 with learning rate 0.009944\n",
      "step 83500, training accuracy 0.964844\n",
      "Validation accuracy: 0.928167\n",
      "Minibatch loss at step 83600: 0.229291 with learning rate 0.009944\n",
      "step 83600, training accuracy 0.972656\n",
      "Validation accuracy: 0.928517\n",
      "Minibatch loss at step 83700: 0.250801 with learning rate 0.009944\n",
      "step 83700, training accuracy 0.964844\n",
      "Validation accuracy: 0.928367\n",
      "Minibatch loss at step 83800: 0.235569 with learning rate 0.009944\n",
      "step 83800, training accuracy 0.953125\n",
      "Validation accuracy: 0.9287\n",
      "Minibatch loss at step 83900: 0.249805 with learning rate 0.009944\n",
      "step 83900, training accuracy 0.980469\n",
      "Validation accuracy: 0.927883\n",
      "Minibatch loss at step 84000: 0.226359 with learning rate 0.009944\n",
      "step 84000, training accuracy 0.976562\n",
      "Validation accuracy: 0.92795\n",
      "Minibatch loss at step 84100: 0.278089 with learning rate 0.009944\n",
      "step 84100, training accuracy 0.972656\n",
      "Validation accuracy: 0.9283\n",
      "Minibatch loss at step 84200: 0.243739 with learning rate 0.009944\n",
      "step 84200, training accuracy 0.96875\n",
      "Validation accuracy: 0.928283\n",
      "Minibatch loss at step 84300: 0.285169 with learning rate 0.009447\n",
      "step 84300, training accuracy 0.9375\n",
      "Validation accuracy: 0.92755\n",
      "Minibatch loss at step 84400: 0.214853 with learning rate 0.009447\n",
      "step 84400, training accuracy 0.988281\n",
      "Validation accuracy: 0.927983\n",
      "Minibatch loss at step 84500: 0.282545 with learning rate 0.009447\n",
      "step 84500, training accuracy 0.964844\n",
      "Validation accuracy: 0.927683\n",
      "Minibatch loss at step 84600: 0.279364 with learning rate 0.009447\n",
      "step 84600, training accuracy 0.964844\n",
      "Validation accuracy: 0.927967\n",
      "Minibatch loss at step 84700: 0.290639 with learning rate 0.009447\n",
      "step 84700, training accuracy 0.945312\n",
      "Validation accuracy: 0.928167\n",
      "Minibatch loss at step 84800: 0.230903 with learning rate 0.009447\n",
      "step 84800, training accuracy 0.980469\n",
      "Validation accuracy: 0.92865\n",
      "Minibatch loss at step 84900: 0.378400 with learning rate 0.009447\n",
      "step 84900, training accuracy 0.9375\n",
      "Validation accuracy: 0.92835\n",
      "Minibatch loss at step 85000: 0.220864 with learning rate 0.009447\n",
      "step 85000, training accuracy 0.980469\n",
      "Validation accuracy: 0.92825\n",
      "Minibatch loss at step 85100: 0.195333 with learning rate 0.009447\n",
      "step 85100, training accuracy 0.976562\n",
      "Validation accuracy: 0.92785\n",
      "Minibatch loss at step 85200: 0.383547 with learning rate 0.009447\n",
      "step 85200, training accuracy 0.917969\n",
      "Validation accuracy: 0.9274\n",
      "Minibatch loss at step 85300: 0.302937 with learning rate 0.009447\n",
      "step 85300, training accuracy 0.964844\n",
      "Validation accuracy: 0.927433\n",
      "Minibatch loss at step 85400: 0.325950 with learning rate 0.009447\n",
      "step 85400, training accuracy 0.945312\n",
      "Validation accuracy: 0.927983\n",
      "Minibatch loss at step 85500: 0.229161 with learning rate 0.009447\n",
      "step 85500, training accuracy 0.976562\n",
      "Validation accuracy: 0.927967\n",
      "Minibatch loss at step 85600: 0.226153 with learning rate 0.009447\n",
      "step 85600, training accuracy 0.96875\n",
      "Validation accuracy: 0.927867\n",
      "Minibatch loss at step 85700: 0.280292 with learning rate 0.009447\n",
      "step 85700, training accuracy 0.964844\n",
      "Validation accuracy: 0.927817\n",
      "Minibatch loss at step 85800: 0.204221 with learning rate 0.009447\n",
      "step 85800, training accuracy 0.988281\n",
      "Validation accuracy: 0.928267\n",
      "Minibatch loss at step 85900: 0.253081 with learning rate 0.009447\n",
      "step 85900, training accuracy 0.984375\n",
      "Validation accuracy: 0.92875\n",
      "Minibatch loss at step 86000: 0.268513 with learning rate 0.009447\n",
      "step 86000, training accuracy 0.964844\n",
      "Validation accuracy: 0.928317\n",
      "Minibatch loss at step 86100: 0.272771 with learning rate 0.009447\n",
      "step 86100, training accuracy 0.957031\n",
      "Validation accuracy: 0.928333\n",
      "Minibatch loss at step 86200: 0.278830 with learning rate 0.008974\n",
      "step 86200, training accuracy 0.960938\n",
      "Validation accuracy: 0.9286\n",
      "Minibatch loss at step 86300: 0.266086 with learning rate 0.008974\n",
      "step 86300, training accuracy 0.957031\n",
      "Validation accuracy: 0.9285\n",
      "Minibatch loss at step 86400: 0.291998 with learning rate 0.008974\n",
      "step 86400, training accuracy 0.960938\n",
      "Validation accuracy: 0.928333\n",
      "Minibatch loss at step 86500: 0.227850 with learning rate 0.008974\n",
      "step 86500, training accuracy 0.976562\n",
      "Validation accuracy: 0.9285\n",
      "Minibatch loss at step 86600: 0.247987 with learning rate 0.008974\n",
      "step 86600, training accuracy 0.980469\n",
      "Validation accuracy: 0.928083\n",
      "Minibatch loss at step 86700: 0.231668 with learning rate 0.008974\n",
      "step 86700, training accuracy 0.96875\n",
      "Validation accuracy: 0.928167\n",
      "Minibatch loss at step 86800: 0.257663 with learning rate 0.008974\n",
      "step 86800, training accuracy 0.96875\n",
      "Validation accuracy: 0.92845\n",
      "Minibatch loss at step 86900: 0.226242 with learning rate 0.008974\n",
      "step 86900, training accuracy 0.984375\n",
      "Validation accuracy: 0.928733\n",
      "Minibatch loss at step 87000: 0.236940 with learning rate 0.008974\n",
      "step 87000, training accuracy 0.964844\n",
      "Validation accuracy: 0.928367\n",
      "Minibatch loss at step 87100: 0.237098 with learning rate 0.008974\n",
      "step 87100, training accuracy 0.96875\n",
      "Validation accuracy: 0.928183\n",
      "Minibatch loss at step 87200: 0.292403 with learning rate 0.008974\n",
      "step 87200, training accuracy 0.953125\n",
      "Validation accuracy: 0.928883\n",
      "Minibatch loss at step 87300: 0.313636 with learning rate 0.008974\n",
      "step 87300, training accuracy 0.953125\n",
      "Validation accuracy: 0.9283\n",
      "Minibatch loss at step 87400: 0.252964 with learning rate 0.008974\n",
      "step 87400, training accuracy 0.976562\n",
      "Validation accuracy: 0.929233\n",
      "Minibatch loss at step 87500: 0.271373 with learning rate 0.008974\n",
      "step 87500, training accuracy 0.960938\n",
      "Validation accuracy: 0.9284\n",
      "Minibatch loss at step 87600: 0.288078 with learning rate 0.008974\n",
      "step 87600, training accuracy 0.960938\n",
      "Validation accuracy: 0.927683\n",
      "Minibatch loss at step 87700: 0.299913 with learning rate 0.008974\n",
      "step 87700, training accuracy 0.964844\n",
      "Validation accuracy: 0.928233\n",
      "Minibatch loss at step 87800: 0.218357 with learning rate 0.008974\n",
      "step 87800, training accuracy 0.976562\n",
      "Validation accuracy: 0.927933\n",
      "Minibatch loss at step 87900: 0.248940 with learning rate 0.008974\n",
      "step 87900, training accuracy 0.964844\n",
      "Validation accuracy: 0.928167\n",
      "Minibatch loss at step 88000: 0.183879 with learning rate 0.008526\n",
      "step 88000, training accuracy 0.996094\n",
      "Validation accuracy: 0.928267\n",
      "Minibatch loss at step 88100: 0.259481 with learning rate 0.008526\n",
      "step 88100, training accuracy 0.960938\n",
      "Validation accuracy: 0.928083\n",
      "Minibatch loss at step 88200: 0.194795 with learning rate 0.008526\n",
      "step 88200, training accuracy 0.976562\n",
      "Validation accuracy: 0.92825\n",
      "Minibatch loss at step 88300: 0.238483 with learning rate 0.008526\n",
      "step 88300, training accuracy 0.980469\n",
      "Validation accuracy: 0.92865\n",
      "Minibatch loss at step 88400: 0.237018 with learning rate 0.008526\n",
      "step 88400, training accuracy 0.976562\n",
      "Validation accuracy: 0.928583\n",
      "Minibatch loss at step 88500: 0.254308 with learning rate 0.008526\n",
      "step 88500, training accuracy 0.972656\n",
      "Validation accuracy: 0.9283\n",
      "Minibatch loss at step 88600: 0.207068 with learning rate 0.008526\n",
      "step 88600, training accuracy 0.984375\n",
      "Validation accuracy: 0.928583\n",
      "Minibatch loss at step 88700: 0.267758 with learning rate 0.008526\n",
      "step 88700, training accuracy 0.957031\n",
      "Validation accuracy: 0.928983\n",
      "Minibatch loss at step 88800: 0.232565 with learning rate 0.008526\n",
      "step 88800, training accuracy 0.960938\n",
      "Validation accuracy: 0.928667\n",
      "Minibatch loss at step 88900: 0.194239 with learning rate 0.008526\n",
      "step 88900, training accuracy 0.980469\n",
      "Validation accuracy: 0.928783\n",
      "Minibatch loss at step 89000: 0.249683 with learning rate 0.008526\n",
      "step 89000, training accuracy 0.96875\n",
      "Validation accuracy: 0.928\n",
      "Minibatch loss at step 89100: 0.356195 with learning rate 0.008526\n",
      "step 89100, training accuracy 0.957031\n",
      "Validation accuracy: 0.92835\n",
      "Minibatch loss at step 89200: 0.262108 with learning rate 0.008526\n",
      "step 89200, training accuracy 0.964844\n",
      "Validation accuracy: 0.928567\n",
      "Minibatch loss at step 89300: 0.224375 with learning rate 0.008526\n",
      "step 89300, training accuracy 0.972656\n",
      "Validation accuracy: 0.928667\n",
      "Minibatch loss at step 89400: 0.260504 with learning rate 0.008526\n",
      "step 89400, training accuracy 0.96875\n",
      "Validation accuracy: 0.928033\n",
      "Minibatch loss at step 89500: 0.221702 with learning rate 0.008526\n",
      "step 89500, training accuracy 0.96875\n",
      "Validation accuracy: 0.928167\n",
      "Minibatch loss at step 89600: 0.213233 with learning rate 0.008526\n",
      "step 89600, training accuracy 0.984375\n",
      "Validation accuracy: 0.928917\n",
      "Minibatch loss at step 89700: 0.245539 with learning rate 0.008526\n",
      "step 89700, training accuracy 0.964844\n",
      "Validation accuracy: 0.9282\n",
      "Minibatch loss at step 89800: 0.290409 with learning rate 0.008099\n",
      "step 89800, training accuracy 0.945312\n",
      "Validation accuracy: 0.928133\n",
      "Minibatch loss at step 89900: 0.267620 with learning rate 0.008099\n",
      "step 89900, training accuracy 0.949219\n",
      "Validation accuracy: 0.928817\n",
      "Minibatch loss at step 90000: 0.245064 with learning rate 0.008099\n",
      "step 90000, training accuracy 0.964844\n",
      "Validation accuracy: 0.9288\n",
      "Minibatch loss at step 90100: 0.284739 with learning rate 0.008099\n",
      "step 90100, training accuracy 0.957031\n",
      "Validation accuracy: 0.929133\n",
      "Minibatch loss at step 90200: 0.278671 with learning rate 0.008099\n",
      "step 90200, training accuracy 0.972656\n",
      "Validation accuracy: 0.92885\n",
      "Minibatch loss at step 90300: 0.213504 with learning rate 0.008099\n",
      "step 90300, training accuracy 0.972656\n",
      "Validation accuracy: 0.928983\n",
      "Minibatch loss at step 90400: 0.277736 with learning rate 0.008099\n",
      "step 90400, training accuracy 0.960938\n",
      "Validation accuracy: 0.928833\n",
      "Minibatch loss at step 90500: 0.255825 with learning rate 0.008099\n",
      "step 90500, training accuracy 0.972656\n",
      "Validation accuracy: 0.928667\n",
      "Minibatch loss at step 90600: 0.302297 with learning rate 0.008099\n",
      "step 90600, training accuracy 0.953125\n",
      "Validation accuracy: 0.928567\n",
      "Minibatch loss at step 90700: 0.324401 with learning rate 0.008099\n",
      "step 90700, training accuracy 0.945312\n",
      "Validation accuracy: 0.928917\n",
      "Minibatch loss at step 90800: 0.347500 with learning rate 0.008099\n",
      "step 90800, training accuracy 0.9375\n",
      "Validation accuracy: 0.92845\n",
      "Minibatch loss at step 90900: 0.254404 with learning rate 0.008099\n",
      "step 90900, training accuracy 0.964844\n",
      "Validation accuracy: 0.928467\n",
      "Minibatch loss at step 91000: 0.216461 with learning rate 0.008099\n",
      "step 91000, training accuracy 0.980469\n",
      "Validation accuracy: 0.928767\n",
      "Minibatch loss at step 91100: 0.268517 with learning rate 0.008099\n",
      "step 91100, training accuracy 0.96875\n",
      "Validation accuracy: 0.9282\n",
      "Minibatch loss at step 91200: 0.243529 with learning rate 0.008099\n",
      "step 91200, training accuracy 0.972656\n",
      "Validation accuracy: 0.928283\n",
      "Minibatch loss at step 91300: 0.292839 with learning rate 0.008099\n",
      "step 91300, training accuracy 0.957031\n",
      "Validation accuracy: 0.928\n",
      "Minibatch loss at step 91400: 0.300261 with learning rate 0.008099\n",
      "step 91400, training accuracy 0.964844\n",
      "Validation accuracy: 0.928367\n",
      "Minibatch loss at step 91500: 0.235359 with learning rate 0.008099\n",
      "step 91500, training accuracy 0.976562\n",
      "Validation accuracy: 0.928567\n",
      "Minibatch loss at step 91600: 0.296274 with learning rate 0.008099\n",
      "step 91600, training accuracy 0.957031\n",
      "Validation accuracy: 0.9282\n",
      "Minibatch loss at step 91700: 0.255972 with learning rate 0.007694\n",
      "step 91700, training accuracy 0.976562\n",
      "Validation accuracy: 0.92845\n",
      "Minibatch loss at step 91800: 0.309150 with learning rate 0.007694\n",
      "step 91800, training accuracy 0.957031\n",
      "Validation accuracy: 0.928117\n",
      "Minibatch loss at step 91900: 0.227054 with learning rate 0.007694\n",
      "step 91900, training accuracy 0.96875\n",
      "Validation accuracy: 0.92845\n",
      "Minibatch loss at step 92000: 0.240982 with learning rate 0.007694\n",
      "step 92000, training accuracy 0.964844\n",
      "Validation accuracy: 0.928067\n",
      "Minibatch loss at step 92100: 0.234659 with learning rate 0.007694\n",
      "step 92100, training accuracy 0.980469\n",
      "Validation accuracy: 0.92845\n",
      "Minibatch loss at step 92200: 0.203678 with learning rate 0.007694\n",
      "step 92200, training accuracy 0.976562\n",
      "Validation accuracy: 0.928517\n",
      "Minibatch loss at step 92300: 0.196941 with learning rate 0.007694\n",
      "step 92300, training accuracy 0.984375\n",
      "Validation accuracy: 0.928433\n",
      "Minibatch loss at step 92400: 0.231193 with learning rate 0.007694\n",
      "step 92400, training accuracy 0.972656\n",
      "Validation accuracy: 0.92835\n",
      "Minibatch loss at step 92500: 0.239747 with learning rate 0.007694\n",
      "step 92500, training accuracy 0.96875\n",
      "Validation accuracy: 0.9282\n",
      "Minibatch loss at step 92600: 0.234305 with learning rate 0.007694\n",
      "step 92600, training accuracy 0.96875\n",
      "Validation accuracy: 0.9285\n",
      "Minibatch loss at step 92700: 0.206779 with learning rate 0.007694\n",
      "step 92700, training accuracy 0.972656\n",
      "Validation accuracy: 0.928383\n",
      "Minibatch loss at step 92800: 0.253626 with learning rate 0.007694\n",
      "step 92800, training accuracy 0.980469\n",
      "Validation accuracy: 0.92885\n",
      "Minibatch loss at step 92900: 0.299888 with learning rate 0.007694\n",
      "step 92900, training accuracy 0.953125\n",
      "Validation accuracy: 0.929133\n",
      "Minibatch loss at step 93000: 0.265778 with learning rate 0.007694\n",
      "step 93000, training accuracy 0.976562\n",
      "Validation accuracy: 0.9278\n",
      "Minibatch loss at step 93100: 0.347887 with learning rate 0.007694\n",
      "step 93100, training accuracy 0.9375\n",
      "Validation accuracy: 0.928\n",
      "Minibatch loss at step 93200: 0.188461 with learning rate 0.007694\n",
      "step 93200, training accuracy 0.980469\n",
      "Validation accuracy: 0.92835\n",
      "Minibatch loss at step 93300: 0.217344 with learning rate 0.007694\n",
      "step 93300, training accuracy 0.964844\n",
      "Validation accuracy: 0.92805\n",
      "Minibatch loss at step 93400: 0.285110 with learning rate 0.007694\n",
      "step 93400, training accuracy 0.953125\n",
      "Validation accuracy: 0.92825\n",
      "Minibatch loss at step 93500: 0.185334 with learning rate 0.007310\n",
      "step 93500, training accuracy 0.992188\n",
      "Validation accuracy: 0.928283\n",
      "Minibatch loss at step 93600: 0.238884 with learning rate 0.007310\n",
      "step 93600, training accuracy 0.96875\n",
      "Validation accuracy: 0.92835\n",
      "Minibatch loss at step 93700: 0.274034 with learning rate 0.007310\n",
      "step 93700, training accuracy 0.953125\n",
      "Validation accuracy: 0.928417\n",
      "Minibatch loss at step 93800: 0.297805 with learning rate 0.007310\n",
      "step 93800, training accuracy 0.96875\n",
      "Validation accuracy: 0.928533\n",
      "Minibatch loss at step 93900: 0.213997 with learning rate 0.007310\n",
      "step 93900, training accuracy 0.972656\n",
      "Validation accuracy: 0.9285\n",
      "Minibatch loss at step 94000: 0.300203 with learning rate 0.007310\n",
      "step 94000, training accuracy 0.964844\n",
      "Validation accuracy: 0.929033\n",
      "Minibatch loss at step 94100: 0.212684 with learning rate 0.007310\n",
      "step 94100, training accuracy 0.980469\n",
      "Validation accuracy: 0.928283\n",
      "Minibatch loss at step 94200: 0.263924 with learning rate 0.007310\n",
      "step 94200, training accuracy 0.964844\n",
      "Validation accuracy: 0.928583\n",
      "Minibatch loss at step 94300: 0.234075 with learning rate 0.007310\n",
      "step 94300, training accuracy 0.972656\n",
      "Validation accuracy: 0.928467\n",
      "Minibatch loss at step 94400: 0.264448 with learning rate 0.007310\n",
      "step 94400, training accuracy 0.972656\n",
      "Validation accuracy: 0.928583\n",
      "Minibatch loss at step 94500: 0.205849 with learning rate 0.007310\n",
      "step 94500, training accuracy 0.976562\n",
      "Validation accuracy: 0.928083\n",
      "Minibatch loss at step 94600: 0.250424 with learning rate 0.007310\n",
      "step 94600, training accuracy 0.972656\n",
      "Validation accuracy: 0.929033\n",
      "Minibatch loss at step 94700: 0.246030 with learning rate 0.007310\n",
      "step 94700, training accuracy 0.976562\n",
      "Validation accuracy: 0.92775\n",
      "Minibatch loss at step 94800: 0.279363 with learning rate 0.007310\n",
      "step 94800, training accuracy 0.960938\n",
      "Validation accuracy: 0.928517\n",
      "Minibatch loss at step 94900: 0.238271 with learning rate 0.007310\n",
      "step 94900, training accuracy 0.972656\n",
      "Validation accuracy: 0.928433\n",
      "Minibatch loss at step 95000: 0.265594 with learning rate 0.007310\n",
      "step 95000, training accuracy 0.960938\n",
      "Validation accuracy: 0.928817\n",
      "Minibatch loss at step 95100: 0.219391 with learning rate 0.007310\n",
      "step 95100, training accuracy 0.972656\n",
      "Validation accuracy: 0.928433\n",
      "Minibatch loss at step 95200: 0.185710 with learning rate 0.007310\n",
      "step 95200, training accuracy 0.980469\n",
      "Validation accuracy: 0.928217\n",
      "Minibatch loss at step 95300: 0.304813 with learning rate 0.006944\n",
      "step 95300, training accuracy 0.960938\n",
      "Validation accuracy: 0.928833\n",
      "Minibatch loss at step 95400: 0.260795 with learning rate 0.006944\n",
      "step 95400, training accuracy 0.96875\n",
      "Validation accuracy: 0.92865\n",
      "Minibatch loss at step 95500: 0.245276 with learning rate 0.006944\n",
      "step 95500, training accuracy 0.972656\n",
      "Validation accuracy: 0.928217\n",
      "Minibatch loss at step 95600: 0.189012 with learning rate 0.006944\n",
      "step 95600, training accuracy 0.980469\n",
      "Validation accuracy: 0.928783\n",
      "Minibatch loss at step 95700: 0.213803 with learning rate 0.006944\n",
      "step 95700, training accuracy 0.980469\n",
      "Validation accuracy: 0.928617\n",
      "Minibatch loss at step 95800: 0.347015 with learning rate 0.006944\n",
      "step 95800, training accuracy 0.9375\n",
      "Validation accuracy: 0.928617\n",
      "Minibatch loss at step 95900: 0.188414 with learning rate 0.006944\n",
      "step 95900, training accuracy 0.988281\n",
      "Validation accuracy: 0.9284\n",
      "Minibatch loss at step 96000: 0.233467 with learning rate 0.006944\n",
      "step 96000, training accuracy 0.96875\n",
      "Validation accuracy: 0.928833\n",
      "Minibatch loss at step 96100: 0.258579 with learning rate 0.006944\n",
      "step 96100, training accuracy 0.960938\n",
      "Validation accuracy: 0.929083\n",
      "Minibatch loss at step 96200: 0.241970 with learning rate 0.006944\n",
      "step 96200, training accuracy 0.96875\n",
      "Validation accuracy: 0.928933\n",
      "Minibatch loss at step 96300: 0.272749 with learning rate 0.006944\n",
      "step 96300, training accuracy 0.960938\n",
      "Validation accuracy: 0.9289\n",
      "Minibatch loss at step 96400: 0.192423 with learning rate 0.006944\n",
      "step 96400, training accuracy 0.976562\n",
      "Validation accuracy: 0.928733\n",
      "Minibatch loss at step 96500: 0.264056 with learning rate 0.006944\n",
      "step 96500, training accuracy 0.960938\n",
      "Validation accuracy: 0.929117\n",
      "Minibatch loss at step 96600: 0.231963 with learning rate 0.006944\n",
      "step 96600, training accuracy 0.964844\n",
      "Validation accuracy: 0.928383\n",
      "Minibatch loss at step 96700: 0.270949 with learning rate 0.006944\n",
      "step 96700, training accuracy 0.964844\n",
      "Validation accuracy: 0.9285\n",
      "Minibatch loss at step 96800: 0.251530 with learning rate 0.006944\n",
      "step 96800, training accuracy 0.960938\n",
      "Validation accuracy: 0.927917\n",
      "Minibatch loss at step 96900: 0.268524 with learning rate 0.006944\n",
      "step 96900, training accuracy 0.949219\n",
      "Validation accuracy: 0.928867\n",
      "Minibatch loss at step 97000: 0.243769 with learning rate 0.006944\n",
      "step 97000, training accuracy 0.972656\n",
      "Validation accuracy: 0.928683\n",
      "Minibatch loss at step 97100: 0.252732 with learning rate 0.006944\n",
      "step 97100, training accuracy 0.972656\n",
      "Validation accuracy: 0.928383\n",
      "Minibatch loss at step 97200: 0.203305 with learning rate 0.006597\n",
      "step 97200, training accuracy 0.976562\n",
      "Validation accuracy: 0.929\n",
      "Minibatch loss at step 97300: 0.226602 with learning rate 0.006597\n",
      "step 97300, training accuracy 0.972656\n",
      "Validation accuracy: 0.928867\n",
      "Minibatch loss at step 97400: 0.268244 with learning rate 0.006597\n",
      "step 97400, training accuracy 0.96875\n",
      "Validation accuracy: 0.929167\n",
      "Minibatch loss at step 97500: 0.252235 with learning rate 0.006597\n",
      "step 97500, training accuracy 0.972656\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-130b103ee5e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_saving\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-89ca2d377e78>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(num_steps, checkpoint_every, batch_size, graph, is_saving, keep_prob)\u001b[0m\n\u001b[0;32m     32\u001b[0m                 print(\"Validation accuracy: %g\" % \n\u001b[0;32m     33\u001b[0m                     accuracy_l.eval(feed_dict={ tf_train_dataset: valid_dataset, tf_train_labels: valid_labels,\n\u001b[1;32m---> 34\u001b[1;33m                                               tf_keep_prob: 1.0}))\n\u001b[0m\u001b[0;32m     35\u001b[0m                 \u001b[1;31m#save model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_saving\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \"\"\"\n\u001b[1;32m--> 460\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   2908\u001b[0m                        \u001b[1;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2909\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 2910\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munique_fetch_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m       return tf_session.TF_Run(self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 428\u001b[1;33m                                target_list)\n\u001b[0m\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(200001, 100, 256, graph, is_saving=True, keep_prob = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.971534\n",
      "Incorrect no: 533\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU0AAAESCAYAAAB5KIfmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRDQJBALBBohoEggEgg0Q0SQQCAQb+H9pIf4P587jRgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efdcb8096d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#restore_and_display_test(graph, \"l2 reg 3h\")\n",
    "restore_and_display_random_incorrect(16, graph, \"l2 reg 1024\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
